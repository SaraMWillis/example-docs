{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UArizona HPC Documentation","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"policies/acceptable_use/","title":"Acceptable Use","text":"<p>High Performance Computing (HPC) facility users are responsible for complying with all University policies.</p> <p>The supercomputers represent a unique resource for the campus community. These computers have special characteristics that are not found, or are of limited availability, on other central computers, including parallel processing, large memory, and a Linux operating system. The allocation of High Performance Computing (HPC) resources requires close supervision by those charged with management of these resources.  </p>"},{"location":"policies/acceptable_use/#controlled-data","title":"Controlled Data","text":"<p>UArizona HPC does not provide support for any type of controlled data. No controlled data (HIPAA, EAR, FERPA, PII, CUI, ITAR, etc.) can be analysed or stored on any HPC storage.</p>"},{"location":"policies/acceptable_use/#federal-regulations","title":"Federal Regulations","text":"<p>Equipment purchased exclusively for research purposes is exempt from Arizona State Sales Tax. See UA FSO statement of Research Equipment Tax Exemption. However, there are exceptions to this exemption that impact central, shared use research facilities. These exceptions include \"research in social sciences or psychology\"\u2014see AZ ARS 42-5061 subsection B.15.</p> <p>Machinery or equipment used in research and development. For the purposes of this paragraph, \"research and development\" means basic and applied research in the sciences and engineering, and designing, developing or testing prototypes, processes or new products, including research and development of computer software that is embedded in or an integral part of the prototype or new product or that is required for machinery or equipment otherwise exempt under this section to function effectively. Research and development do not include manufacturing quality control, routine consumer product testing, market research, sales promotion, sales service, research in social sciences or psychology, computer software research that is not included in the definition of research and development, or other nontechnological activities or technical services. (emphasis added)</p> <p>In order to provide research computing resources to researchers from these exception areas, some of the research computing resources have been purchased with Arizona taxes included. The result is that there are resources available to all campus researchers, with the caveat that researchers in the social sciences, psychology and instructional projects areas are restricted to using resources that are purchased with taxes paid.</p> <p>Please contact our HPC consultants to learn about the resources that are available for social sciences, psychology and instructional purposes.</p>"},{"location":"policies/acceptable_use/#access-for-research-and-limited-access-for-instruction","title":"Access for Research and Limited Access for Instruction","text":"<p>As described in the 'Sales Tax Exemption' section above, most of the HPC systems are limited to research applications as defined in Section B-14 of ARS Statute 42-5061 by the Arizona Legislature. All users are expected to use these resources accordingly and to use the U-System or other computing systems for non-research purposes.</p>"},{"location":"policies/acknowledgements/","title":"Acknowledgements","text":"<p>PIs should notify our HPC consultants about posters or other publications (published, accepted, submitted, or in preparation) that benefited from the use of UA High Performance Computing, Statistical Consulting, and/or Data &amp; Visualization Consulting. These will be listed in the Collection of Published Results.</p> <p>Results</p>"},{"location":"policies/acknowledgements/#acknowledging-the-uarizona-hpc-resources","title":"Acknowledging the UArizona HPC Resources","text":"<p>The suggested format to acknowledge University of Arizona High-Performance Computing in a paper, poster, or presentation is:</p> <p>This material is based upon High Performance Computing (HPC) resources supported by the University of Arizona TRIF, UITS, and Research, Innovation, and Impact (RII) and maintained by the UArizona Research Technologies department.</p>"},{"location":"policies/acknowledgements/#acknowledging-contributions-from-a-uarizona-research-technologies-staff-member-or-consultant","title":"Acknowledging Contributions from a UArizona Research Technologies Staff Member or Consultant","text":"<p>If you wish to additionally acknowledge an individual who assisted you from University of Arizona High-Performance Computing, the suggested format is:</p> <p>We thank [consultant's name(s)] for [his/her/their] assistance with [describe tasks accomplished], which was made possible through University of Arizona Research Technologies Collaborative Support program. </p>"},{"location":"policies/buy_in/","title":"Buy-in","text":""},{"location":"policies/buy_in/#overview","title":"Overview","text":"<p>The University of Arizona's High Performance Computing (HPC) clusters are servers (computing nodes) and associated high performance storage. There are additional nodes to meet specific needs like high amounts of memory or GPUs. All UA research faculty can sign up for free monthly allocation (following these directions). For researchers who need compute resource beyond the free standard allocation, and who have funding available, we encourage 'buy-in' of additional compute nodes.</p>"},{"location":"policies/buy_in/#benefits","title":"Benefits","text":"Dedicated Research Compute Research groups can 'Buy-In' (add resources such as processors, memory, etc.) to the base HPC systems as funding becomes available. Researchers receive 100% of the CPU*hour time their purchases create as a monthly high-priority allocation. This time receives the highest priority queue on the HPC systems. Quality Environment The Buy-In option allows research groups to take advantage of the central machine room space that is designed for maintaining high performance computing resources. The UITS Research Technologies group physically maintains the purchased nodes, applies updates and patches, monitors the systems for performance and security, and manages software. Additionally, Research Technologies staff is available for research support and questions through hpc-consult@list.arizona.edu. In short, essentially all costs associated with maintaining compute resources are covered by UITS rather than individual researchers. Flexible Capacity Buy-in research group members also benefit from their resources being integrated into a larger computing resource. This means the buy-in resources can be used in conjunction with the free allocation and resources provided to address computational projects that would be beyond the capacity of a group running an independent system alone. Shared Resource The University Research Computing Community as a whole benefits from buy-in expansions to the HPC systems. As mentioned above, researchers who buy-in receive 100% of the allocation of time for their purchase. However if the buy-in resources are not fully utilized, they are made available as windfall resources. This helps to ensure full use of all HPC resources and can be used to justify future purchases of computing resources. Cost Competitiveness Lower costs included in the grant proposals (i.e. hardware only, no operations costs) and evidence of campus cost\u2010sharing give a positive advantage during funding agency review. Pricing For the year following the award the UA HPC request for proposal (RFP) pricing is locked in and is often considerably less than the \"market price.\""},{"location":"policies/buy_in/#buy-in-policies","title":"Buy-in Policies","text":"<ul> <li>University of Arizona can only purchase whole chassis units from Penguin Computing. That is 4 CPU nodes (option 1D), 1 GPU node with 4 GPUs (option 2D), or 1 high memory nodes (option 3). Research Computing will work to match partial node buy-in requests to make full nodes.</li> <li>Monthly high priority time is calculated as: (Number of CPUs * 24 hours * 365 year) / 12 months</li> <li>Purchasing GPUs expands the limit the PI has on number of GPUs that can be used at any time</li> <li>Buy-in high priority allocations will last the lifetime of the system. Puma was purchased in August 2020 and will be end-of-life August 2025.</li> <li>The HPC Buy-in program is not designed to replace or compete with the very large\u2010scale resources at national NSF and DOE facilities, e.g. XSEDE, the Open Science Grid. National resources are available at no financial cost to most US-based researchers through competitive proposal processes. Please contact hpc-consult@list.arizona.edu if you are interested in applying for these resources.</li> <li>The HPC Buy-in program is designed to meet the needs of researchers with medium\u2010scale HPC requirements who want guaranteed, consistent access to compute resources.</li> </ul>"},{"location":"policies/buy_in/#high-priority-allocation-policies","title":"High-priority Allocation Policies","text":"<ul> <li>Standard and high priority jobs will preempt windfall jobs when necessary. </li> <li>Standard jobs do not run on high priority nodes since standard jobs can not be preempted</li> <li>High priority jobs are run on both the buy-in nodes and the centrally-funded nodes. This is advantageous if there is a short-term project deadline.</li> </ul>"},{"location":"policies/buy_in/#compute-buy-in-details-puma-2020","title":"Compute Buy-in Details (Puma 2020)","text":""},{"location":"policies/buy_in/#hardware","title":"Hardware","text":"<p>The buy-in option pricing for the 2024 cluster should be available November 2023.</p> Buy-in Option Technical Specs CPU-Only NodePenguin Computing Altus XE2242 There are 4 CPU nodes in an Altus XE2242 chassisTechnical specs for 1 node of 4 in an Altus XE2242 chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225W) - 512GB RAM, DDR4-3200MHz REG, ECC, 2Rx4 (16 x 32GB) - 2TB SSD local hard drive, 2.5\u201d, NVMe, 4 Lane, 1 DWPD, 3D TLC GPU Node Penguin Computing Altus XE2214GT GPU chassis have 4 GPUs in themTechnical specs for the full XE2214GT chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.3 GHz, 225W) - 4 NVIDIA Tesla V100S-PCIe, 32GB video memory, 5120 CUDA, 640 Tensor, 250W   - 512GB RAM, DDR4-3200MHz REG, ECC, 2Rx4 (16 x 32GB) - 2TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC High Memory NodePenguin Computing Altus XE1212 - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225W) - 3072 GB RAM, DDR4-2933MHz LR, ECC, 4R (24 x 128GB)  - 2TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC"},{"location":"policies/buy_in/#cost-and-allocations","title":"Cost and Allocations","text":"Notice <ul> <li>With V100S GPU's no longer available the pricing will be different.</li> <li>The locked in pricing expired February 28, 2022.  </li> </ul> Option Number CPU Cores V100s GPU RAM (GB) Monthly High-priority Allocation Cost CPU-only Options 1A - One CPU node 96 512 70,080 $8,037.50  (expired) 1B - Two CPU nodes 192 512 140,160 $16,075.00  (expired) 1C - Three CPU nodes 288 512 210,240 $24,112.00  (expired) 1D - Full Altus XE2242 384 512 280,320 $32,150.00  (expired) GPU Node Options 2A - 1/4 Altus XE2214GT 24 1 512 17,520 $8,523.75  (expired) 2B - 2/4 Altus XE2214GT 48 2 512 35,040 $17,047.50  (expired) 2C - 3/4 Altus XE2214GT 72 3 512 52,560 $25,571.25  (expired) 2D - Full Altus XE2214GT 96 4 512 70,080 $34,095.00  (expired) High Memory Node 3 - Full Altus XE1212 96 3072 70,080 $42,230.00  (expired)"},{"location":"policies/committees/","title":"Committees","text":"<p>The Research Computing Guidance Committee (RCGC) is a cross-departmental group of researchers and IT professionals at the University of Arizona with oversight of central research computing resources.</p> <p>The charge to the committee is to design and implement the policies and procedures; the oversight of the operations; and promotion and recommendations for the centrally funded and administered research computing resources. The policies and procedures will be monitored and updated by this steering committee and related task forces created by this committee. The resources will be administered, maintained, and supported by UITS Research Computing, Systems, and Operations.</p>"},{"location":"policies/loss_of_university_affiliation/","title":"Loss of University Affiliation","text":""},{"location":"policies/loss_of_university_affiliation/#affiliation-loss-policy","title":"Affiliation Loss Policy","text":"<p>Losing affiliation with the university (e.g. via graduating, leaving a work position, etc) will result in the denial of access to HPC resources. This will happen automatically on the day of termination according to the University of Arizona Records Database. </p> <p>We unfortunately cannot extend HPC access prior to or after affiliation loss, however, if you require continued access to HPC services, you may register as a Designated Campus Colleague (DCC) through Human Resources. Once your DCC status is approved, you may request sponsorship from a university faculty member. </p>"},{"location":"policies/loss_of_university_affiliation/#data-after-affiliation-loss","title":"Data After Affiliation Loss","text":"<p>Home directory data are maintained for 90 days following affiliation loss. If university affiliation is reestablished and HPC access is restored within those 90 days, when you log into HPC again you will have access to your files. Data in shared storage (e.g. <code>/groups</code> or <code>/xdisk</code>) persist for as long as the storage location is available and are not deleted after a user loses access to the system.</p>"},{"location":"policies/maintenance/","title":"Maintenance","text":""},{"location":"policies/maintenance/#overview","title":"Overview","text":"<p>Most maintenance is performed during regular hours with no interruption to service.  System wide maintenance is usually planned ahead of time and is scheduled for Wednesdays from 8AM to 5PM with at least 10 days notice.  These will be planned to occur four times per year.</p> <p>Maintenance windows represent periods when UITS may choose to drain the queues of running jobs and suspend access to the cluster operation for HPC maintenance purposes.</p> <p>The notification will describe the nature and extent (partial or full) of the interruptions of HPC services. </p> <p>Batch queues will also be modified prior to scheduled downtimes to hold jobs which request more wallclock time than remains before the shutdown. Held jobs will be released to run once maintenance concludes.</p>"},{"location":"policies/maintenance/#emergency-maintenance","title":"Emergency Maintenance","text":"<p>Unavoidable (emergency) downtime may occur as a result of any of the above reasons at almost any time. Such events are rare and great effort is made to avoid these situations. However, when emergency maintenance is needed, the UITS unit responsible for the item affected will provide as much notice to users as possible and work to resolve the fault as quickly as possible.</p> <p>Any emergency outages will be announced via email through the hpc-announce@list.arizona.edu mailing list. </p>"},{"location":"policies/special_projects/","title":"Special Projects","text":""},{"location":"policies/special_projects/#overview","title":"Overview","text":"<p>The RCGC (Research Computing Governance Committee) has approved support for \"Special Projects\" that use more than the standard allocation of hours. When a special project request is granted, an additional allocation of standard hours is provided for a limited period of time.   </p> <p>There is not a specific definition of what comprises a project, but it is often support for publication or grant deadlines, or graduation dates. Each request is considered on a case by case basis.</p>"},{"location":"policies/special_projects/#authorized-requestors","title":"Authorized requestors","text":"<p>Project requests must be submitted by a PI; partly because the additional time granted will go to the allocation of the PI.</p>"},{"location":"policies/special_projects/#guidelines","title":"Guidelines","text":"<ul> <li>All special project allocations are temporary</li> <li>Special project allocations cannot be granted if they will impact the community of users with a standard allocation</li> <li>PIs can only be granted a special project once within a 12 month period (starting from the ending period of the last special project)<ul> <li>PI groups that routinely need more standard computing hours should supplement their needs in other ways (e.g. buy-in, accessing national/international computing resources, etc)</li> <li>The Research Computing group can help UA PIs access national-scale computing centers (e.g. ACCESS, TACC, Open Science Grid, etc)</li> </ul> </li> </ul>"},{"location":"policies/special_projects/#categories","title":"Categories","text":"Size Requirements 10,000 hours per month or less The majority of requests fall into this category. These requests provide a general statement of the need and are one month or less in duration. These requests are occasional and they can be granted automatically by UITS staff with minimal impact to windfall queue usage. This will be done at the discretion of the Research Technologies staff. 10,000 to 100,000 hours per month These requests must provide a defined number of hours and not to exceed 3 months. A defined statement of need (e.g. publication deadline) will be provided. These requests are occasional, and can be granted by Research Technologies staff after considering the researcher\u2019s need, alternative ways to solve those needs, and assessing that there is no overall impact to the system. Greater than 100,000 hours per month These requests must provide a defined number of hours, defined duration not to exceed 6 months, and a defined statement of need (e.g. publication deadline). These requests required PIs to report back the results of their calculations (publications, conference presentations, etc) for potential inclusion in our online documentation. Benchmarking, profiling, or assessment of analyses run should also be provided to help the UA HPC understand how the additional computing time was used and ways that need can be met in the future without a special project. These requests will be forwarded to the HPC policies subcommittee of RCGC and require committee approval before being granted."},{"location":"policies/special_projects/#submitting-a-request","title":"Submitting a Request","text":"<p>Requests are submitted via a web form in the HPC user portal under Support Requests.  Please include:</p> <ul> <li>Number of additional standard hours needed for the special project</li> <li>Duration of the Special Project in months</li> <li>Reason for the temporary increase</li> </ul>"},{"location":"quick_start/batch_jobs/","title":"Batch Jobs","text":""},{"location":"quick_start/batch_jobs/#batch-jobs","title":"Batch Jobs","text":""},{"location":"quick_start/batch_jobs/#what-is-a-batch-job","title":"What is a Batch Job?","text":"<p>Batch jobs are the real meat and potatoes of HPC. They are a way of submitting work to run on a cluster without the need to be physically present. They are essentially blueprints that the scheduler uses to determine what interpreter to use, the resources you need, and the instructions to run your job. </p> <p>Because you do not need to be physically present while your work is running, you can log out of the system, turn off you computer and your work will not be interrupted. Additionally, it enables you to submit tens, hundreds, or even thousands of jobs to run simultaneously which can help you get a tremendous amount of work done!</p> <p>The basic layout of a batch script is fairly straight forward. It is a plain text file that is divided into three sections:</p>"},{"location":"quick_start/batch_jobs/#creating-a-batch-script","title":"Creating a Batch Script","text":"<p>Let's now create a directory in your home using <code>mkdir</code> where we'll create the files needed for this tutorial. In this directory, we'll create a batch script called hello_world.slurm with <code>touch</code>. </p> <pre><code>mkdir ~/hello_world\ncd ~/hello_world\ntouch hello_world.slurm\n</code></pre> <p>This batch script will be written with three sections:</p> <pre>#!/bin/bash</pre> <pre>#SBATCH --option=value</pre> <pre>[code here]</pre> <ol> <li>The first section will always be the line <code>#!/bin/bash</code>. This is called a \"shebang\" and tells the system to interpret your file as a bash script. Our HPC systems use bash for all our environments, so it should be used in your scripts to get the most consistent, predictable results.</li> <li>The second section will have multiple lines, all of which start with <code>#SBATCH</code>. These lines are interpreted as directives by the scheduler and are how you request resources on the compute nodes, set your output filenames, set your job name, request emails, etc. A list of directives is shown in our Running Jobs documentation.</li> <li>The third section in your script is a set of bash commands that tells the system how to run your analyses. This includes any module load commands you'd need to run to access your software, software commands to execute your analyses, directory changes, etc. </li> </ol> <p>Let's try writing a bash script now.</p> <p>Open this new batch file in your favorite text editor. If you have never used a terminal editor before, use the command <code>nano hello_world.slurm</code>. In the editor, enter the contents shown below. Make sure to replace <code>&lt;PI GROUP&gt;</code> next to <code>--account=</code> with the group name you found with <code>va</code> in the Accessing Compute Nodes section. Do not include the <code>&lt;</code> or <code>&gt;</code> in your group name.</p> <pre><code>#!/bin/bash\n</code><code># --------------------------------------------------------------\n### PART 1: Requests resources to run your job.\n# --------------------------------------------------------------\n#SBATCH --job-name=hello_world\n#SBATCH --account=your_group\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:01:00\n</code><code># --------------------------------------------------------------\n### PART 2: Executes bash commands to run your job\n# --------------------------------------------------------------\nmodule load python/3.9\ncd ~/hello_world\npython3 -c \"print(\"hello world\")\n### sleep is used for demonstration purposes\nsleep 30\n</code>\n</pre> <p>Then save and exit. To do this in nano, type Ctrl+X, select Y to save, and hit Enter to confirm your filename.</p>"},{"location":"quick_start/batch_jobs/#submitting-your-batch-job","title":"Submitting Your Batch Job","text":"<p>Tip</p> <p>In this tutorial, we are submitting our job from an interactive session on a compute node. You may also submit jobs from a login node. You do not need to stay connected to the compute node for your batch job to run.</p> <p>The next step is to submit your job request to the scheduler. To do this, you\u2019ll use the command <code>sbatch</code>. This will place your job in line for execution and will return a job ID. This job ID can be used to check your job\u2019s status with <code>squeue</code>. A more comprehensive look at job commands can be found in our Slurm documentation. </p> <p>Let\u2019s submit the script and check its status (substitute your own job ID below where relevant):</p> <pre><code>[netid@gpu66 hello_world]$ sbatch hello_world.slurm\nSubmitted batch job 807387\n[netid@gpu66 hello_world]$ squeue --job 807387\n             JOBID PARTITION     NAME     USER ST       TIME  NODES\n            807387  standard hello_wo    netid PD       0:06      1 \n</code></pre> <p>The command <code>squeue</code> gives us detailed information about our batch jobs while they're in queue or running. Under <code>ST</code> you can check the state of your job. In this case, it's pending (<code>PD</code>) which means it's waiting in line with other jobs. Once the job starts running, it's state will change to <code>R</code>, and when the job has completed running, <code>squeue</code> will return a blank line. </p> <p>Once your job completes, you should see an output file in the directory where you submitted the batch script. This output file captures anything that would have been printed to the terminal if you had run it interactively. By default, output filenames will be <code>slurm-&lt;jobid&gt;.out</code>, so in this example, our output file is <code>slurm-807387.out</code>. </p> <p>Let's check the contents <pre><code>[netid@gpu66 hello_world]$ cat slurm-807387.out\nhello world\n[netid@gpu66 hello_world]$\n</code></pre></p>"},{"location":"quick_start/batch_jobs/#additional-resources","title":"Additional Resources","text":"<p>That's it! You've now successfully run both a batch and interactive job on HPC. To continue learning about HPC, our online documentation has a lot more information that can help get you started. For example FAQs, information on HPC storage, and file transfers. You can also find additional example batch jobs in our Running Jobs section.</p> <p>Other great resources include: virtual office hours every Wednesday from 2:00-4:00pm, consultation services offered through ServiceNow, an examples Github page with sample jobs, and a YouTube channel with training videos. </p> <p>\u25c0 Software</p>"},{"location":"quick_start/common_misconceptions/","title":"Common Misconceptions","text":""},{"location":"quick_start/common_misconceptions/#common-misconceptions","title":"Common Misconceptions","text":"<p>Before we cover the specifics of using the system, let's quickly cover some common misconceptions about running code on HPC systems. </p>"},{"location":"quick_start/common_misconceptions/#if-i-move-my-code-to-hpc-it-will-automatically-run-faster","title":"If I move my code to HPC, it will automatically run faster","text":"<p>You might be surprised to learn that if you move code from a local computer to a supercomputer, it will not automatically run faster and may even run slower. This is because the power of a supercomputer comes from the volume of resources available (compute nodes, CPUs, GPUs, etc.) and not the clockspeed of the processors themselves. Performance boosts come from optimizing your code to make use of the resources available. An example of this is parallelizing your code. </p> <p>One way to think about this is with a car analogy. If your local computer is a sedan, a supercomputer is not a lamborghini. Instead, a supercomputer can be thought of as a fleet of (large) sedans. </p>"},{"location":"quick_start/common_misconceptions/#if-i-allocate-more-cpus-to-my-job-my-software-will-use-them","title":"If I allocate more CPUs to my job, my software will use them","text":"<p>Most software needs to be designed to use multiple CPUs as part of its execution. You will need to ensure your software has the capability to make use of multiple CPUs for it to be able to take advantage of additional hardware. The job scheduler only provides the resources, the code itself is what needs to know how to make use of them.</p>"},{"location":"quick_start/common_misconceptions/#if-i-allocate-more-nodes-physical-computers-to-my-job-my-software-will-use-them","title":"If I allocate more nodes (physical computers) to my job, my software will use them","text":"<p>Similar to above, software must be designed to be able to take advantage of additional computers when they are allocated to your job. Most often, this is using something like MPI. If your software is not MPI-enabled, it cannot use more than one computer as part of its analyses, even if it is using something like OpenMP or Python multiprocessing to make use of multiple CPUs. </p> \u25c0 What is HPC? Logging In \u25b6"},{"location":"quick_start/interactive_jobs/","title":"Accessing Compute Nodes","text":""},{"location":"quick_start/interactive_jobs/#compute-nodes-and-interactive-jobs","title":"Compute Nodes and Interactive Jobs","text":""},{"location":"quick_start/interactive_jobs/#the-compute-nodes","title":"The Compute Nodes","text":"<p>Unlike the bastion host and login nodes, there are many compute nodes and each has, as the name suggests, a large amount of computational resources available to run your work. For example, Puma standard nodes have 94 available CPUs and a whopping 470GB of RAM. </p> <p></p> <p>To get a sense of what the cluster looks like, try running the command <code>nodes-busy</code>. This should look something like:</p> <pre><code>\u271a    Buy-in nodes. Only accept high_priority and windfall jobs\n(puma) [sarawillis@wentletrap ~]$ nodes-busy \n==============================================================\n\n                      \u2592 System Status \u2592\n              Wed Feb 14, 03:42:09 PM (MST) 2024\n\nStandard Nodes\n==============================================================\nr1u16n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u17n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u18n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u25n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u26n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u26n2  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \nr1u27n1  :[\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592] 100.0%   \n</code></pre> <p>Each line shows one compute node on the cluster you're connected to and how busy it currently is running jobs. By default, when you first log in you're connected to the Puma cluster. This is the largest and newest and generally provides the most in terms of computational resources. However, we have two other clusters available: Ocelote and ElGato, each with a good number of computational resources available and shorter wait times to access them. </p> <p>When you first connected to a login node in the previous section, your terminal should have displayed:</p> <pre><code> ***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n</code></pre> <p>This shows you the various shortcuts you can use to connect to the different clusters. Try running the command <code>elgato</code> now. You should see a change in your terminal prompt to indicate that your cluster has changed. </p> <pre><code>(puma) [user@wentletrap ~]$ elgato\n(elgato) [user@wentletrap ~]$ \n</code></pre> <p>Now that you've switched clusters, you're ready to try running some work. </p>"},{"location":"quick_start/interactive_jobs/#job-charging","title":"Job Charging","text":"<p>Before you connect to a compute node, let's quickly cover job charges. </p> <p>Every HPC group gets a free allocation of CPU hours that they can spend every month to access compute resources. You can think of a CPU hour as a token to buy one CPU for one hour, so if you want to reserve 5 CPUs for 10 hours, this will charge 50 CPU hours to your account. You can see more detailed information on job queues, allocations, and job charging on our Time Allocations page which has a comprehensive breakdown.</p> <p>For this tutorial, we'll focus on the standard partition. This is a job queue and is the one that consumes your standard allocation. To use this job queue, you'll need to know your account name. To check, use the command <code>va</code> which stands for \"view allocation\". The output will look something like:</p> <pre><code>(elgato) [user@gpu5 ~]$ va\nWindfall: Unlimited\n\nPI: parent_974 Total time: 7000:00:00\n    Total used*: 1306:39:00\n    Total encumbered: 92:49:00\n    Total remaining: 5600:32:00\n    Group: group_name Time used: 862:08:00 Time encumbered: 92:49:00\n\n*Usage includes all subgroups, some of which may not be displayed here\n</code></pre> <p>You should see a name next to the <code>Group</code> field (in the example above <code>group_name</code>). If you see multiple groups, then you are sponsored in multiple groups and can choose any one of your group names. Note the name of your account and hang onto it for the upcoming sections.</p>"},{"location":"quick_start/interactive_jobs/#interactive-jobs","title":"Interactive Jobs","text":"<p>Now, let's actually access a compute node. When you're connected to a login node, you can connect to a compute node by using the command <code>interactive</code>. The command <code>interactive</code> by default will give you one CPU for one hour (which will charge your account one CPU hour -- don't worry, you have a lot!). You can adjust this using the different flags available which are documented on our Interactive Jobs page. For now, we'll stick with the default resources. </p> <p>To access a session, run the following, substituting your own group name (that you found with <code>va</code> in the section above) in for <code>group_name</code>: <pre><code>interactive -a group_name\n</code></pre></p> <p>For example, in my case: <pre><code>(elgato) [netid@wentletrap ~]$ interactive -a hpcteam\nRun \"interactive -h for help customizing interactive use\"\nSubmitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=1 --time=01:00:00 --account=hpcteam --partition=standard\nsalloc: Granted job allocation 1800857\nsalloc: Nodes cpu39 are ready for job\n[netid@cpu39 ~]$ hostname\ncpu39.elgato.hpc.arizona.edu\n</code></pre></p> <p>You'll notice once your job starts that your command line prompt changes to display the name of the compute node. If you run hostname, this should match your command line prompt and show you the name of the compute node you're connected to. in my case, I'm connected to the ElGato compute node <code>cpu39</code>.</p> <p>You'll also notice that your session has been assigned a job number (in the above, you can see this as <code>Granted job allocation 1800857</code>). A job number is assigned to every job on the system and is used to keep track of job statistics and metrics. This is especially useful for batch jobs which we will cover in a few sections. </p> \u25c0 Storage and Transfers Accessing Software \u25b6"},{"location":"quick_start/logging_in/","title":"Logging In","text":""},{"location":"quick_start/logging_in/#logging-in-and-system-layout","title":"Logging In and System Layout","text":"<p>Tip</p> <p>If you experience any issues during the login process, see our FAQs for common problems.</p> <p>HPC systems can sometimes feel a little like a complicated black box and it can be a challenge to know exactly where you are and what the structure looks like. In this section, we'll give you an idea of how the system is laid out so you understand exactly where you are at each stage of the login process and what activities are performed where. </p>"},{"location":"quick_start/logging_in/#the-bastion-host","title":"The bastion host","text":"<p>In another browser window, open our instructions on logging in from the command line. Start by following the first step shown that's specific to your operating system. Stop when your terminal displays </p> <p><pre><code>Success. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n</code></pre> If all has gone well, you are now connected to what is known as the bastion host. </p> <p></p> <p>The bastion host is the first computer you land on when you log in using the hostname <code>hpc.arizona.edu</code>. This machine is only used to validate your credentials and provide a gateway to the rest of the HPC environment. It is not used for storing files and has no software installed so no computational work is done at this stage. As a test, try running the command <code>hostname</code>:</p> <pre><code>[user@gatekeeper 14:50:49 ~]$ hostname\ngatekeeper.hpc.arizona.edu\n</code></pre> <p>The output shows <code>gatekeeper</code> which is the name of this node and is how you can tell you're connected to the bastion. </p> <p>Next, to leave the bastion host, type the command <code>shell</code>.</p>"},{"location":"quick_start/logging_in/#the-login-nodes","title":"The login nodes","text":"<p>After you type <code>shell</code> on the bastion host, you're connected to a computer called a login node. </p> <p></p> <p>We have two of these available and you will be assigned one at random. If you run the <code>hostname</code> command as you did on the bastion host, you should see either <code>wentletrap</code> or <code>junonia</code>. </p> <p>A login node is a shared workspace with minimal computational capabilities and very little software installed. They are not the place where computational work is done so users should not run their analyses, compile their software, or perform computationally intensive work in this location. Instead, they are meant for activities such as managing files, writing scripts, submitting and monitoring jobs, and viewing system resources.</p> \u25c0 Common Misconceptions Storage and Transfers \u25b6"},{"location":"quick_start/overview/","title":"UArizona HPC Quick Start Guide","text":"<p>Just getting started with our HPC and don't know where to start? You've found the place!</p> <p>If you're using an HPC system for the first time, this tutorial is designed to give you the basic tools to get you started. </p> <p>If you're experienced with HPC already but haven't used our system, this guide is intended to help familiarize you with the specifics of our HPC clusters. This should help you get up and running quickly since every HPC system is slightly different; each with its own specific commands, allocation policy, partitions, accounts, and even system layout. </p> <p>By the end of this quick start, you should know:</p> <ol> <li>What HPC is</li> <li>How HPC is structured</li> <li>How to log in</li> <li>How jobs are charged</li> <li>How to access software</li> <li>How to run a job</li> </ol> What is HPC? \u25b6 <p></p>"},{"location":"quick_start/software/","title":"Software","text":""},{"location":"quick_start/software/#software-on-hpc","title":"Software on HPC","text":"<p>It's hard to perform analyses without software. We learned that software packages are not available on the login nodes, but now that we're connected to a compute node, we can see and use what's available. </p> <p>Software on HPC comes installed as modules. Modules make it easy to load and unload software from your environment. This allows hundreds of packages to be available on the same system without dependency or versioning conflicts. It's always good practice to specify which version of the software you need when loading to ensure a stable environment.</p> <p>You can view and load software modules using the commands <code>module avail</code> and <code>module load</code>, respectively. For example:</p> <pre><code>[netid@gpu66 ~]$ module avail python\n\n------------------------------------------------- /opt/ohpc/pub/modulefiles --------------------------------------------------\n   python/3.6/3.6.5    python/3.8/3.8.2 (D)    python/3.8/3.8.12    python/3.9/3.9.10\n\n[netid@gpu66 ~]$ module load python/3.9\n[netid@gpu66 ~]$ python3 --version\nPython 3.9.10\n</code></pre> <p>Try running <code>module avail</code> without specifying any arguments. You'll notice we have a lot available.</p> <p>Since you're in an interactive session, you have exclusive access to the resources you've reserved which means you can use this software to develop, test, run, and debug your code. Interactive sessions are optimal for these sorts of actions. However, you might run into problems executing your analyses if:</p> <ul> <li>Your session times out due to inactivity</li> <li>Your internet connection gets disrupted</li> <li>Your computer gets closed or turned off</li> <li>You want to run more than one job at a time</li> </ul> <p>That's where batch jobs come in. </p> \u25c0 Accessing Compute Nodes Batch Jobs \u25b6"},{"location":"quick_start/storage_and_transfers/","title":"Storage and Transfers","text":""},{"location":"quick_start/storage_and_transfers/#storage-and-transfers","title":"Storage and Transfers","text":"<p>When you first access a login node, you're located in your home directory. This is a space with a 50GB limit and is accessible to only you. The files you store here are housed on a large storage array and are accessible anywhere you are on the system except the bastion host. </p> <p></p> <p>To store your files in your home, you will need to transfer them to the system. Small files can most easily be transferred to/from HPC using our web interface Open OnDemand. In the upper-left you'll see a dropdown called Files where you can select Home Directory. </p> <p></p> <p>On the following page, select \"Upload\" to open a window where you can drag/drop files.</p> <p></p> <p>For larger files, we have a designated file transfer node. Comprehensive instructions for alternative methods for file transfers can be found on our data transfer page. </p> <p>With larger files comes the need for more storage. If you find your home is insufficient to store your data, group allocations are available. See our storage documentation for details on options that are available. </p> <p>Now that we're on the login nodes and know where our files are, it's time to access a compute node. </p> \u25c0 Logging In Accessing Compute Nodes \u25b6"},{"location":"quick_start/supercomputing_in_plain_english/","title":"Supercomputing in Plain English","text":""},{"location":"quick_start/supercomputing_in_plain_english/#supercomputing-in-plain-english","title":"Supercomputing in Plain English","text":""},{"location":"quick_start/supercomputing_in_plain_english/#what-is-hpcwhat-is-a-supercomputer","title":"What is HPC/What is a Supercomputer?","text":"<p>If you've never used an HPC system before, you may be wondering what one is and why you'd want to use it. HPC stands for High Perfomance Computing and is synonymous with Supercomputer. A supercomputer is a collection, or cluster, of a large number of regular computers (referred to as compute nodes) connected over a network. Each of the computers is like a local workstation though typically much more capable. For example, a standard laptop might have 4 CPUs and 8gb of RAM. Compare this with a standard compute node on Puma which has a whopping 94 CPUs and 470gb of RAM!</p> <p>Another thing that differentiates supercomputers from your personal workstation is a supercomputer is a shared resource. This means there may be hundreds or even thousands of simultaneous users. Without some sort of coordination, you can imagine it would be a logistical nightmare to figure out who can run their code, what resources they can use, and where they should run it. That's why supercomputers use job schedulers (we use one called Slurm).</p> <p>A job scheduler is software used to coordinate user jobs. You can use it by writing a special script that requests compute resources (e.g., CPUs, RAM, GPUs) and includes instructions for running your code. You submit this script to the job scheduler using a special command called <code>sbatch</code> and it does the work of finding the required space on the supercomputer for you. It then runs your code and returns the results to your account in a text file. As part of this quick start tutorial, we will go through the process of writing and submitting one of these scripts. </p>"},{"location":"quick_start/supercomputing_in_plain_english/#why-should-i-use-a-supercomputer","title":"Why Should I Use a Supercomputer?","text":"<p>There are lots of reasons to use a supercomputer! For example, say you have analyses that require a tremendous amount of memory or storage space. It's not feasible (or very expensive) to use 3TB of memory or 10TB of disk space on a local workstation, but on our systems it's very possible (and free). This is how you scale up from the workstation under your desk. </p> <p>Another possibility is you may have thousands of simulations to do. This may take an unreasonable amount of time and be a serious bottleneck for your research if you're running them in serial locally. However, on a supercomputer you can run hundreds of jobs at the same time using thousands of CPUs. This means you may wind up getting results in hours instead of months. This how you scale out your work.</p> <p>You may also have experience with being frustrated with a job's runtime. What happens if it takes a week or longer to complete one of your analyses? On a local workstation, keeping your computer awake for the duration of the run may be difficult, inconvenient, or impossible. On a supercomputer, the process of running jobs can be fully automated. Once you have a special script written with all the necessary instructions, you can submit it to the scheduler and it does the rest. This means you can log out and close your computer without any worry about interrupting your work. Your results are returned to you as a text file in your account in real time so you can always log in and check your progress.</p> \u25c0 Overview Common Misconceptions \u25b6"},{"location":"registration_and_access/account_creation/","title":"Account Creation","text":""},{"location":"registration_and_access/account_creation/#overview","title":"Overview","text":"<p>All UArizona Faculty, Staff, Students, and Affiliates are eligible for HPC accounts free of cost.</p> <p>Faculty members will receive a PI Account which is different from the Sponsored Accounts that staff, students, and affiliates receive. PI Accounts are self-sponsored, manage their own HPC group(s), and receive their own time and storage allocations to be shared among group members. Faculty members may add other users to their groups to grant them HPC access. Users may be a member of more than one HPC group.</p>"},{"location":"registration_and_access/account_creation/#how-to-register","title":"How to Register","text":"<p>The process of getting registered for an HPC account varies depending on your affiliation with the University. Take a look at the list below to determine how you should register:</p> I'm a faculty member/principal investigator (PI) <p>If you are a research faculty member or principal investigator, you can sponsor yourself for access through our user portal.       Step 1: Visit https://portal.hpc.arizona.edu/. This will automatically create an HPC account for you.       Step 2: Go to https://portal.hpc.arizona.edu/portal/sendlink.php and click the link on the left-hand side as shown below             This will automatically redirect you back to the user portal, create a research group for you, and add you as a member. You will briefly see a pop-up notification verifying you've been added to your group. You can check this by going to the Manage Groups tab and clicking your group's dropdown menu to view its members.        </p> I'm a student, postdoc, staff member, or Designated Campus Colleague <p>      If you are affiliated with the University of Arizona but are not faculty, you will need to request sponsorship from a faculty member. This can be done through our web portal.       Step 1: Create an HPC account by navigating to https://portal.hpc.arizona.edu/.      Step 2: Request sponsorship from a UArizona faculty member. Note: Your faculty sponsor will need their own HPC account before they are able to sponsor others.           To request sponsorship, navigate to https://portal.hpc.arizona.edu/portal/sendlink.php. On the right-hand side, enter your sponsor's email address and click send. Your sponsor will then receive an email with a link used to authorize your account. Once they confirm your request, you will receive an email with instructions for accessing the HPC systems.          Note: it may take up to 15 minutes after approval to receive a confirmation email and for your account to officially be activated.          If you do not receive an email verification, you should contact your sponsor and confirm receipt and approval of the HPC account request. If your account has been approved but you have not received verification, you should contact HPC consulting and provide your NetID, your name, and the email address of your sponsor.      </p> I'm not affiliated with the university <p>     HPC systems are restricted to those with valid university credentials and are not available for general public use. However, if you are not officially affiliated with the university but are actively collaborating with university members, you may register for Designated Campus Colleague (DCC) status. This is done through human resources and provides collaborators with active UArizona credentials. Once your DCC status is approved, you may request sponsorship from a university faculty member. For instructions on this process, see the section above.     </p>"},{"location":"registration_and_access/account_deletion/","title":"Account Deletion","text":""},{"location":"registration_and_access/account_deletion/#manual-deletion","title":"Manual Deletion","text":"<p>If you wish to delete your HPC account, you may do so through the User Portal. Navigate to the Support tab and click the Close Your HPC Account link. You will be prompted to Manually Confirm by entering \"confirm\" at the prompt. Click Close Account to complete the process.</p> <p></p>"},{"location":"registration_and_access/account_deletion/#loss-of-university-affiliation","title":"Loss of University Affiliation","text":"<p>Losing affiliation with the university will result in the denial of access to HPC resources. This will happen automatically on the day of termination according to the University of Arizona Records Database. Data may be retrievable if a student or employee is reinstated, or by the PI. Please contact us for support in this case. </p> <p>If you are losing affiliation and require continued access to HPC services, you may register as a Designated Campus Colleague (DCC) through Human Resources. Once your DCC status is approved, you may request sponsorship from a university faculty member. </p>"},{"location":"registration_and_access/group_management/class_groups/","title":"Class Groups","text":"<p>Tip</p> <p>If you are interested in having an HPC staff member come to your class to do an Intro to HPC presentation, reach out to our consultants.</p> <p>If you are a faculty member and are teaching a course that makes use of HPC resources, you can create a Class Group that will grant your students system access. Class groups are designed to be created and used for one semester only.</p>"},{"location":"registration_and_access/group_management/class_groups/#creating-a-class-group","title":"Creating a Class Group","text":"<p>Log into your User Portal, navigate to the Manage Groups tab, and select the Add New Group dropdown option at the top of the page.There will be an option to specify your Group Type on the right. Choose Class from the dropdown menu</p> <p></p> <p>Under Group Name, enter something descriptive and then complete the process by clicking Add Group.</p> <p></p> <p>Once this process is complete, you can find your group's dropdown tab under Manage Groups. There you can add students either individually or in batch by uploading a CSV file with your student's NetIDs. You may also remove students from the group by clicking their NetIDs and then selecting Remove Member(s), or delete the group itself by selecting Delete Group.</p> <p></p>"},{"location":"registration_and_access/group_management/class_groups/#file-permissions-and-storage","title":"File Permissions and Storage","text":"<p>Students in your class group will only be able to access files and directories owned by the class group. This means they will not be able to access files and directories owned by your standard research group. </p>"},{"location":"registration_and_access/group_management/class_groups/#running-jobs-and-allocations","title":"Running Jobs and Allocations","text":"<p>Due to Arizona sales tax restrictions, class groups may only using the windfall queue on Puma. However, standard hours may be used by students on Ocelote. To submit standard jobs on Ocelote, students will use the class group's name for the <code>--account</code> SLURM directive. For example:</p> <p><pre><code>#SBATCH --account=hpc101\n#SBATCH --partition=standard\n</code></pre> Standard hours used on Ocelote are pulled from the same pool as your research group so make sure to plan accordingly. If you run the command <code>va</code>, you will see the class group as being nested under the total time allocated to your primary research group as well as any others you may have created. Students will not see the names of your other research groups if they run va unless they are members. </p> <pre><code>(ocelote) [faculty_netid@wentletrap ~]$ va\nWindfall: Unlimited\n\nPI: parent_000 Total time: 35000:00:00\n    Group: hpc101 Time used: 0:00:00 Time encumbered: 0:00:00\n    Group: faculty_netid Time used: 0:00:00 Time encumbered: 0:00:00\n    Total used: 0:00:00\n    Total encumbered: 0:00:00\n    Total remaining: 35000:00:00\n</code></pre>"},{"location":"registration_and_access/group_management/delegating_group_management_rights/","title":"Delegating Group Management Rights","text":"<p>PI's can delegate management rights to trusted group members. Delegates may create research and class groups, sponsor users, remove users, and request and manage storage offerings on behalf of their faculty sponsor. To add a group member as a delegate, the PI can click the Manage Delegates link on the home page of the user portal. In the Manage Delegates window that appears, select Add Delegate, enter your group member's NetID, and click Add.</p> <p></p> <p>Once a group member has been added as a delegate, they can log into the user portal, selecting Switch User, enter their PI's NetID in the pop-up field, and click Switch User. This will allow them to perform functions on their PI's behalf. They may switch back to their own account at any time by selecting Switch User and entering their own NetID.</p> <p></p>"},{"location":"registration_and_access/group_management/overview/","title":"Group Management","text":"<p>HPC groups are ways for faculty members to manage file permissions, job allocations, and group members. There are two types: Research Groups and Class Groups.</p> <ul> <li>Research groups include any faculty, postdocs, graduate students, DCCs, staff, or student workers actively affiliated with your group's research. </li> <li>Class groups are for educational purposes only and will include students enrolled in a semester-long course.</li> </ul>"},{"location":"registration_and_access/group_management/research_groups/","title":"Research Groups","text":"<p>If you are a faculty member who has registered for an HPC account, a research group with your UArizona NetID name has automatically been created for you. This group has an allocation of CPU hours associated with it as well as communal storage for your data.</p>"},{"location":"registration_and_access/group_management/research_groups/#adding-members","title":"Adding Members","text":"<p>To add members to your research group, go to https://portal.hpc.arizona.edu/ and click the Manage Groups tab at the top of the screen. Click your group's dropdown tab and click Add Member</p> <p></p> <p>Enter the user's UArizona NetID in the box that appears, and select Add</p> <p></p> <p>To add members in bulk, you may also select Upload Member List and upload a CSV file of UArizona NetIDs.</p> <p>The process of adding new members may take a few seconds to complete. Once the changes have taken place, you will see the user's NetID in your group:</p> <p></p>"},{"location":"registration_and_access/group_management/research_groups/#creating-a-new-group","title":"Creating a New Group","text":"<p>A new group can be created at any time through the user portal. New groups will share their time and storage allocations with your primary group. Alternate research groups can be a good solution for managing file permissions. For example, if you needed to restrict access of a particular directory to a certain subset of your group, you could do this by creating a new research group, adding the group members who need access to those files, and then changing the group ownership of the files/directories that need restricted access.</p> <p>To create a new group, log into the user portal, select the Manage Groups and select the Add New Group dropdown menu. </p> <p></p> <p>Once your group has been created, you will see it when running <code>va</code> (short for View Allocation) in the same block as your primary group:</p> <pre><code>(puma) [faculty-netid@junonia ~]$ va\nPI: parent_1206 Total time: 7000:00:00\n    Group: faculty-netid Time used: 0:00:00 Time encumbered: 0:00:00\n    Group: your-new-group Time used: 0:00:00 Time encumbered: 0:00:00\n    Total used: 0:00:00\n    Total encumbered: 0:00:00\n    Total remaining: 7000:00:00\n</code></pre>"},{"location":"registration_and_access/system_access/command_line_access/","title":"Command Line Access","text":"Tip <ul> <li>Credentials: To log into HPC, you will need NetID+ enabled, an HPC account, and internet access. Because we require Duo-authentication to access the system, no VPN is required. </li> <li>Password Visibility: When entering your password in the terminal at the prompt, you will not see any characters appear on the screen while typing during this step. This is normal and everything is working as it should.</li> </ul> Linux/MacWindows Tip <p>Mac systems provide a built-in SSH client, so there is no need to install any additional software. You will find the terminal application under Applications \u2192 Utilities \u2192 Terminal.</p> <p>Open the terminal and enter: <pre><code>$ ssh netid@hpc.arizona.edu\n</code></pre> where netid is your UArizona NetID. When you press enter, you will be prompted for your university password. After successfully entering your password, you will be prompted to Duo Authenticate. If everything is successful, you will be connected to the bastion host.</p> <p>Windows systems do not have any built-in support for using SSH, so you will have to download a software package to do so. There are several available for Windows workstations.  Free SSH clients are available for download from the University of Arizona's Site License website.  </p> PuTTYMobaXterm <p>PuTTY is the most popular open source SSH Windows client. To use it: download, install, and open the Putty client. Next, open a connection and enter <code>hpc.arizona.edu</code> under Host Name and press Open</p> <p></p> <p>This will open a terminal. At the prompt, enter the following, replacing <code>&lt;netid&gt;</code> with your own NetID:</p> <pre><code>Login as: &lt;netid&gt;\n</code></pre> <p>You will then be prompted to Duo-Authenticate. If the process is successful, you will be connected to the bastion host.</p> <p>MobaXterm is another available SSH Windows client. To connect to HPC, download and install MobaXterm, open the software, select Session \u2192 SSH and enter <code>hpc.arizona.edu</code> under Remote host. Next, select the box next to Specify username and enter your UArizona NetID. To connect, click OK at the bottom of the screen:</p> <p></p> <p>This will open a terminal and will prompt you for your UArizona password. You will then need to Duo-authenticate. If everything is successful, you will be connected to the bastion host.</p> <p>Once you reach the bastion host, regardless of method, you should see the following: <pre><code>Success. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n</code></pre> From there, type <code>shell</code> to connect to the login nodes that will provide access to our three clusters. On the login nodes, you should see: <pre><code>***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nPuma:\n$ puma\n(puma) $\nOcelote:\n$ ocelote\n(ocelote) $\nElGato:\n$ elgato\n(elgato) $\n-----------------------------------------\n</code></pre></p>"},{"location":"registration_and_access/system_access/overview/","title":"Overview","text":"<p>Logging into the HPC supercomputers starts with your UArizona NetID and password with two-factor authentication enabled. This section is intended to provide you with instructions on getting terminal access to the system from your specific OS, how to log into the system from our web interface (Open OnDemand), how to set up X11 (image) forwarding, and how to configure your account to allow for a password-less login with SSH keys.</p> <p>If you experience any problems, refer to our FAQ page which provides some solutions to common problems.</p>"},{"location":"registration_and_access/system_access/ssh_keys/","title":"SSH Keys","text":""},{"location":"registration_and_access/system_access/ssh_keys/#why-use-ssh-keys","title":"Why Use SSH Keys?","text":"<p>The Bastion Host uses two-factor authentication and will, by default, prompt you for a password and 2nd factor when you attempt to log in. As an alternative, you can use PKI (Public Key Authentication). This means you will not have to provide a password or Duo-authenticate for any future sessions. To do this, you will need to create an SSH Key on your local workstation and copy the public key to the <code>~/.ssh/authorized_keys</code> file in your HPC account on the bastion host.</p>"},{"location":"registration_and_access/system_access/ssh_keys/#create-and-use-ssh-keys","title":"Create and Use SSH Keys","text":"Linux/MacWindows <p>In a Terminal session on your local workstation:</p> <ol> <li> <p>Create a public-key pair:  <pre><code>$ ssh-keygen -t rsa\n</code></pre> You will be prompted to enter a passphrase. This is optional, but we strongly recommend that you do so.</p> </li> <li> <p>After running that command, you will have two new files on your local computer: <code>~/.ssh/id_rsa</code> and <code>~/.ssh/id_rsa.pub</code> <code>id_rsa</code> is your private key file. Do not share this with anybody! It is analagous to your password; anybody who has this file can impersonate you.     <code>id_rsa.pub</code> is your public key file. You will upload this onto any servers that you wish to automatically login to.</p> </li> <li> <p>Copy the public key to the Bastion Host (you will need to enter your password this one time):  <pre><code>$ ssh-copy-id netid@hpc.arizona.edu\n</code></pre> 3b. If your computer does not support the ssh-copy-id command, run the following commands: <pre><code>$ scp ~/.ssh/id_rsa.pub netid@hpc.arizona.edu:\n$ ssh netid@hpc.arizona.edu # (you will need to use your password this time)\n$ mkdir -p ~/.ssh &amp;&amp; cat ~/id_rsa.pub &gt;&gt; .ssh/authorized_keys &amp;&amp; rm ~/id_rsa.pub # On the server, copies the key into the appropriate file\n</code></pre></p> </li> </ol> <p>Now, logout and attempt to login to the server again. You should not be prompted for a password!</p> <p>To setup SSH keys on Windows with the PuTTy client, refer to the official PuTTy documentation.</p>"},{"location":"registration_and_access/system_access/ssh_keys/#using-ssh-keys-for-file-transfers","title":"Using SSH Keys for file transfers","text":"<p>Note that SSH Keys can also be used to avoid entering a password and 2nd factor when transferring files to to the cluster via the file transfer node (<code>filexfer.hpc.arizona.edu</code>) using command line programs like <code>scp</code> or <code>sftp</code>.  Follow the steps above (2-4 under the Mac and Linux instructions), except use <code>filexfer.hpc.arizona.edu</code> instead of <code>hpc.arizona.edu</code>. Note that you only need to generate the keys in Step 1 once. The same <code>~/.ssh/id_rsa.pub</code> file may be used to identify yourself to multiple hosts.</p>"},{"location":"registration_and_access/system_access/web_access/","title":"Web Access","text":"<p>Open OnDemand</p> Browser TerminalVirtual Desktop <p>Users can gain command line access to HPC through our OOD web interface as an alternative to using a local SSH Client. To use this interface:</p> <ol> <li>Log into Open OnDemand</li> <li>Go to the dropdown menu at the top of the screen and select <code>Clusters</code></li> <li> <p>Click <code>&gt;_Shell Access</code></p> <p></p> </li> <li> <p>This will put you on the command line on one of the login nodes where you may perform regular housekeeping work, submit jobs, or request an interactive session. By default, you will automatically be connected to Puma. To navigate to a different cluster, use the displayed shortcuts. </p> </li> </ol> <p>Users may also interact with a cluster using a virtual desktop interface. To do this:</p> <ol> <li> <p>Log into Open OnDemand and, under My Interactive Sessions, select Interactive Desktop under Desktops on the left-hand side of the page.</p> </li> <li> <p>A form will appear where you will select the target cluster, enter the amount of time you'd like to be allotted (in hours), the number of cores you need, your PI Group (if you are unsure what your group name is, you can check in https://portal.hpc.arizona.edu/portal/), and the queue. Once you've filled in your request, click Launch.</p> <p></p> </li> <li> <p>A window will appear with the status of your request. It will start in a Pending state and will switch to Running when your desktop session is ready. Click Launch Interactive Desktop to access your session.</p> <p></p> </li> <li> <p>That's it! You can now use the cluster with a Desktop interface</p> <p></p> </li> </ol>"},{"location":"registration_and_access/system_access/x11_forwarding/","title":"X11 Forwarding","text":"<p>X11 forwarding is a mechanism that allows a user to start up a remote application (e.g. VisIt or Matlab) and forward the application display to their local machine. The key to make forwarding work successfully is to include the <code>-X</code> flag at each login step. To check whether X11 forwarding is active, you may run the command:</p> <p><pre><code>$ echo $DISPLAY\n</code></pre> If it comes back blank, X11 forwarding is not enabled.</p> Mac/LinuxWindows Tips <ul> <li> <p>Mac users will want to install the additional software package XQuartz onto their machines to use X11 forwarding with HPC. </p> </li> <li> <p>On a Mac, if you get a blank response to <code>echo $DISPLAY</code>, you might need this line in your <code>~/.ssh/config</code> file: <code>ForwardX11Trusted yes</code></p> </li> <li> <p>Be aware forwarding X traffic does not work with the DEPRECATED menu interface enabled.  You should disable the menu option and use the hostname shortcuts instead.</p> </li> </ul> <p>Start a terminal session and connect as you typically would with an additional flag <code>-X</code> in your ssh command. Once you're connected to the bastion host, enter the name of the cluster you want to access, including the additional <code>-X</code> flag again. An example of this process is provided below: <pre><code>$ ssh -X netid@hpc.arizona.edu\nPassword:\nDuo two-factor login for netid\nEnter a passcode or select one of the following options:\n\n1. Duo Push to XXX-XXX-8969\n2. Phone call to XXX-XXX-8969\n3. Phone call to XXX-XXX-0502\n4. SMS passcodes to XXX-XXX-8969\n\nPasscode or option (1-4): 1\nSuccess. Logging you in...\nLast login:\nThis is a bastion host used to access the rest of the RT/HPC environment.\n\nType \"shell\" to access the job submission hosts for all environments\n-----------------------------------------            \n[netid@gatekeeper ~]$ echo $DISPLAY\nlocalhost:13.0\n\n[netid@gatekeeper ~]$ shell -X\n***\nThe default cluster for job submission is Puma\n***\nShortcut commands change the target cluster\n-----------------------------------------\nOcelote:\n$ ocelote\n(ocelote) $\nPuma:\n$ puma\n(puma) $\n\n(puma)[netid@junonia ~]$ echo $DISPLAY\nlocalhost:18.0\n</code></pre></p> <p>To use X11 forwarding on a Windows system, you will need to download an X11 display server such as Xming. </p> PuTTYMobaXterm <p>To enable X11 forwarding in PuTTY, go to SSH \u2192 X11 and select the box next to Enable X11 forwarding.</p> <p></p> <p>Once you've connected to the bastion host, connect to the login nodes with the an additional flag <code>-X</code>:</p> <pre><code>shell -X\n</code></pre> <p>To enable X11 forwarding in MobaXterm, open a new session, select SSH, and open Advanced SSH settings. Select the option below called X11-Forwarding.</p> <p></p> <p>Once you've connected to the bastion host, connect to the login nodes with the an additional flag <code>-X</code>: <pre><code>shell -X\n</code></pre></p>"},{"location":"registration_and_access/vpn/","title":"VPN","text":""},{"location":"registration_and_access/vpn/#overview","title":"Overview","text":"<p>A virtual private network (VPN) is a mechanism for creating a secure connection between a computer and a computing network using an insecure communication medium like the Internet. You can access the resources available within the network from your computer using a VPN.</p> <p>You will find the following VPN services useful for accessing some of the resources offered by the Research and Discovery Technologies:</p> <ul> <li>UA SSL VPN (<code>vpn.arizona.edu</code>): If you are not connected to the UA campus network you will need to connect to this VPN to access R-DAS.</li> <li>UA HPC VPN (<code>vpn.hpc.arizona.edu</code>): You will need to connect to this VPN to use graphical applications that need X11 forwarding with the HPC clusters.</li> </ul>"},{"location":"registration_and_access/vpn/#instructions-for-connecting","title":"Instructions for Connecting","text":"GUICLI <p>You can connect to the UArizona VPN services with the software Cisco Secure Client. It is available for Windows, Mac, and Linux distributions. On Linux distributions you might have a better experience with OpenConnect VPN (see CLI). Follow the UITS Knowledge Base guide for Windows, Mac, or Linux, to install Cisco Secure Client on your computer. The guide also shows how you can connect to the UA SSL VPN.</p> <p>Follow the steps below to connect to UA HPC VPN (the screenshots are from a Mac, but the experience is similar across OSs):</p> <ol> <li>Open Cisco Secure Client</li> <li>Enter <code>vpn.hpc.arizona.edu</code> in the address bar and click Connect.</li> <li>In the window that launches, enter your UArizona NetID as your Username and click OK.</li> <li>In the next window, enter your UArizona NetID password and click OK.</li> <li>In the window that launches, enter the NetID+ method you selected when you enrolled.</li> <li>Lastly, review the notice box and click Accept.</li> </ol> <p>Use of sudo</p> <p>Do not run any <code>sudo</code> commands on the HPC clusters when following the instructions below. These are strictly meant for your personal machines.</p> <p>You can connect to UArizona VPN services from the command line with OpenConnect VPN. To do this, you will need <code>sudo</code> privileges.</p> <p>OpenConnect VPN is available for Windows, Mac and Linux distributions, however installation on Windows can be difficult. On Windows, you might have a better experience with Cisco Secure Client (see GUI). You can find more information on platforms supported by OpenConnect from the project website. Select your operating system from the list below to view installation instructions:</p> MacLinux <p>Install with the Homebrew package manager: <code>brew install openconnect</code></p> <p>Follow the instructions from Open Build Service for your distribution.</p> <p>Once you have OpenConnect installed, you can connect to UArizona VPNs using the following:</p> UA SSL VPNUA HPC VPN <ol> <li>Open your terminal</li> <li>Enter <code>sudo openconnect vpn.arizona.edu</code></li> <li>A prompt will appear asking you to choose a VPN <code>GROUP</code>. Enter <code>1</code>.</li> <li>A prompt will appear asking you for your Username. Enter your UArizona NetID.</li> <li>A prompt will appear asking for your Password. Enter your UArizona NetID password. </li> <li>A second prompt will appear asking for your Password. Enter the NetID+ method you selected when you enrolled. <ol> <li>If you selected the Push method, then enter <code>push</code>.</li> <li>If you selected the SMS method, enter <code>sms</code>. If you do this, it will show that the login has failed and will ask you to reenter your Username, Password, and NetID+ method. For Username and Password do the same as before. For NetID+ method, enter the SMS passcode you received.</li> <li>If you selected the Passcode method, then enter your passcode. </li> </ol> </li> </ol> <ol> <li>Open your terminal</li> <li>Enter <code>sudo openconnect vpn.hpc.arizona.edu</code></li> <li>A prompt will appear asking you for your Username. Enter your UArizona NetID.</li> <li>A prompt will appear asking for your Password. Enter your UArizona NetID password. </li> <li>A second prompt will appear asking for your Password. Enter the NetID+ method you selected when you enrolled. <ol> <li>If you selected the Push method, then enter <code>push</code>.</li> <li>If you selected the SMS method, enter <code>sms</code>. If you do this, the prompt will appear again. Enter the SMS passcode that you received. </li> <li>If you selected the Passcode method, then enter your passcode.         </li> </ol> </li> </ol>"},{"location":"resources/allocations/","title":"Time Allocations","text":"<p>All University of Arizona Principal Investigators (PIs; aka Faculty) that register for access to the UA High Performance Computing (HPC) receive these free allocations on the HPC machines which is shared among all members of their team. Currently all PIs receive:</p> HPC Cluster Standard Allocation Time per Month per PI Windfall Puma 100,000 CPU Hours Unlimited but can be pre-empted Ocelote 70,000 CPU Hours Unlimited but can be pre-empted Elgato 7,000 CPU Hours Unlimited but can be pre-empted"},{"location":"resources/allocations/#how-allocations-are-charged","title":"How Allocations are Charged","text":"<p>The number of CPU hours a job consumes is determined by the number of CPUs it is allocated multiplied by its requested walltime. When a job is submitted, the CPU hours it requires are automatically deducted from the account. If the job ends early, the unused hours are automatically refunded.</p> <p>For example, a job requesting 50 CPUs for 10 hours will be charged 500 CPU hours. When the job is submitted, all 500 CPU hours are deducted from the user's account, however, if the job only runs for 5 hours and then completes, the unused 250 hours would be refunded.</p> <pre><code>graph LR\n  A[Request 50 CPUs&lt;br&gt;for 10 hours] --&gt; B[500 CPU hours&lt;br&gt;charged];\n  B --&gt; C[Job starts];\n  C --&gt; D[Job completes&lt;br&gt;after 8 hours];\n  D --&gt; E[100 CPU hours&lt;br&gt;refunded];</code></pre> <p>This accounting is the same regardless of which type of node you request. Standard, GPU, and high memory nodes are all charged using the same model and use the same allocation pool. If you find you are being charged for more CPUs that you are specifying in your submission script, it may be an issue with your job's memory request.</p> <p>Allocations are refreshed on the first day of each month. Unused hours from the previous month do not roll over.</p>"},{"location":"resources/allocations/#how-to-use-your-allocation","title":"How to Use Your Allocation","text":"<p>To use your allocation, you will include your account and partition information as a Slurm directive in your batch script. </p> Standard HoursWindfall HoursHigh Priority HoursQualified Hours <p>To use your group's standard hours, include the following in your batch script, replacing <code>&lt;PI GROUP&gt;</code> with your own group's name: <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=standard\n</code></pre></p> <p>To use preemptible Windfall hours, include the following in your batch script (do not include the <code>--account</code> directive): <pre><code>#SBATCH --partition=windfall\n</code></pre></p> <p>If your group has purchased compute resources, you will have access to a high priority allocation. To use these hours, an additional <code>--qos</code> flag must be supplied. Use the following syntax, replacing <code>&lt;PI GROUP&gt;</code> with your own group's name: <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=high_priority\n#SBATCH --qos=user_qos_&lt;PI GROUP&gt;\n</code></pre></p> <p>If your group has requested a temporary allocation increase via a special project, you may use these hours by supplying an additional <code>--qos</code> flag. Use the following syntax, replacing <code>&lt;PI GROUP&gt;</code> with your own group's name: <pre><code>#SBATCH --account=&lt;PI GROUP&gt;\n#SBATCH --partition=standard\n#SBATCH --qos=qual_qos_&lt;PI GROUP&gt;\n</code></pre></p>"},{"location":"resources/allocations/#how-to-find-your-remaining-allocation","title":"How to Find Your Remaining Allocation","text":"<p>To view your allocation's used, unused, and encumbered hours, use the command <code>va</code> in a terminal. For example: <pre><code>(elgato) [user@gpu5 ~]$ va\nWindfall: Unlimited\n\nPI: parent_974 Total time: 7000:00:00\n    Total used*: 1306:39:00\n    Total encumbered: 92:49:00\n    Total remaining: 5600:32:00\n    Group: group1 Time used: 862:08:00 Time encumbered: 92:49:00\n    Group: group2 Time used: 0:00:00 Time encumbered: 0:00:00\n\n*Usage includes all subgroups, some of which may not be displayed here\n</code></pre></p>"},{"location":"resources/compute_resources/","title":"Compute Resources","text":""},{"location":"resources/compute_resources/#compute-resources-available-by-cluster","title":"Compute Resources Available by Cluster","text":"<p>Before submitting a Slurm script, you must know (or at least have a general idea) of the resources needed for your job. This will tell you which type of node to request, how much memory, and other useful information that can be provided to the system via your batch script. A detailed list of Slurm batch flags are included below. </p> <p>Node Types</p> Node Type Description Standard CPU Node This is the general purpose node, designed to be used by the majority of jobs. High Memory CPU Node Similar to the standard nodes, but with significantly more RAM. There a only a few of them and they should only be requested for jobs that are known to require more RAM than is provided by standard CPU nodes. GPU Node Similar to the standard node, but with one or more GPUs available. The number of GPUs available per node is cluster-dependent. <p>Available Hardware by Cluster and Node Type</p> PumaOceloteElGato <p>Resources Available</p> Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 236 94 5gb 470gb - - - - High Memory 3 standard2 buy-in 94 32gb 3008gb - - - - GPU 8 standard7 buy-in 94 5gb 470gb 4 32gb 128gb 32 standard28 buy-in Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 400 28 6gb 168gb - - - - High Memory 1 48 41gb 1968gb - - - - GPU 46 28 8gb 224gb 1 16gb 16gb 46 Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node Standard 130 16 4gb 64gb"},{"location":"resources/compute_resources/#job-memory-and-cpu-count","title":"Job Memory and CPU Count","text":"<p>Job Memory and CPU Count are Correlated</p> <p>The memory your job is allocated is dependent on the number of CPUs you request.</p> <p>For example, on Puma standard nodes, you get 5G for each CPU you request. This means a standard job using 4 CPUs gets 5G/CPU \u00d7 4 CPUs = 20G of total memory. Each node has its own memory ratio that's dependent on its total memory \u00f7 total number of CPUs. A reference for all the node types, the memory ratios, and how to request each can be found in the Node Types/Example Resource Requests section above.</p> <p>What Happens if My Memory and CPU Requests Don't Match?</p> <p>Our systems are configured to try to help when your memory request does not match your CPU count.</p> <p>For example, if you request 1 CPU and 470G of memory on Puma, the system will automatically scale up your CPU count to 94 to ensure that you get your full memory requirements. This does not go the other way, so if you request less memory than would be provided by your CPU count, no adjustments are made. If you omit the <code>--memory</code> flag entirely, the system will use the memory ratio for the standard nodes on that cluster.</p> <p>Possible Problems You Might Encounter</p> <p>Be careful when using <code>--mem-per-cpu ratio</code>. If you use a higher value than a standard node ratio, you may inadvertently wind up in queue for a high memory node. On Puma there are three of these machines available for standard jobs and only one on Ocelote. This means the wait times are frequently longer than those for standard nodes. If you notice your job is in queue much longer than you would expect, check your job using job-history to ensure the memory ratio looks correct. Stick to using <code>--ntasks=N</code> and <code>--cpus-per-task=M</code> to request \\(N \u00d7 M\\) CPUs. Using the flag <code>-c</code> N to request CPUs has been found to cause problems with memory requests and may inadvertently limit you to ~4MB of total memory.</p> <ol> <li> <p>Where \\(1\\leq N \\leq 4\\) \u21a9</p> </li> </ol>"},{"location":"resources/compute_resources/compute_resources/","title":"Compute Resources","text":""},{"location":"resources/compute_resources/compute_resources/#compute-resources-available-by-cluster","title":"Compute Resources Available by Cluster","text":"<p>Before submitting a Slurm script, you must know (or at least have a general idea) of the resources needed for your job. This will tell you which type of node to request, how much memory, and other useful information that can be provided to the system via your batch script. A detailed list of Slurm batch flags are included below. </p> <p>Node Types</p> Node Type Description Standard CPU Node This is the general purpose node, designed to be used by the majority of jobs. High Memory CPU Node Similar to the standard nodes, but with significantly more RAM. There a only a few of them and they should only be requested for jobs that are known to require more RAM than is provided by standard CPU nodes. GPU Node Similar to the standard node, but with one or more GPUs available. The number of GPUs available per node is cluster-dependent. <p>Available Hardware by Cluster and Node Type</p> PumaOceloteElGato <p>Resources Available</p> Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 236 94 5gb 470gb - - - - High Memory 3 standard2 buy-in 94 32gb 3008gb - - - - GPU 8 standard7 buy-in 94 5gb 470gb 4 32gb 128gb 32 standard28 buy-in Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node GPUs/Node RAM/GPU GPU RAM/Node Total GPUs Standard 400 28 6gb 168gb - - - - High Memory 1 48 41gb 1968gb - - - - GPU 46 28 8gb 224gb 1 16gb 16gb 46 Node Type Number of Nodes CPUs/Node RAM/CPU CPU RAM/Node Standard 130 16 4gb 64gb"},{"location":"resources/compute_resources/compute_resources/#job-memory-and-cpu-count","title":"Job Memory and CPU Count","text":"<p>Job Memory and CPU Count are Correlated</p> <p>The memory your job is allocated is dependent on the number of CPUs you request.</p> <p>For example, on Puma standard nodes, you get 5G for each CPU you request. This means a standard job using 4 CPUs gets 5G/CPU \u00d7 4 CPUs = 20G of total memory. Each node has its own memory ratio that's dependent on its total memory \u00f7 total number of CPUs. A reference for all the node types, the memory ratios, and how to request each can be found in the Node Types/Example Resource Requests section above.</p> <p>What Happens if My Memory and CPU Requests Don't Match?</p> <p>Our systems are configured to try to help when your memory request does not match your CPU count.</p> <p>For example, if you request 1 CPU and 470G of memory on Puma, the system will automatically scale up your CPU count to 94 to ensure that you get your full memory requirements. This does not go the other way, so if you request less memory than would be provided by your CPU count, no adjustments are made. If you omit the <code>--memory</code> flag entirely, the system will use the memory ratio for the standard nodes on that cluster.</p> <p>Possible Problems You Might Encounter</p> <p>Be careful when using <code>--mem-per-cpu ratio</code>. If you use a higher value than a standard node ratio, you may inadvertently wind up in queue for a high memory node. On Puma there are three of these machines available for standard jobs and only one on Ocelote. This means the wait times are frequently longer than those for standard nodes. If you notice your job is in queue much longer than you would expect, check your job using job-history to ensure the memory ratio looks correct. Stick to using <code>--ntasks=N</code> and <code>--cpus-per-task=M</code> to request \\(N \u00d7 M\\) CPUs. Using the flag <code>-c</code> N to request CPUs has been found to cause problems with memory requests and may inadvertently limit you to ~4MB of total memory.</p> <ol> <li> <p>Where \\(1\\leq N \\leq 4\\) \u21a9</p> </li> </ol>"},{"location":"resources/data_center/","title":"Research Data Center","text":""},{"location":"resources/data_center/#central-computing-facilities","title":"Central Computing Facilities","text":"<p>The University of Arizona (UArizona) has two data center facilities available to assist researchers on campus:</p> <ul> <li> <p>Research Data Center (RDC): 1200 ft<sup>2</sup> raised floor data center designed for water-cooled racks dedicated to centrally managed research computing systems</p> </li> <li> <p>Co-location Data Center:  1900 ft<sup>2</sup> of raised floor data center space for air-cooled research co-located equipment</p> </li> </ul> <p>These campus data centers are managed by the UArizona\u2019s central computing organization, University Information Technology Services (UITS). Other than installation costs no bandwidth or other recurring charges will be levied for co-location of research systems in these facilities.</p>"},{"location":"resources/data_center/#power-and-cooling","title":"Power and Cooling","text":"<p>The UITS data centers are both located in the Computer Center with 1192kW of battery backup and a 1750kW generator for backup power.</p> <p>Cooling in the RDC is both in-rack cooling with chilled water heat exchangers and Computer Room Air Conditioning (CRAC) units. The co-location Data Center is cooled with chilled water CRAC units and dual cool CRACs. Both data centers are equipped with 18\u201d raised floors that allow for full coverage of cooling to all the equipment, and leak detection systems in the subfloor.</p>"},{"location":"resources/data_center/#fire-suppression","title":"Fire Suppression","text":"<p>The fire suppression system is a multi-tiered defense with clean agent compressed gas, dry pipe pre-action sprinkler and EPO (Emergency Power Off) systems zoned to deploy in affected areas.  For prevention storage of combustible materials such as cardboard, flammable liquids and other hazardous materials is prohibited within the data centers.</p>"},{"location":"resources/data_center/#security","title":"Security","text":"<p>UITS data centers have badge swipe access with two-factor authentication and video surveillance in data center and surrounding building. The data centers are monitored by a co-located 24/7 Operations and dedicated infrastructure team. With automated environmental and system monitoring to assist with issue triaging and escalation. All personnel with swipe access to the data centers have undergone background checks and are required to be US Citizens.</p>"},{"location":"resources/data_center/#network-and-connectivity","title":"Network and Connectivity","text":"<p>In addition to direct connections to commodity Internet carriers, the UArizona connection to Internet2 is through the Sun Corridor Network \u2013 an Arizona regional network established through a collaborative effort sponsored by the Arizona Board of Regents\u2019 (ABOR) three state universities \u2013 Arizona State University (ASU), Northern Arizona University (NAU), and the University of Arizona (UArizona). The Sun Corridor Network provides advanced networking services beyond those available from the individual Arizona Universities and builds an environment essential to leading-edge education, research, and the sharing of digital communications resources, network services, and applications among eligible members.</p> <p>The UArizona manages and operates the Sun Corridor Network. The current connection from UArizona to Sun Corridor is dual 10G, while Sun Corridor is connected to Internet2 via dual 100G connections in Tucson and Phoenix. Network traffic to Internet2 is automatically routed via the Internet2 infrastructure; no action or configuration by the user is required to take advantage of Internet2 connectivity.</p> <p>The UArizona\u2019s Research Data Center has 40GB/s connections to the UArizona core with all the servers connected by 1GB/s or 10GB/s connections.  In-rack switching is enabled with Cisco FEX switches used in a top of rack configuration in both data centers with servers connected to two different switches for (N + 1) redundancy.</p> <p>In addition to direct connectivity to the campus network at the building level, researchers have an opportunity to use a Science DMZ for fast and high volume data transfers to outside collaborating institutions. The Science DMZ is deployed at the University of Arizona network perimeter, outside border firewalls, and is directly connected to Sun Corridor via 10G link. It is secured via static access lists deployed at the Sun Corridor router without impact to performance. There are two high-performance Data Transfer Nodes (DTNs) deployed in the Science DMZ. DTN\u2019s are dedicated servers with hardware and operating system optimized for high speed transfer.</p>"},{"location":"results/","title":"UArizona HPC Documentation","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/","title":"Index","text":"<p>foo</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_and_gnu_parallel/","title":"Array Job with GNU Parallel","text":"<p>Click here to download example files</p> <p>This script combines the methods of the basic array job and the basic parallel job to execute a large number of jobs.</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_and_gnu_parallel/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>Sometimes you need to run a lot of jobs. More than can be reasonably accomplished using arrays since submitting thousands of jobs can be a problem for the system, and GNU Parallel can be challenging to make work in a multi-node environment. In this case, we can combine the forces of GNU Parallel and array jobs to distribute a chunk of tasks across multiple nodes where GNU Parallel will execute them</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_and_gnu_parallel/#example","title":"Example","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Sample_Array_With_GNU_Parallel\n#SBATCH --ntasks=94\n#SBATCH --nodes=1                    \n#SBATCH --time=00:05:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-2\n\nmodule load parallel\nBLOCK_SIZE=200\nseq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE)) | parallel echo \"JOB ID: $SLURM_JOB_ID HOST NODE: $HOSTNAME EXAMPLE COMMAND: ./executable input_{}\"\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_and_gnu_parallel/#script-breakdown","title":"Script Breakdown","text":"<p>Like with the basic parallel example, GNU Parallel is accessible as a module. The general goal here for illustration purposes is to set up a \"block size\". This is the number of tasks GNU Parallel will be executing in each subjob</p> <pre><code>BLOCK_SIZE=200\n</code></pre> <p>In this case, we're asking for 200 tasks per subjob and since we're submitting an array job, that totals 400 tasks. The array indices then used to differentiate tasks. <code>seq n m</code> generates a sequence of integers from <code>n</code> to <code>m</code> (inclusive).</p> <p><code>SLURM_ARRAY_TASK_ID</code> in this case is either 1 or 2, depending on the subjob, so combined with <code>BLOCK_SIZE</code>:</p> <p>Subjob 1: </p> <pre><code>seq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE))\n# Doing the math --&gt; seq 1*200-200+1 1*200 --&gt; seq 1 200\n</code></pre> <p>Subjob 2:  <pre><code>seq $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID*$BLOCK_SIZE))\n# Doing the math --&gt; seq 2*200-200+1 2*200 --&gt; seq 201 400\n</code></pre></p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_and_gnu_parallel/#script-submission-command","title":"Script Submission Command","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-and-Parallel.slurm \nSubmitted batch job 1693973\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_and_gnu_parallel/#output-files","title":"Output Files","text":"<pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1693973_1.out  slurm-1693973_2.out\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_and_gnu_parallel/#file-contents","title":"File Contents","text":"<p>A total of 400 tasks were executed using an array job with two subjobs on two nodes, r2u07n1 and r2u13n2. To view the full contents of the output file, download the example above. </p> <pre><code>(puma) [netid@junonia ~]$ head *.out\n==&gt; slurm-1693973_1.out &lt;==\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_1\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_2\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_3\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_4\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_5\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_6\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_7\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_8\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_9\nJOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_10\n\n==&gt; slurm-1693973_2.out &lt;==\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_201\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_202\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_203\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_204\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_205\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_206\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_207\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_208\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_209\nJOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_210\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/","title":"Array Job With Text Filenames","text":"<p>Click here to download example files</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>If you want to run multiple jobs where each opens a different file to analyze but the naming scheme isn't conducive to automating the process using simple array indices (i.e., 1.txt, 2.txt, ...)</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#example","title":"Example","text":""},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#submission-script","title":"Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Array-Read-Filenames\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-4\n\nCurrentFile=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\"\necho \"JOB NAME: $SLURM_JOB_NAME, JOB ID: $SLURM_JOB_ID, EXAMPLE COMMAND: ./executable -o output${SLURM_ARRAY_TASK_ID} ${CurrentFile}\"\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#input-file","title":"Input File","text":"<p>For this example, you'll want to have a file called InputFiles in your working directory. This will contain one filename per line. Contents: <pre><code>SRR2309587.fastq\nSRR3050489.fastq\nSRR305356.fastq\nSRR305p0982.fastq\n</code></pre></p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#script-breakdown","title":"Script Breakdown","text":"<p>For each of the four subjobs, we'll make use of <code>SLURM_ARRAY_TASK_ID</code> to pull the line number (line numbers 1 to 4) from InputFiles:</p> <pre><code>CurrentFile=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\"\n</code></pre> <p>We will print a sample command that includes our filename to verify that everything is working as expected for demonstration purposes:</p> <pre><code>echo \"JOB NAME: $SLURM_JOB_NAME, JOB ID: $SLURM_JOB_ID, EXAMPLE COMMAND: ./executable -o output${SLURM_ARRAY_TASK_ID} ${CurrentFile}\"\n</code></pre> <p>To generate your own InputFile, you can either manually add your filenames or can automate the process, for example if you have all your files in a single location:</p> <pre><code>$ ls *fastq &gt; InputFiles\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#script-submission-command","title":"Script Submission Command:","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-Read-Filenames.slurm \nSubmitted batch job 1694071\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#output-files","title":"Output Files","text":"<p>Each of the subjobs in the array will output its own file of the form <code>slurm-&lt;job_id&gt;_&lt;array_id&gt;.out</code> as seen below:</p> <pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1694071_1.out  slurm-1694071_2.out  slurm-1694071_3.out\nslurm-1694071_4.out\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_filenames/#file-contents","title":"File Contents:","text":"<pre><code>(puma) [netid@junonia ~]$ cat *.out | grep fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694072, EXAMPLE COMMAND: ./executable -o output1 SRR2309587.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694073, EXAMPLE COMMAND: ./executable -o output2 SRR3050489.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694074, EXAMPLE COMMAND: ./executable -o output3 SRR305356.fastq\nJOB NAME: Array-Read-Filenames, JOB ID: 1694071, EXAMPLE COMMAND: ./executable -o output4 SRR305p0982.fastq\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/","title":"Array Jobs With Multiple Input Parameters","text":"<p>Click here to download example files</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/#example","title":"Example","text":"<p>This script demonstrates how to feed parameters to different subjobs in an array by pulling them from an input file, e.g.:</p> <ul> <li>Job 1: <code>./executable job1_variable1 job1_variable2 job1_variable3</code></li> <li>Job 2: <code>./executable job2_variable1 job2_variable2 job2_variable3</code> etc.</li> </ul>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/#submission-script","title":"Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=Array-Read-Parameters\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array=1-10\n\nread first_parameter second_parameter third_parameter &lt;&lt;&lt; $( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputParameters )\necho \"Job ID: $SLURM_JOB_ID ; Host Node : $HOSTNAME ; Sample Command : ./executable $first_parameter $second_parameter $third_parameter\"\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/#input-file","title":"Input file","text":"<pre><code>job1_param1 job1_param2 job1_param3\njob2_param1 job2_param2 job2_param3\njob3_param1 job3_param2 job3_param3\njob4_param1 job4_param2 job4_param3\njob5_param1 job5_param2 job5_param3\njob6_param1 job6_param2 job6_param3\njob7_param1 job7_param2 job7_param3\njob8_param1 job8_param2 job8_param3\njob9_param1 job9_param2 job9_param3\njob10_param1 job10_param2 job10_param3\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/#script-breakdown","title":"Script Breakdown","text":"<p>The line number that corresponds with the job's <code>SLURM_ARRAY_TASK_ID</code> is read in and parsed to extract the input parameters. The parameters here should be space-delimited (of course, you can modify your script to change these specifications). There are three parameters per line that are assigned to the variables on the left:</p> <pre><code>read first_parameter second_parameter third_parameter &lt;&lt;&lt; $( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputParameters )\n</code></pre> <p>A sample command is printed along with job information for demonstration purposes:</p> <pre><code>echo \"Job ID: $SLURM_JOB_ID ; Host Node : $HOSTNAME ; Sample Command : ./executable $first_parameter $second_parameter $third_parameter\"\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/#script-submission-command","title":"Script Submission Command","text":"<pre><code>(puma) [netid@junonia ~]$ sbatch Array-Read-Parameters.slurm \nSubmitted batch job 1694093\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/#output-files","title":"Output Files","text":"<pre><code>(puma) [netid@junonia ~]$ ls *.out\nslurm-1694093_10.out  slurm-1694093_1.out  slurm-1694093_2.out\nslurm-1694093_3.out   slurm-1694093_4.out  slurm-1694093_5.out\nslurm-1694093_6.out   slurm-1694093_7.out  slurm-1694093_8.out\nslurm-1694093_9.out\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/array_read_parameters/#file-contents","title":"File Contents","text":"<pre><code>(puma) [netid@junonia ~]$ cat *.out | grep param\nJob ID: 1694093 ; Host Node : r1u03n2 ; Sample Command : ./executable job10_param1 job10_param2 job10_param3\nJob ID: 1694094 ; Host Node : r1u03n1 ; Sample Command : ./executable job1_param1 job1_param2 job1_param3\nJob ID: 1694095 ; Host Node : r1u03n1 ; Sample Command : ./executable job2_param1 job2_param2 job2_param3\nJob ID: 1694096 ; Host Node : r1u03n1 ; Sample Command : ./executable job3_param1 job3_param2 job3_param3\nJob ID: 1694097 ; Host Node : r1u03n1 ; Sample Command : ./executable job4_param1 job4_param2 job4_param3\nJob ID: 1694098 ; Host Node : r1u03n1 ; Sample Command : ./executable job5_param1 job5_param2 job5_param3\nJob ID: 1694099 ; Host Node : r1u03n1 ; Sample Command : ./executable job6_param1 job6_param2 job6_param3\nJob ID: 1694100 ; Host Node : r1u03n1 ; Sample Command : ./executable job7_param1 job7_param2 job7_param3\nJob ID: 1694101 ; Host Node : r1u03n2 ; Sample Command : ./executable job8_param1 job8_param2 job8_param3\nJob ID: 1694102 ; Host Node : r1u03n2 ; Sample Command : ./executable job9_param1 job9_param2 job9_param3\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_array/","title":"Basic Array Job","text":"<p>Click to download example files</p> <p>Array jobs are used to execute the same script multiple times with different input.</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_array/#what-problem-does-this-help-fix","title":"What problem does this help fix?","text":"<p>To execute multiple analyses, a user may be tempted to submit jobs with a scripted loop, e.g.: <pre><code>for i in $( seq 1 10 ); do sbatch script.slurm &lt;submission options&gt; ; done\n</code></pre></p> <p>For a large number of analyses, this isn't a good solution because it submits too many jobs too quickly and overloads the scheduler. Instead, an array job can be used to achieve the same ends.</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_array/#example-submission-script","title":"Example Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n#SBATCH --array 1-5\n\necho \"./sample_command input_file_${SLURM_ARRAY_TASK_ID}.in\"\n ```\n\n## Script Breakdown\n\nWhat differentiates the script above from standard submissions is the ```--array``` directive. This is what tells Slurm  that you're submitting an array. Following this flag, you will specify the number of jobs you wish to run. In this case, we're running 5:\n\n```bash\n#SBATCH --array 1-5\n</code></pre> <p>Each job in the array has its own associated environment variable <code>SLURM_ARRARY_TASK_ID</code> that can be used to differentiate subjobs. To demonstrate how we can use each of these to read in different input files, we'll print a sample command:</p> <pre><code>echo \"./sample_command input_file_${SLURM_ARRAY_TASK_ID}.in\"\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_array/#script-submission","title":"Script Submission","text":"<pre><code>(ocelote) [netid@junonia ~]$ sbatch basic_array_job.slurm \nSubmitted batch job 73958\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_array/#output-files","title":"Output Files","text":"<p>Each of the subjobs in the array will produce its own output file of the form <code>slurm_jobid_arrayid.out</code> as seen below:</p> <pre><code>(ocelote) [netid@junonia ~]$ ls\nslurm-73958_1.out  slurm-73958_2.out      slurm-73958_3.out  slurm-73958_4.out\nslurm-73958_5.out  basic_array_job.slurm\n</code></pre> <p>For more information on naming Bash files, see our online documentation</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_array/#file-contents","title":"File Contents:","text":"<p>Below is a concatenation of the job's output files. Notice how the array indices function to differentiate the input files in the sample command:</p> <pre><code>(ocelote) [netid@junonia ~]$ cat slurm-73958_* | grep sample\n./sample_command input_file_1.in\n./sample_command input_file_2.in\n./sample_command input_file_3.in\n./sample_command input_file_4.in\n./sample_command input_file_5.in\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_gnu_parallel_job/","title":"Basic Parallel Job","text":"<p>Click here to download example files</p> <p>This example demonstrates how to parallelize multiple tasks within one job using gnu parallel</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_gnu_parallel_job/#example","title":"Example","text":"<pre><code>#!/bin/bash\n#SBATCH --ntasks=28\n#SBATCH --nodes=1             \n#SBATCH --time=00:01:00   \n#SBATCH --partition=standard\n#SBATCH --account=YOUR_GROUP\n\nmodule load parallel\nseq 1 100 | parallel 'DATE=$( date +\"%T\" ) &amp;&amp; sleep 0.{} &amp;&amp; echo \"Host: $HOSTNAME ; Date: $DATE; {}\"'\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_gnu_parallel_job/#script-breakdown","title":"Script Breakdown","text":"<p>In this case, we'll make use of a full node with GNU Parallel which is available as a module:</p> <pre><code>module load parallel\n</code></pre> <p>The meat of the command lies here:</p> <pre><code>seq 1 100 | parallel 'DATE=$( date +\"%T\" ) &amp;&amp; sleep 0.{} &amp;&amp; echo \"Host: $HOSTNAME ; Date: $DATE; {}\"'\n</code></pre> <p><code>seq 1 100</code> generates a list between 1 and 100 (inclusive), and we pipe that into a parallel command which will generate one task per element (so 100 tasks).</p> <p>GNU Parallel will find the space on our node as it works through the relevant tasks. </p> <p>Inside the command: <code>DATE=$( date + \"%T\" )</code> sets <code>DATE</code> so we can visualize tasks and when they're being executed</p> <p><code>sleep 0.{}</code> forces each task to sleep for <code>0.n</code> seconds, where <code>n</code> is the input integer from the <code>seq</code> command. This means, for example, the 2nd task will wait longer than the 10th task, as can be seen in the output file. This is used to demonstrate that these tasks are being executed in parallel. </p> <p><code>echo \"HOST: $HOSTNAME ; Date: $DATE; {}\"</code> prints out information about the task. <code>{}</code> is piped input which, in this case, is an integer generated by <code>seq</code> between 1 and 100. </p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_gnu_parallel_job/#submitting-the-script","title":"Submitting the Script","text":"<pre><code> (ocelote) [netid@junonia ~]$ sbatch basic-parallel-job.slurm \nSubmitted batch job 74027\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_gnu_parallel_job/#output-files","title":"Output Files","text":"<p>Since this isn't an array job, there will only be one output file:</p> <pre><code>(ocelote) [netid@junonia ~]$ ls\nslurm-74027.out  basic-parallel-job.slurm\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/array_and_parallel_examples/basic_gnu_parallel_job/#file-contents","title":"File Contents","text":"<p>For the full file contents, download the example above <pre><code>(ocelote) [netid@junonia ~]$ head slurm-74027.out \nHost: i10n18 ; Date: 16:45:55; 1\nHost: i10n18 ; Date: 16:45:55; 10\nHost: i10n18 ; Date: 16:45:55; 11\nHost: i10n18 ; Date: 16:45:55; 12\nHost: i10n18 ; Date: 16:45:55; 13\nHost: i10n18 ; Date: 16:45:55; 14\nHost: i10n18 ; Date: 16:45:55; 15\nHost: i10n18 ; Date: 16:45:55; 16\nHost: i10n18 ; Date: 16:45:55; 17\nHost: i10n18 ; Date: 16:45:55; 2\n</code></pre></p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/","title":"SLURM Job Dependencies Example","text":"<p>Click here to download the example</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#overview","title":"Overview","text":"<p>Sometimes projects need to be split up into multiple parts where each step is dependent on the step (or several steps) that came before. SLURM dependencies are a way to automate this process. </p> <p>In this example, we'll create a number of three-dimensional plots using Python and will combine them into a gif as the last step. A job dependency is a good solution in this case since the job that creates the gif is dependent on all the images being present.</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#data-structure","title":"Data structure","text":"<p>We'll try to keep things in order by partitioning our data, output, and images in distinct directories. These directories and files can be downloaded by clicking the button at the top of the page.</p> <pre><code>(elgato) [user@wentletrap volcano]$ tree\n.\n\u251c\u2500\u2500 create_gif.slurm\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.csv\n\u251c\u2500\u2500 generate_frames.slurm\n\u251c\u2500\u2500 images\n\u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 archives\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurm_files\n\u251c\u2500\u2500 submit-gif-job\n\u2514\u2500\u2500 volcano.py\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#scripts","title":"Scripts","text":""},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#python-script","title":"Python script","text":"<p>The Python example script was pulled and modified from the Python graph gallery and the CSV file used to generate the image was downloaded from: https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv</p> <p>Below you'll notice one modification to the original script: <code>n = int(sys.argv[1])</code>. We're going to execute this in an array job and will be importing the array indices (integers <code>n</code> where <code>1 \u2264 n \u2264 360</code>) into this script as arguments so that we can manipulate the viewing angle (<code>ax.view_init(30, 45 + n)</code>). Each frame will be slightly different and, when combined into a gif, will allow us to execute a full rotation of the 3D volcano plot. </p> <pre><code>#!/usr/bin/env python3\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport sys\n\nn = int(sys.argv[1]) # &lt;- We'll import an array index to rotate our image\n\n# Original example from: https://www.python-graph-gallery.com/3d/\n# CSV available from   : https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv\ndata = pd.read_csv(\"data/volcano.csv\")\n\n# Transform it to a long format\ndf=data.unstack().reset_index()\ndf.columns=[\"X\",\"Y\",\"Z\"]\n\n# And transform the old column name in something numeric\ndf['X']=pd.Categorical(df['X'])\ndf['X']=df['X'].cat.codes\n\n# Make the plot\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.set_axis_off()\nax.plot_trisurf(df['Y'], df['X'], df['Z'], cmap=plt.cm.viridis, linewidth=0.2)\nax.view_init(30, 45 + n)\nplt.savefig('images/image%s.png'%n,format='png',transparent=False)\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#slurm-script-to-generate-images","title":"Slurm Script to Generate Images","text":"<p>This is the job where we will generate all of our images. In each step, we will pass our array index to our python script to determine the viewing angle of our plot. </p> <p>Script: <code>generate_frames.slurm</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:05:00\n#SBATCH --job-name=generate_frames\n#SBATCH -o output/slurm_files/%x-%A.out\n#SBATCH -e output/slurm_files/%x-%A.err\n#SBATCH --open-mode=append\n#SBATCH --array=1-360\n\npython3 volcano.py $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#slurm-script-to-combine-frames-into-gif","title":"SLURM script to combine frames into gif","text":"<p>Once each frame has been generated, we'll use ffmpeg to combine our images into a gif and will clean up our workspace. The bash script shown in the next section is what will ensure that this script isn't run until the array has completed. </p> <p>Script: <code>create_gif.slurm</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --account=hpcteam\n#SBATCH --partition=standard\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --time=00:10:00\n#SBATCH --job-name=make_gif\n#SBATCH -o output/slurm_files/%x-%j.out\n#SBATCH -e output/slurm_files/%x-%j.err\n\nmodule load ffmpeg \nffmpeg -framerate 25 -i $PWD/images/image%d.png -r 30  -b 5000k volcano.mp4\nffmpeg -i volcano.mp4 -loop 0 -vf scale=400:240 volcano.gif\nrm volcano.mp4\ncd images\nDATE_FORMAT=$(date +%m-%d-%Y.%H:%M:%S)\ntar czvf volcano-images-${DATE_FORMAT}.tar.gz image*.png\nmv *tar.gz ../output/archives\nrm -rf ./*.png\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#script-to-automate-job-submissions","title":"Script to Automate Job Submissions","text":"<p>This simple bash script is what implements the SLURM job dependency magic. Each step is described in detail below.</p> <p>Script: <code>submit-gif-job</code></p> <pre><code>#!/bin/bash\n\nprintf \"Submitting job to generate images\\n\"\njobid=$(sbatch --parsable generate_frames.slurm)\n\nprintf \"Job submitted with ID: $jobid\\n\\n\"\n\nprintf \"Submitting job dependency. Combines images into a gif\\n\"\nsbatch --dependency=afterany:$jobid create_gif.slurm \n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#step-by-step","title":"Step-by-step:","text":"<p>1) <code>jobid=$(sbatch --parsable generate_frames.slurm)</code></p> <p>In this case, we're capturing the job ID output from our array submission. Typically, when you submit a SLURM job without arguments, you get back something that looks like:  <pre><code>(elgato) [user@wentletrap ~]$ sbatch example.slurm \nSubmitted batch job 448243\n</code></pre> The parsable option is what reduces this to simply the job ID and allows us to easily capture it: <pre><code>(elgato) [user@wentletrap ~]$ sbatch --parsable example.slurm \n448244\n</code></pre> As a general comment, when you run something like: <pre><code>VAR=$(command)\n</code></pre> You are running <code>command</code> and setting the variable <code>VAR</code> to the output. In the specifc case of our bash script, we've set the bash variable <code>jobid</code> to the output of our <code>sbatch --parsable</code> command. </p> <p>2) <code>sbatch --dependency=afterany:$jobid create_gif.slurm</code></p> <p>Now that we have the Job ID, we'll submit the next job with a dependency flag: <code>--dependency=afterany:$jobid</code>. </p> <p>The <code>dependency</code> option tells the scheduler that this job should not be run until the job with Job ID <code>$jobid</code> has completed. The <code>afterany</code> specifies that the exit status of the previous job does not matter. Other options are <code>afterok</code> (meaning only execute the dependent job if the previous job ended successfully) or <code>afternotok</code> (meaning only execute if the previous job terminated abnormally, e.g. was cancelled or failed). You might consider setting up multiple job dependencies that depend on the previous job's exit status. </p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#submitting-the-jobs","title":"Submitting the Jobs","text":"<p>Once we've gotten everything set up, it's time to execute our workflow. We can check our jobs once we've run our bash script. In this case, while the array job used to generate the different image frames is running, the <code>make_gif</code> job will sit in queue with the reason <code>(Dependency)</code> indicating that it is waiting to run until its dependency has been satisfied. </p> <pre><code>(elgato) [user@wentletrap volcano]$ bash submit-gif-job \nSubmitting job to generate images\nJob submitted with ID: 447878\n\nSubmitting job dependency. Combines images into a gif file\nSubmitted batch job 447879\n\n(elgato) [user@wentletrap volcano]$ squeue --user user\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n    447878_[9-360]  standard generate     user PD       0:00      1 (None)\n            447879  standard make_gif     user PD       0:00      1 (Dependency)\n          447878_1  standard generate     user  R       0:02      1 cpu16\n          447878_2  standard generate     user  R       0:02      1 cpu37\n          447878_3  standard generate     user  R       0:02      1 cpu37\n          447878_4  standard generate     user  R       0:02      1 cpu37\n          447878_5  standard generate     user  R       0:02      1 cpu37\n          447878_6  standard generate     user  R       0:02      1 cpu37\n          447878_7  standard generate     user  R       0:02      1 cpu37\n          447878_8  standard generate     user  R       0:02      1 cpu37\n</code></pre> <p>Once the job has completed, you should see something that looks like the following structure with output files: <pre><code>(elgato) [user@wentletrap volcano]$ tree\n.\n\u251c\u2500\u2500 create_gif.slurm\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.csv\n\u251c\u2500\u2500 generate_frames.slurm\n\u251c\u2500\u2500 images\n\u251c\u2500\u2500 output\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 archives\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano-images-10-25-2022.12:52:19.tar.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 volcano.gif\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 slurm_files\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 generate_frames-447878.err\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 generate_frames-447878.out\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 make_gif-447879.err\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 make_gif-447879.out\n\u251c\u2500\u2500 submit-gif-job\n\u2514\u2500\u2500 volcano.py\n\n6 directories, 11 files\n</code></pre></p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/job_dependencies/#output","title":"Output","text":"<p>If everything is successful, there should be a gif of a rotating volcano under <code>./output/gifs/volcano.gif</code></p> <p></p> <p> </p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/pipefail/","title":"Using a Pipefail","text":"<p>This script uses a pipefail to kill a SLURM job in the event of a failure at any point in the pipeline rather than continuing on to the next step.</p>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/pipefail/#example-submission-script","title":"Example Submission Script","text":"<pre><code>#!/bin/bash\n#SBATCH --account=YOUR_GROUP\n#SBATCH --partition=standard\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --time=00:01:00\n\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import foo\"\n\nset -oe pipefail # &lt;- kills the batch script on the next error it encounters\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import bar\"\nsingularity exec --nv /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 -c \"import baz\"\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/pipefail/#job-submission","title":"Job Submission","text":"<pre><code>[user@cpu37 fail_test]$ sbatch pipefail.slurm\nSubmitted batch job 361837\n</code></pre>"},{"location":"running_jobs/batch_jobs/example_batch_jobs/general_examples/pipefail/#output","title":"Output","text":"<p>Notice that before the pipefail is set, the script moves on to subsequent commands (trying to import bar). After the pipefail is set, the job exits when it can't import bar and never tries to import baz. </p> <pre><code>[user@cpu37 fail_test]$ cat slurm-361837.out\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'foo'\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'bar'\n</code></pre>"},{"location":"running_jobs/batch_jobs/quick_start/","title":"Index","text":"<p>foo</p>"},{"location":"running_jobs/batch_jobs/slurm_documentation/","title":"Slurm Documentation","text":"<p>This page includes important system commands, batch job directives, environment variables, reason codes, among others to help you get started. For more detailed information </p> <p>Official SchedMD User Documentation</p>"},{"location":"running_jobs/batch_jobs/slurm_documentation/#uarizona-system-commands","title":"UArizona System Commands","text":"Command Purpose Example <code>va</code> Displays your group membership, your account usage, and CPU allocation. Short for \"view allocation\" <code>va</code> <code>interactive</code> Shortcut for quickly requesting an interactive job. Use <code>interactive --help</code> to get full usage <code>interactive -a $GROUP_NAME</code> <code>job-history</code> Retrieves a running or completed job's history in a user-friendly format <code>job-history $JOBID</code> <code>seff</code> Retrieves a completed job's memory and CPU efficiency <code>seff $JOBID</code> <code>past-jobs</code> Retrieves past jobs run by user. Can be used with option <code>-d N</code> to search for jobs run in the past <code>N</code> days <code>past-jobs -d 5</code> <code>job-limits</code> View your group's job resource limits and current usage. <code>job-limits $GROUP</code> <code>nodes-busy</code> Display a visualization of nodes on a cluster and their usage <code>nodes-busy --help</code> <code>system-busy</code> Display a text-based summary of a cluster's usage <code>system-busy</code>"},{"location":"running_jobs/batch_jobs/slurm_documentation/#slurm-commands","title":"Slurm Commands","text":"Command Purpose Example <code>sbatch</code> Submits a batch script for execution <code>sbatch script.slurm</code> <code>srun</code> Run parallel jobs. Can be in place of <code>mpirun</code>/<code>mpiexec</code>. Can be used interactively as well as in batch scripts <code>srun -n 1 --mpi=pmi2 a.out</code> <code>salloc</code> Requests a session to  work on a compute node interactively see: Interactive Sessions <code>squeue</code> Checks the status of pending and running jobs <code>squeue --job $JOBID</code> <code>scancel</code> Cancel a running or pending job <code>scancel $JOBID</code> <code>scontrol hold</code> Place a hold on a job to prevent it from being executed <code>scontrol hold $JOBID</code> <code>scontrol release</code> Releases a hold placed on a job allowing it to be executed <code>scontrol release $JOBID</code>"},{"location":"running_jobs/batch_jobs/slurm_documentation/#slurm-batch-job-directives","title":"Slurm Batch Job Directives","text":"Command Purpose <code>#SBATCH --account=group_name</code> Specify the account where hours are charged. Don't know your group name? Run the command <code>va</code> to see which groups you belong to <code>#SBATCH --partition=partition_name</code> Set the job partition. This determines your job's priority and the hours charged <code>#SBATCH --time=DD-HH:MM:SS</code> Set the job's runtime limit in days, hours, minutes, and seconds. A single job cannot exceed 10 days or 240 hours <code>#SBATCH --nodes=N</code> Allocate <code>N</code> nodes to your job.For non-MPI enabled jobs, this should be set to \"\u2013-nodes=1\" to ensure access to all requested resources and prevent memory errors <code>#SBATCH --ntasks=N</code> ntasks specifies the number of tasks (or processes) the job will run. For MPI jobs, this is the number of MPI processes.  Most of the time, you can use ntasks to specify the number of CPUs your job needs. However, in some odd cases you might run into issues. For example, see: Using Matlab <code>#SBATCH --cpus-per-task=M</code> By default, you will be allocated one CPU/task. This can be increased by including the additional directive <code>--cpus-per-task</code>. The number of CPUs a job is allocated is <code>cpus/task * ntasks</code> <code>#SBATCH --mem=Ngb</code> Select <code>N</code> GB of memory per node. If <code>gb</code> is not included, this value defaults to MB. Directives <code>--mem</code> and <code>--mem-per-cpu</code> are mutually exclusive. <code>#SBATCH --mem-per-cpu=Ngb</code> Select <code>N</code> GB of memory per CPU. If \"gb\" is not included, this value defaults to MB. <code>#SBATCH --gres=gpu:N</code> Optional: Request <code>N</code> GPUs. <code>#SBATCH --gres=gpu:ampere:N</code> Optional: Request <code>N</code> A100 GPUs. <code>#SBATCH --gres=gpu:volta:N</code> Optional: Request <code>N</code> V100s GPUs. <code>#SBATCH --constraint=hi_mem</code> Optional: Request a high memory node (Ocelote and Puma only). <code>#SBATCH --array=N-M</code> Submits an array job from indices N to M <code>#SBATCH --job-name=JobName</code> Optional: Specify a name for your job. This will not automatically affect the output filename. <code>#SBATCH -e output_filename.err</code><code>#SBATCH -o output_filename.out</code> Optional: Specify output filename(s). If <code>-e</code> is missing, stdout and stderr will be combined. <code>#SBATCH --open-mode=append</code> Optional: Append your job's output to the specified output filename(s).``` <code>#SBATCH --mail-type=BEGIN|END|FAIL|ALL</code> Optional: Request email notifications. Beware of mail bombing yourself. <code>#SBATCH --mail-user=email@address.xyz</code> Optional: Specify email address. If this is missing, notifications will go to your UArizona email address by default. <code>#SBATCH --exclusive</code> Optional: Request exclusive access to node. <code>#SBATCH --export=VAR</code> Optional: Export a comma-delimited list of environment variables to a job. <code>#SBATCH --export=all</code> Optional: Export your working environment to your job. This is the default. <code>#SBATCH --export=none</code> Optional: Do not export working environment to your job."},{"location":"running_jobs/batch_jobs/slurm_documentation/#slurm-environment-variables","title":"Slurm Environment Variables","text":"Variable Purpose Example Value <code>$SLURM_ARRAY_JOB_ID</code> Job array's parent ID <code>399124</code> <code>$SLURM_ARRAY_TASK_COUNT</code> Total number of subjobs in the array <code>4</code> <code>$SLURM_ARRAY_TASK_ID</code> Job index number (unique for each job in the array) <code>1</code> <code>$SLURM_ARRAY_TASK_MAX</code> Maximum index for the job array <code>7</code> <code>$SLURM_ARRAY_TASK_MIN</code> Minimum index for the job array <code>1</code> <code>$SLURM_ARRAY_TASK_STEP</code> Job array's index step size <code>2</code> <code>$SLURM_CLUSTER_NAME</code> Which cluster your job is running on <code>elgato</code> <code>$SLURM_CONF</code> Points to the Slurm configuration file <code>/var/spool/slurm/d/conf-cache/slurm.conf</code> <code>$SLURM_CPUS_ON_NODE</code> Number of CPUs allocated to target node <code>3</code> <code>$SLURM_GPUS_ON_NODE</code> Number of GPUs allocated to the target node <code>1</code> <code>$SLURM_GPUS_PER_NODE</code> Number of GPUs per node. Only set if <code>--gpus-per-node</code> is specified <code>1</code> <code>$SLURM_JOB_ACCOUNT</code> Account being charged <code>groupname</code> <code>$SLURM_JOB_GPUS</code> The global GPU IDs of the GPUs allocated to the job. Only set in batch and interactive jobs. <code>0</code> <code>$SLURM_JOB_ID</code> Your Slurm Job ID <code>399072</code> <code>$SLURM_JOB_CPUS_PER_NODE</code> Number of CPUs per node. This can be a list if there is more than one node allocated to the job. The list has the same order as <code>SLURM_JOB_NODELIST</code> <code>3,1</code> <code>$SLURM_JOB_NAME</code> The job's name <code>interactive</code> <code>$SLURM_JOB_NODELIST</code> The nodes that have been assigned to your job <code>gpu[73-74]</code> <code>$SLURM_JOB_NUM_NODES</code> The number of nodes allocated to the job <code>2</code> <code>$SLURM_JOB_PARTITION</code> The job's partition <code>standard</code> <code>$SLURM_JOB_QOS</code> The job's QOS/Partition <code>qos_standard_part</code> <code>$SLURM_JOB_USER</code> The username of the person who submitted the job <code>netid</code> <code>$SLURM_JOBID</code> Same as <code>SLURM_JOB_ID</code>, your Slurm Job ID <code>399072</code> <code>$SLURM_MEM_PER_CPU</code> The memory/CPU ratio allocated to the job <code>4096</code> <code>$SLURM_NNODES</code> Same as <code>SLURM_JOB_NUM_NODES</code> \u2013 the number of nodes allocated to the job <code>2</code> <code>$SLURM_NODELIST</code> Same as <code>SLURM_JOB_NODELIST</code>, The nodes that have been assigned to your job <code>gpu[73-74]</code> <code>$SLURM_NPROCS</code> The number of tasks allocated to your job <code>4</code> <code>$SLURM_NTASKS</code> Same as <code>SLURM_NPROCS</code>, the number of tasks allocated to your job <code>4</code> <code>$SLURM_SUBMIT_DIR</code> The directory where <code>sbatch</code> was used to submit the job <code>/home/u00/netid</code> <code>$SLURM_SUBMIT_HOST</code> The hostname where <code>sbatch</code> was used to submit the job <code>wentletrap.hpc.arizona.edu</code> <code>$SLURM_TASKS_PER_NODE</code> The number of tasks to be initiated on each node. This can be a list if there is more than one node allocated to the job. The list has the same order as <code>SLURM_JOB_NODELIST</code> <code>3,1</code> <code>$SLURM_WORKING_CLUSTER</code> Valid for interactive jobs, will be set with remote sibling cluster's IP address, port and RPC version so that any sruns will know which cluster to communicate with. <code>elgato:foo:0000:0000:000</code>"},{"location":"running_jobs/batch_jobs/slurm_documentation/#slurm-reason-codes","title":"Slurm Reason Codes","text":"<p>Sometimes, if you check a pending job using squeue, there are some messages that show up under Reason indicating why your job may not be running. Some of these codes are non-intuitive so a human-readable translation is provided below:</p> Reason Explanation <code>AssocGrpCpuLimit</code> Your job is not running because your group CPU limit has been reached<sup>1</sup> <code>AssocGrpMemLimit</code> Your job is not running because your group memory limit has been reached<sup>1</sup> <code>AssocGrpCPUMinutesLimit</code> Either your group is out of CPU hours or your job will exhaust your group's CPU hours. <code>AssocGrpGRES</code> Your job is not running because your group GPU limit has been reached<sup>1</sup> <code>Dependency</code> Your job depends on the completion of another job. It will wait in queue until the target job completes. <code>QOSGrpCPUMinutesLimit</code> This message indicates that your high priority or qualified hours allocation has been exhausted for the month. <code>QOSMaxWallDurationPerJobLimit</code> Your job's time limit exceeds the max allowable and will never run<sup>1</sup> <code>Nodes_required_for_job_are_DOWN,_DRAINED_or_reserved_or_jobs_in_higher_priority_partitions</code> This very long message simply means your job is waiting in queue until there is enough space for it to run <code>Priority</code> Your job is waiting in queue until there is enough space for it to run. <code>QOSMaxCpuPerUserLimit</code> Your job is not running because your per-user CPU limit has been reached<sup>1</sup> <code>ReqNodeNotAvail, Reserved for maintenance</code> Your job's time limit overlaps with an upcoming maintenance window. Run \"uptime_remaining\" to see when the system will go offline. If you remove and resubmit your job with a shorter walltime that does not overlap with maintenance, it will likely run. Otherwise, it will remain pending until after the maintenance window. <code>Resources</code> Your job is waiting in queue until the required resources are available."},{"location":"running_jobs/batch_jobs/slurm_documentation/#slurm-output-filename-patterns","title":"Slurm Output Filename Patterns","text":"Variable Meaning Example Slurm Directive(s) Sample Output <code>%A</code> A job array's main job ID <code>#SBATCH --array=1-2</code><code>#SBATCH -o %A.out</code><code>#SBATCH --open-mode=append</code> <code>12345.out</code> <code>%a</code> A job array's index number <code>#SBATCH --array=1-2</code><code>#SBATCH -o %A_%a.out</code> <code>12345_1.out</code><code>12345_2.out</code> <code>%J</code> Job ID plus stepid <code>#SBATCH -o %J.out</code> <code>12345.out</code> <code>%j</code> Job ID <code>#SBATCH -o %j.out</code> <code>12345.out</code> <code>%N</code> Hostname of the first compute node allocated to the job <code>#SBATCH -o %N.out</code> <code>r1u11n1.out</code> <code>%u</code> Username <code>#SBATCH -o %u.out</code> <code>netid.out</code> <code>%x</code> Job name <code>#SBATCH --job-name=JobName</code><code>#SBATCH -o %x.out</code> <code>JobName.out</code> <ol> <li> <p>Groups and users are subject to limitations on resource usage. For more information, see job limits.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"running_jobs/best_practices/","title":"Best Practices","text":"<p>Unlike your local computer, you share HPC with many other users. As a result, what you do on the system can affect others. Exercise good citizenship to ensure that your activity does not adversely impact the system and the research community with whom you share it. Below are some rules of thumb. </p>"},{"location":"running_jobs/best_practices/#login-nodes","title":"Login Nodes","text":"<p>A login node serves as a staging area where you can perform housekeeping work, edit scripts, and submit job requests for execution on one/some of the cluster\u2019s compute nodes. It is important to know that the login nodes are not the location where scripts are run. Heavy computation on the login nodes slows the system down for all users and will not give you the resources or performance you need. It should also be stressed that software is not available on the login nodes. General practices:</p> <ul> <li> <p>Know when you are on a login node</p> <p>You can use your Linux prompt or the \"hostname\" command. It is where you land when you choose shell from the bastion node. To differentiate, the bastion host (gatekeeper, or keymaster) is a secure portal from the outside and serves no compute function.</p> </li> <li> <p>What activities are appropriate</p> <p>edit and manage files, request interactive sessions, submit jobs, and track jobs.</p> </li> <li> <p>Avoid computationally intensive activity</p> <ul> <li>Don't run research applications.  Use an interactive session if running a job is not appropriate.</li> <li>Don't launch too many simultaneous processes.  </li> <li>That script you run to monitor job status several times a second should probably run every few minutes.</li> <li>I/O activity can slow the login node for everyone, like multiple copies or \"ls -l\" on directories with 000's of files.</li> </ul> </li> </ul>"},{"location":"running_jobs/best_practices/#shared-file-system","title":"Shared File System","text":"<p>The shared file system is the location for everything in /home, /xdisk, and /groups. The exception is /tmp which is the local disk on each node. Your I/O activity can have dramatic activity on other users. The general statement here is to ask the consultants for advice on improving your I/O activity. The time spent can be saved many times over in faster job execution.    </p> <ul> <li> <p>Use /tmp for working space</p> <p>If you have multiple jobs that will use the same data, there are ways to copy it to /tmp and run multiple jobs increasing performance and reducing I/O load.</p> </li> <li> <p>Be aware of I/O load.</p> <p>If your workflow creates a lot of I/O activity then creating dozens of jobs doing the same thing may be detrimental.</p> </li> <li> <p>Avoid storing many files in a single directory</p> <p>Hundreds of files is probably ok; tens of thousands is not.    </p> </li> <li> <p>Avoid opening and closing files repeatedly in tight loops</p> <p>If possible, open files once at the beginning of your workflow / program, then close them at the end.</p> </li> <li> <p>Watch your quotas</p> </li> <li> <p>You are limited in capacity and file count. Use \"uquota\". In /home the scheduler writes files in a hidden directory assigned to you.</p> </li> <li> <p>Avoid frequent snapshot files</p> <p>This can stress the storage.</p> </li> <li> <p>Limit file copy sessions</p> <p>You share the bandwidth with others.  Two or three scp sessions are probably ok; &gt;10 is not.</p> </li> <li> <p>Consolidate files</p> <p>If you are transferring many small files consider collecting them in a tarball first.</p> </li> <li> <p>Use parallel I/O</p> <p>If available like \"module load phdf5\"</p> </li> </ul>"},{"location":"running_jobs/best_practices/#submitting-jobs","title":"Submitting Jobs","text":"<ul> <li> <p>Don't ask for more time than you really need</p> <p>The scheduler will have an easier time finding a slot for the 2 hours you need rather than the 48 hours you request.  When you run a job it will report back on the time used which you can use as a reference for future jobs. However don't cut the time too tight.  If something like shared I/O activity slows it down and you run out of time, the job will fail.</p> </li> <li> <p>Test your submission scripts</p> <p>Start small. You can use an interactive session to help build your script and run tests in real time.</p> </li> <li> <p>Respect memory limits</p> <p>If your application needs more memory than is available, your job could fail and leave the node in a state that requires manual intervention.</p> </li> <li> <p>Do not run scripts automating hundreds or thousands of job submissions</p> <p>Executing large numbers of job submissions in rapid succession (e.g. in a scripted loop) can overload the system's scheduler and cause problems with overall system performance. A better alternative is to submit job arrays.</p> </li> <li> <p>Hyperthreading is turned off</p> <p>Running multiple threads per core is generally not productive.  MKL is an exception to that if it is relevant to you.</p> </li> </ul>"},{"location":"running_jobs/interactive_jobs/","title":"Interactive Jobs","text":""},{"location":"running_jobs/interactive_jobs/#overview","title":"Overview","text":"<p>Interactive sessions are a way to gain access to a compute node from the command line. This is useful for checking available modules, testing submission scripts, compiling software, and running programs in real time. When you are on a login node, you can request a session by simply entering: <code>interactive</code>. For example:</p> <pre><code>(ocelote) [netid@junonia ~]$ interactive\nRun \"interactive -h for help customizing interactive use\"\nSubmitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1    --ntasks=1 --time=01:00:00 --account=windfall --partition=windfall\nsalloc: Pending job allocation 531843\nsalloc: job 531843 queued and waiting for resources\nsalloc: job 531843 has been allocated resources\nsalloc: Granted job allocation 531843\nsalloc: Waiting for resource configuration\nsalloc: Nodes i16n1 are ready for job\n[netid@i16n1 ~]$\n</code></pre> <p>Notice in the example above how the command prompt changes once your session starts. When you're on a login node, your prompt will show \"junonia\" or \"wentletrap\". Once you're in an interactive session, you'll see the name of the compute node you're connected to. </p>"},{"location":"running_jobs/interactive_jobs/#customizing-your-resources","title":"Customizing Your Resources","text":"<p>The command <code>interactive</code> when run without any arguments will allocate you a windfall session using one CPU for one hour which isn't ideal for most use cases. You can modify this by including additional flags. To see the available options, you can use the help flag <code>-h</code></p> <p><pre><code>(ocelote) [netid@junonia ~]$ interactive -h\nUsage: /usr/local/bin/interactive [-x] [-g] [-N nodes] [-m memory per core] [-n total number of tasks] [-Q optional qos] [-t hh::mm:ss] [-a account to charge]\n</code></pre> The values can be combined and each mean the following:</p> Flag Explanation Example <code>-a</code> This is followed by your group's name and will switch you to using the standard partition. This is highly recommended to keep your sessions from being interrupted and to help them start faster <code>-a my_group</code> <code>-t</code> The amount of time to reserve for your job in the format <code>hhh:mm:ss</code> <code>-t 05:00:00</code> <code>-n</code> Total number of tasks (CPUs) to allocate to your job. By default, this will be on a single node <code>-n 16</code> <code>-N</code> Total number of nodes (physical computers) to allocate to your job <code>-N 2</code> <code>-m</code> Total amount of memory per CPU. See Compute Resources for more information and potential complications <code>-m 5gb</code> <code>Q</code> Used to add a qos to access high priority or qualified hours. Only for groups with buy-in/special project hours <code>-Q user_qos_pi_netid</code> <code>-g</code> Request a GPU. This flag takes no arguments. If you want to request more than one GPU in an interactive session, you can use salloc directly <code>-q</code> <code>-x</code> Enable X11 forwarding. This flag takes no arguments. <code>-x</code> <p>You may also create your own salloc commands using any desired Slurm directives for maximum customization.</p>"},{"location":"running_jobs/interactive_jobs/#tips-and-tricks-for-faster-sessions","title":"Tips and Tricks for Faster Sessions","text":"<ul> <li> <p>Switch to ElGato </p> <p>This cluster shares the same operating system, software, and file system as Puma so often your workflows are portable across clusters. Ocelote and ElGato standard nodes have 28 and 16 CPUs, respectively, and are often less utilized than Puma meaning much shorter wait times. Before you run the interactive command, type elgato to switch.</p> </li> <li> <p>Use the account flag</p> <p>By default, <code>interactive</code> will request a session using the windfall partition. Windfall is lower priority than standard and so these types of jobs take longer to get through the queue. If you include the account flag, that will switch your partition to standard. An example of this type of request: <pre><code>interactive -a YOUR_GROUP\n</code></pre></p> </li> </ul>"},{"location":"running_jobs/job_limits/","title":"Job Limits","text":"<p>There are two main types of limits imposed on all jobs: (1) those due to hardware, and (2) those due to policy.</p>"},{"location":"running_jobs/job_limits/#hardware-limits","title":"Hardware Limits","text":"<p>See our page on Compute Resources for more information. In particular, note that there are only a certain number of CPUs available per node, only a certain number of nodes available on each cluster, and that memory and CPU count are linked due to the physical connection of these resources. </p>"},{"location":"running_jobs/job_limits/#policy-limits","title":"Policy Limits","text":"<p>The following limits are imposed on all user-submitted jobs as a matter of policy.</p> <ul> <li>Wall Time: The maximum time a job can run for is 10 days, or 240 hours. Any job requesting more than the maximum time will fail. </li> <li>Others??</li> </ul>"},{"location":"running_jobs/open_on_demand/file_browser/","title":"File Browser","text":"<p>The file browser provides easy access to your <code>/home</code>, <code>/xdisk</code>, and <code>/groups</code> directories and allows you to view, edit, copy, and rename your files. You may also transfer small files between HPC and your local workstation using this interface. For larger transfers, see our section on Transferring Data for more efficient methods.</p>"},{"location":"running_jobs/open_on_demand/file_browser/#access","title":"Access","text":"<p>In the browser at the top of the screen, select the Files dropdown</p> <p></p> <p>You will be able to select your home directory, <code>/groups</code>, or <code>/xdisk</code>. If you select <code>/groups</code> or <code>/xdisk</code>, enter your PI's NetID in the Filter field to find your shared group space.</p> <p></p>"},{"location":"running_jobs/open_on_demand/file_browser/#editing-files","title":"Editing Files","text":"<p>First, navigate to the file you wish to edit. Then, click the vertical ellipses on the right-hand side and select Edit</p> <p></p> <p>This will open a file editor in your browser where you may select your color theme, text size, and syntax highlighting. To save your changes, click Save in the upper left side of the screen</p> <p></p>"},{"location":"running_jobs/open_on_demand/file_browser/#transferring-files","title":"Transferring Files","text":""},{"location":"running_jobs/open_on_demand/file_browser/#uploading-files","title":"Uploading Files","text":"<p>In the file browser, navigate to the directory where you would like to upload your files, then select \"Upload\".  </p> <p></p> <p>This will bring up a popup where you can open a file browser to search for your files. Alternatively, you can drag/drop files/directories onto the tile.</p> <p></p>"},{"location":"running_jobs/open_on_demand/file_browser/#downloading-files","title":"Downloading Files","text":"<p>To download small files from your HPC account to your local workstation, navigate to the file(s) you'd like to to transfer and check the box(es) on the left. Clicking download will initiate the transfer to your local Downloads directory.</p> <p></p>"},{"location":"running_jobs/open_on_demand/graphical_applications/","title":"Interactive Graphical Applications","text":""},{"location":"running_jobs/open_on_demand/graphical_applications/#overview","title":"Overview","text":"<p>Open OnDemand provides access to graphical interfaces for some popular software. These can be found under Interactive Apps through the Open OnDemand web browser. The process of starting and accessing these applications is the same. </p>"},{"location":"running_jobs/open_on_demand/graphical_applications/#web-form","title":"Web Form","text":"<p>First, select the desired application from Interactive Apps. This will take you to a form where you will enter your job information. This includes the entries in the following table:</p> <p></p> Field Description Cluster Select which cluster to submit the job request to. Run Time The maximum number of hours the job can run. Please note that the maximum possible run time is 10 days (240 hours). Core Count on a Single Node The number of CPUs needed. This affects the amount of memory your job is allocated. The maximum that can be requested is dependent on which cluster you choose. Memory per Core The amount of memory needed per core. The amount that can be requested is dependent on which cluster you choose and your desired node type. Warning: if you request more than is available on a standard node, you may be allocated a high memory node. The wait times for these machines can be significantly longer. GPUs Required The number of GPUs needed for your job, if any. This field may be left blank or set to 0 if no GPU is desired. Up to 4 may be requested on a single Puma node, 1 may be requested on Ocelote, and 0 on ElGato. PI Group Your accounting group. If you do not know your group name, you can either check in the user portal, or can run <code>va</code> on the command line. If the group you entered does not exist, you will receive an error \"sg: group 'foo' does not exist\" Queue The queue, or partition, to use. Standard is the most common. If your group has buy-in hours, you may use High Priority. <p>Once you've entered all your details, click Launch at the bottom of the page. This will take you to a tile with information about your job including job ID and session ID. This information can used for debugging purposes.</p> <p>When you first submit your job, it will show as having a status of \"Queued\". Once your job reaches the front of the queue, it will show a status of \"Starting\". When your session is ready, you can launch the application using Connect at the bottom of the tile.</p>"},{"location":"running_jobs/open_on_demand/graphical_applications/#applications-available","title":"Applications Available","text":"Virtual DesktopJupyter NotebooksRstudioMatlabAnsysAbaqus <p>One nice feature of Open OnDemand is the ability to interact with HPC using a virtual Desktop environment. This provides a user-friendly way to run applications, perform file management, and navigate through your directories as though you were working with a local computer. Additionally, it eliminates the need to use X11 forwarding when working with GUI applications allowing an easy way to interact with software such as Matlab, VisIt, or Anaconda.</p> <p></p> <p>Tip</p> <p>To access your own python packages in Jupyter, you can create custom kernels either using a python module or using anaconda. </p> <p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. </p> <p>When you start a Jupyter notebook, by default your working directory will be your home. If you would like to change this so that your session starts in a different location, you'll need to add a line to the hidden file ~/.bashrc in your home. To do this, open your ~/.bashrc in a text editor and add the following, substituting your desired path in for /path/to/directory:</p> <pre><code>export NOTEBOOK_ROOT=/path/to/directory\n</code></pre> <p></p> <p>RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. For an overview of the RStudio IDE, see: https://www.rstudio.com/products/RStudio/.</p> <p>For information on using R on HPC, see our online documentation on Using R Packages.</p> <p></p> <p>A GUI for multiple versions of Matlab is available. You can select which version to use in the web form when specifying your resources.</p> <p></p> <p>Multiple versions of the engineering application Ansys are available. You can specify which version to use in the web form when specifying your resources. To receive Ansys-specific support, see: Community and External Resources</p> <p></p> <p>A GUI for Abaqus is available. </p> <p></p>"},{"location":"running_jobs/open_on_demand/job_viewer_and_composer/","title":"Job Viewer and Composer","text":""},{"location":"running_jobs/open_on_demand/job_viewer_and_composer/#job-viewer","title":"Job Viewer","text":"<p>The Job Viewer allows you to check the status and time remaining of your running jobs. You can also cancel your jobs using this interface. Note: be careful looking at All Jobs since this will likely timeout trying to organize them all. To use the Job Viewer, navigate to the Jobs dropdown and select Active Jobs.</p> <p></p> <p>This will open a new page listing all your running and pending jobs. You may delete them by clicking the red trash icon under Actions, or view more information about individual jobs using the dropdown on the left next to the ID.</p> <p></p>"},{"location":"running_jobs/open_on_demand/job_viewer_and_composer/#job-composer","title":"Job Composer","text":"<p>The Job Composer lets you create and run a SLURM script on any of our three clusters. It should be noted that the Job Composer creates a special string of directories in your /home starting with ondemand/ which is where both your submission scripts and output files will be stored. Make note of the path to your files on the right-hand side of the Job Composer screen under Script location.</p> <p></p>"},{"location":"running_jobs/open_on_demand/overview/","title":"Open OnDemand","text":"<p>Open OnDemand</p> <p>Open OnDemand is an NSF-funded open-source HPC portal. This web interface is available for users to interact with HPC providing shell access, file management capabilities, and access to graphical applications.</p> <p></p>"},{"location":"running_jobs/open_on_demand/shell_access/","title":"Shell Access","text":"<p>Tip</p> <p>Shell access through OOD is also documented in System Access</p> <p>Need command line access to a terminal on HPC? No problem! Simply select the Clusters dropdown menu to connect to one of HPC's login nodes. </p> <p></p>"},{"location":"running_jobs/overview/","title":"Overview","text":"<p>The HPC is a shared system, and its resources are in high demand. Computational work must be run on dedicated compute resources. These resources are granted to each user for a limited time per session, and sessions are organized by Slurm, an open source, fault-tolerant, and highly scalable cluster manager and task scheduler. Users can interact  with Slurm from one of the login nodes to start an interactive job or submit a batch job..</p> Login Nodes <p>Do not run intensive programs or calculations on the Login Nodes. These are shared nodes that are not intended for computation. Instead, please request computing resources by following the directions in these pages. </p> <p>Below is a FAQ that includes answers to common questions and misconceptions about running jobs on HPC. Many of these are general information about HPC systems, but some contain information specific to the UA HPC. We recommend reviewing this FAQ whether you are a new user getting started on our system, or an experienced user returning to our documentation.</p>"},{"location":"running_jobs/overview/#frequently-asked-questions","title":"Frequently Asked Questions","text":"What is a Job? <p>A single instance of an allocation selection of computing resources is known as a Job. Jobs can be in the format of a graphical Open OnDemand application instance, interactive terminal session, or batch submission (a script scheduled for  execution at a later time), and require an active allocation of CPU-time under an associated PI account. Each PI is granted a standard monthly allocation of 100,000 CPU-hrs on Puma; 70,000 CPU-hrs on Ocelote; and 7,000 CPU-hrs on El Gato. See Time Allocations for more information.      </p> Button <p>text      </p> Button <p>text      </p>"},{"location":"secure_hpc/access/","title":"Access","text":"<p>Warning</p> <p>You must be connected to the Soteria VPN to access the system.</p>"},{"location":"secure_hpc/access/#command-line-access","title":"Command Line Access","text":"<p>Soteria command line access is available with ssh. The hostname is <code>shell.cougar.hpc.arizona.edu</code></p> <pre><code>$ ssh your_netid@shell.cougar.hpc.arizona.edu\n\nAuthorized uses only. All activity may be monitored and reported.\nLast login: Tue Nov 29 06:18:33 2022 from ans-02.hpc.arizona.edu\nAuthorized uses only. All activity may be monitored and reported.\nnetid@taub:~ $\n</code></pre> <p>Taub is a login node and will provide the same functionality and have the same policies as the other HPC clusters. Modules are available on Soteria's compute nodes but not on the login node. The command <code>interactive</code> is available to request a session on a compute node and jobs may be submitted using the standard sbatch. More details on Slurm commands can be found in Running Jobs</p>"},{"location":"secure_hpc/access/#graphical-interface","title":"Graphical Interface","text":"<p>Similar to the other HPC clusters, we offer the service Open OnDemand to provide web browser access to Soteria. This can be used to navigate, view, and edit files as well as gain access to graphical applications.</p> <p>In your favorite browser, go to: https://ondemand-hipaa.hpc.arizona.edu</p> <p>The applications currently available are  RStudio, Matlab and Python 3.9 (Jupyter). </p>"},{"location":"secure_hpc/overview/","title":"Secure HPC Overview","text":"<p>Warning</p> <p>Currently Soteria is in a pilot test mode and is not generally available. It is expected that the testing will run until the middle of 2023 </p> <p>Research Technologies in partnership with the Data Science Institute is providing a secure research enclave that is HIPAA compliant. It is called Soteria. In Greek mythology, Soteria (Greek: \u03a3\u03c9\u03c4\u03b7\u03c1\u03af\u03b1) was the goddess or spirit (daimon) of safety and salvation, deliverance, and preservation from harm.</p> <p>Soteria will be available for access using the OnDemand graphical interface.</p>"},{"location":"secure_hpc/overview/#available-resources","title":"Available Resources","text":"<p>This small HPC cluster has many of the capabilities of the main HPC.  There are compute nodes, with the same core count and memory. And there are two GPU nodes.</p> <p>This small cluster has four standard compute nodes. Each has 94 cores and 512GB memory available. The two GPU nodes have the same resources but there are also four V100 GPU's in each. You can use the regular parts of the documentation to learn how to use slurm with these nodes. In case you are interested, these nodes are named by their physical location and you will see this if you are connected to an interactive session. And with OnDemand (OOD) your connection will show the node hostname.</p> Node Type Node Names Standard Nodes r1u26n1,r1u27n1,r1u28n1,r1u29n1 GPU Nodes r1u30n1,r1u32n1"},{"location":"secure_hpc/overview/#allocations-and-storage","title":"Allocations and Storage","text":"<p>For the purpose of this early testing, the allocations of time and space will be similar to HPC. The time allocation will be 100,000 hours. Your account will come with space in <code>/home</code> and <code>/groups</code> where you can put your data.  Currently those directories are not subject to a quota limit.</p>"},{"location":"secure_hpc/prerequisites/","title":"Prerequisites to Access","text":"<p>To gain access, you will need to submit a Soteria request form. Once your form has been reviewed and approved, you will receive an email with the subject UA Soteria Access Request Approved. This email will contain the next steps to take which are detailed below:</p>"},{"location":"secure_hpc/prerequisites/#complete-required-training-in-edge-learning","title":"Complete Required Training in Edge Learning","text":"<p>The CRRSP team will register you for the required trainings listed below (courses can also be found here: https://uaccess.arizona.edu): 1. HIPAA Essentials 2. Information Security: Insider Threat Awareness 3. Information Security Awareness Certification</p>"},{"location":"secure_hpc/prerequisites/#assignment-to-the-soteria-vpn","title":"Assignment to the Soteria VPN","text":"<p>Once you have completed your required training, the CRRSP team will notify you via email when you have been assigned access to the Soteria VPN. This VPN is an important part of our HIPAA compliance and differentiates Soteria usage from the standard HPC clusters. Soteria access cannot be established when not connect to the VPN. For VPN access, use: <code>vpn.arizona.edu/soteria</code>.</p> <p></p>"},{"location":"secure_hpc/prerequisites/#additional-requirements","title":"Additional Requirements","text":"<p>The computer you will use to access Soteria services must meet the following requirements:</p> <ol> <li>The Operating System and applications must be updated with the latest patches.</li> <li>You must have a strong password to log into the computer (at least 8 characters and a mix of character types). </li> <li>This must not be a shared computer with other users.</li> <li>Up to date anti-virus software.</li> </ol>"},{"location":"secure_hpc/storage_and_transfers/","title":"Storage and Transfers","text":""},{"location":"secure_hpc/storage_and_transfers/#file-paths","title":"File Paths","text":"<p>Your files can be accessed on the filexfer nodes(1):</p> <ol> <li>hostname: <code>filexfer.hpc.arizona.edu</code></li> </ol> <ul> <li> <code>/hipaa/groups/ <li><code>/hipaa/home/uxx/ <p>When connected to a Soteria login/compute node, you can find these under:</p> <ul> <li><code>/groups/&lt;pi_netid&gt;</code></li> <li><code>/home/uxx/&lt;your_netid&gt;</code></li> </ul>"},{"location":"secure_hpc/storage_and_transfers/#transferring-data","title":"Transferring Data","text":"<p>Globus can be used for moving data in and out of the Soteria environment. For more information on using Globus, see our Globus documentation</p> <p>Soteria's endpoint is: UA HPC HIPAA Filesystems</p>"},{"location":"software/common_datasets/","title":"Common Datasets","text":"<p>We host several large community datasets.  It is beneficial to you and us.  For you, it saves all that time downloading and filling up your storage allocation.  And for us it reduces the occurrence of the same data in many places. We do not currently update them on any particular cadence. You can request updates if you feel those would be useful to the community.</p> <p>These datasets and databases are available on the compute nodes in the filesystem called /contrib/datasets in read-only mode. You can view them there.</p>"},{"location":"software/common_datasets/#alphafold-2","title":"AlphaFold 2","text":"<p>AlphaFold is an AI system developed by the Google DeepMind project to predict the 3D structure of a protein from its amino acid sequence. AlphaFold needs multiple datasets to run, the combined size of which is around 2.62 TB. It takes a long time to download them. We save you that effort by locating these datasets at <code>/contrib/datasets/alphafold</code>.</p> <p>We also host containers at <code>/contrib/singularity/alphafold</code> that you can use with the provided datasets to predict protein structures with AlphaFold. You can access the container by loading the alphafold module from an interactive session or a batch submission script. When you load the alphafold module, it defines the following additional environment variables that you can use to easily access the container and the datasets:</p> <ul> <li><code>ALPHAFOLD_DIR</code> which points to <code>/contrib/singularity/alphafold</code></li> <li><code>ALPHAFOLD_DATADIR</code> which points to <code>/contrib/datasets/alphafold</code></li> </ul> <p>The following batch submission script shows how you can use the container with the provided datasets.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=alphafold-run\n#SBATCH --time=04:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=standard\n#SBATCH --account=&lt;pi-account&gt;\n\n# Uncomment the following two lines to make predictions on proteins that would be too long to fit into GPU memory.\n# export APPTAINERENV_TF_FORCE_UNIFIED_MEMORY=1 \n# export APPTAINERENV_XLA_PYTHON_CLIENT_MEM_FRACTION=4.0\n\nmodule load alphafold\n\nalphafold --nv \\\n          --use_gpu_relax \\\n          --uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n          --uniref30_database_path=/data/uniref30/UniRef30_2021_03 \\\n          --mgnify_database_path=/data/mgnify/mgy_clusters_2022_05.fa  \\\n          --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt  \\\n          --pdb70_database_path=/data/pdb70/pdb70  \\\n          --template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n          --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n          --model_preset=monomer \\\n          --max_template_date=2023-08-02 \\\n          --db_preset=full_dbs \\\n          --output_dir=results \\\n          --fasta_paths=input.fasta\n</code></pre>"},{"location":"software/common_datasets/#llama-2","title":"Llama 2","text":"<p>The Meta Large Language Model project has several datasets that we make available.</p>"},{"location":"software/common_datasets/#ncbi-blast","title":"NCBI Blast","text":"<p>The Blast databases are provided in support of the Blast module.</p>"},{"location":"software/containers/building_containers/","title":"Building Containers","text":"<p>Warning</p> <p>The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation. </p> <p>Tip</p> <p>For detailed information on Apptainer recipes, see </p>"},{"location":"software/containers/containers_on_hpc/","title":"Containers on HPC","text":"<p>Tip</p> <p>Apptainer is installed on the operating systems of all HPC compute nodes, so can be easily accessed either from an interactive session or batch script without worrying about software modules. </p>"},{"location":"software/containers/containers_on_hpc/#available-containers","title":"Available Containers","text":"<p>We support the use of HPC and ML/DL containers available on NVIDIA GPU Cloud (NGC). Many of the popular HPC applications including NAMD, LAMMPS and GROMACS containers are optimized for performance and available to run in Apptainer on Ocelote or Puma. The containers and respective README files can be found in <code>/contrib/singularity/nvidia</code>. They are only available from compute nodes, so start an interactive session if you want to view them.</p>  Container  Description <code>nvidia-caffe.20.01-py3.simg</code> Caffe is a deep learning framework made with expression, speed, and modularity in mind. It was originally developed by the Berkeley Vision and Learning Center (BVLC). <code>nvidia-gromacs.2018.2.simg</code> <code>nvidia-julia.1.2.0.simg</code> <code>nvidia-lammps.24Oct2018.sif</code> <code>nvidia-namd_2.13-multinode.sif</code> <code>nvidia-pytorch.20.01-py3.simg</code> PyTorch is a Python package that provides two high-level features:- Tensor computation (like numpy) with strong GPU acceleration- Deep Neural Networks built on a tape-based autograd system <code>nvidia-rapidsai.sif</code> <code>nvidia-relion_2.1.b1.simg</code> <code>nvidia-tensorflow_2.0.0-py3.sif</code> TensorFlow is an open source software library for numerical computation using data flow graphs. TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research. <code>nvidia-theano.18.08.simg</code> Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently."},{"location":"software/containers/containers_on_hpc/#sharing-your-containers","title":"Sharing Your Containers","text":"<p>If you have containers that you would like to share with your research group or broader HPC community, you may do so in the space <code>/contrib/singularity/shared</code>.</p>"},{"location":"software/containers/containers_on_hpc/#cache-directory","title":"Cache Directory","text":"<p>To speed up image downloads for faster, less redundant builds and pulls, Apptainer sets a cache directory in your home under <code>~/.apptainer</code>. This directory stores images, metadata, and docker layers that can wind up being reasonably large. If you're struggling with space usage and your home's 50GB quota, one option is to set a new Apptainer cache directory. You can do this by setting the environment variable <code>APPTAINER_CACHEDIR</code> to a new directory. From Apptainer's documentation:</p> <p>If you change the value of <code>APPTAINER_CACHEDIR</code> be sure to choose a location that is:</p> <pre><code>1. Unique to you. Permissions are set on the cache so that private images cached for one user are not exposed to another. This means that ```APPTAINER_CACHEDIR``` cannot be shared.\n2. Located on a filesystem with sufficient space for the number and size of container images anticipated.\n3. Located on a filesystem that supports atomic rename, if possible.\n</code></pre> <p>For example, if you wanted to set your cache directory to your PI's /groups directory under a directory you own, you could use:</p> <pre><code>export APPTAINER_CACHEDIR=/groups/pi/your_netid/.apptainer\n</code></pre> <p>To make the change permanent, add this line to the hidden file in your home directory <code>~/.bashrc</code>.</p>"},{"location":"software/containers/pulling_containers/","title":"Pulling Containers","text":"<p>Warning</p> <p>The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation. </p>"},{"location":"software/containers/pulling_containers/#pulling-docker-containers","title":"Pulling Docker Containers","text":"<p>Apptainer has the ability to convert available docker images into sif format allowing them to be run on HPC. If you find an image on Docker Hub that you would like to use, you can pull it using the <code>apptainer pull command &lt;local_image_name&gt;.sif docker://docker_image</code>. </p> <p>As an example, we could pull an Ubuntu image from Docker Hub with OS 22.04 by searching for Ubuntu, opening the Tags tab, and copying their <code>docker pull</code> command:</p> <p></p> <p>Then, on HPC, we can run:</p> <pre><code>[netid@cpu37 pull_example]$ apptainer pull ubuntu_22.04.sif docker://ubuntu:22.04\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob 01007420e9b0 done  \nCopying config 3db8720ecb done  \nWriting manifest to image destination\nStoring signatures\n2024/02/20 09:02:02  info unpack layer: sha256:01007420e9b005dc14a8c8b0f996a2ad8e0d4af6c3d01e62f123be14fe48eec7\nINFO:    Creating SIF file...\n[netid@cpu37 pull_example]$ ls\nubuntu_22.04.sif\n</code></pre>"},{"location":"software/containers/pulling_containers/#pulling-nvidia-images","title":"Pulling Nvidia Images","text":"<p>The NVIDIA GPU Cloud (NGC) provides GPU-accelerated HPC and deep learning containers for scientific computing.  NVIDIA tests HPC container compatibility with the Apptainer runtime through a rigorous QA process. Application-specific information may vary so it is recommended that you follow the container-specific documentation before running with Apptainer. If the container documentation does not include Apptainer information, then the container has not yet been tested under Apptainer. Apptainer can be used to pull, execute, and bootstrap off of Docker images. </p> <p>To pull images, you'll need to register with Nvidia. Once you have an account, you can view their images from their catalogue. Click on the name of the software you're interested in to view available versions</p> <p></p> <p>If you click on the Tags tab at the top of the screen, you'll find the different versions that are available for download. For example, if we click on TensorFlow, we can get the pull statement for the latest tag of TensorFlow 2 by clicking the ellipses and selecting Pull Tag.</p> <p></p> <p>This will copy a <code>docker pull</code> statement to your clipboard, in this case:</p> <pre><code>$ docker pull nvcr.io/nvidia/tensorflow:22.02-tf2-py3\n</code></pre> <p>To pull and convert this NGC image to a local Apptainer image file, we'll convert this to:</p> <pre><code>$ apptainer build ~/tensorflow2-22.02-py3.sif docker://nvcr.io/nvidia/tensorflow:22.02-tf2-py3\n</code></pre> <p>The general format for any pull you want to do is:</p> <pre><code>$ apptainer build &lt;local_image_name&gt; docker://nvcr.io/&lt;registry&gt;/&lt;app:tag&gt;\n</code></pre> <p>This Apptainer build command will download the app:tag NGC Docker image, convert it to Apptainer format, and save it to the local filename local_image_name. </p>"},{"location":"software/containers/using_containers/","title":"Using Containers","text":"<p>Apptainer can be used to execute your workflows in various ways. Running a prepackaged workflow embedded in the image, executing commands within the container's environment, or starting an interactive instance.</p>"},{"location":"software/containers/using_containers/#running-apptainer-images","title":"Running Apptainer Images","text":"Command Function Example <code>apptainer run</code> Run without arguments. This executes a predefined workflow embedded in the image. <code>apptainer run app.sif</code> <code>apptainer exec &lt;commands&gt;</code> Invokes <code>&lt;commands&gt;</code> from inside the container environment. <code>apptainer exec app.sif python3 script.py</code> <code>apptainer shell</code> Starts an interactive session within the container's environment. Optimal for testing and debugging. <code>apptainer shell app.sif</code>"},{"location":"software/containers/using_containers/#apptainer-nvidia-and-gpus","title":"Apptainer, Nvidia, and GPUs","text":""},{"location":"software/containers/what_are_containers/","title":"What are Containers?","text":"<p>A container is a packaged unit of software that contains code and all its dependencies including, but not limited to: system tools, libraries, settings, and data. This makes applications and pipelines portable and reproducible, allowing for a consistent environment that can run on multiple platforms.</p> <p>Shipping containers have frequently been used as an analogy because the container is standard, does not care what is put inside, and will be carried on any ship; or in the case of computing containers, it can run on many different systems.</p> <p>Docker is widely used by researchers, however, Docker images require root privileges which means they cannot be run in an HPC environment.</p> <p>Apptainer (formerly Singularity) addresses this by completely containing the authority so that all privileges needed at runtime stay inside the container. This makes it ideal for the shared environment of a supercomputer. Even better, a Docker image can be encapsulated inside an Apptainer image. Some ideal use cases that can be supported by Apptainer on HPC include:</p> <ul> <li>You already use Docker and want to run your jobs on HPC.</li> <li>You want to preserve your environment so a system change will not affect your work.</li> <li>You need newer or different libraries than are offered on the system.</li> <li>Someone else developed a workflow using a different version of Linux.</li> <li>You prefer to use a Linux distribution other than CentOS (e.g. Ubuntu).</li> <li>You want a container with a database server like MariaDB.</li> </ul> <p>The documentation here provides instructions on how to either take a Docker image and run it from Apptainer, or create an image using Apptainer only.</p>"},{"location":"software/modules/","title":"Software Modules","text":"<p>Tip</p> <p>Software modules are not available on the login nodes. You will need to be on a compute node to access them.</p> <p>Software packages are available as modules and are accessible from the compute nodes of any of our three clusters. A software module is a tool used to manage software environments and dependencies. It allows you to easily load and unload different software packages, libraries, and compilers needed for computational tasks without conflicts. This ensures access to many specific tools, and even different versions of the same tools, without affecting the overall system configuration.</p>"},{"location":"software/modules/#module-commands","title":"Module Commands","text":"<p>Warning</p> <p>If multiple versions of software are available on the system, the newest is made the default. This means loading a module without specifying the version will select the most recent. We strongly recommend including version information in your module statements. This ensures that you maintain a consistent environment for your analyses in the event of a software upgrade.</p> Command Description <code>module avail</code> Display all the software and versions installed on the system <code>module avail &lt;module_name&gt;</code> Display all installed versions of the software <code>&lt;module_name&gt;</code> <code>module list</code> Display the software you have loaded in your environment <code>module whatis &lt;module_name&gt;</code> Displays some descriptive information about a specific module <code>module show &lt;module_name&gt;</code> Displays system variables that are set/modified when loading module <code>&lt;module_name&gt;</code> <code>module load &lt;module_name&gt;</code> Load a software module in your environment <code>module unload &lt;module_name&gt;</code> Unload a specific software package from your environment <code>module swap &lt;module_name&gt;/&lt;version1&gt; &lt;module_name&gt;/&lt;version2&gt;</code> Switch versions of a software module <code>module purge</code> Unload all the software modules from your environment <code>module help</code> Display a help menu for the module command"},{"location":"software/modules/#example","title":"Example","text":"<pre><code>[netid@cpu39 ~]$ module avail python\n\n------------------- /opt/ohpc/pub/modulefiles --------------------\n   python/3.6/3.6.5     python/3.9/3.9.10\n   python/3.8/3.8.2     python/3.11/3.11.4 (D)\n   python/3.8/3.8.12\n[netid@cpu39 ~]$ module load python/3.9\n[netid@cpu39 ~]$ python3 --version\nPython 3.9.10\n[netid@cpu39 ~]$ module swap python/3.9 python/3.11\n\nThe following have been reloaded with a version change:\n  1) python/3.9/3.9.10 =&gt; python/3.11/3.11.4\n\n[netid@cpu39 ~]$ python3 --version\nPython 3.11.4\n</code></pre>"},{"location":"software/modules/#compilers","title":"Compilers","text":"<p>Puma, Ocelote, and El Gato all run CentOS7 and have the following compilers available:</p> Compiler Version Module Command Intel 2020.1 <code>module load intel/2020.1</code> Intel 2020.4 <code>module load intel/2020.4</code> gcc 5.4.0 <code>module load gnu/5.4.0</code> gcc 7.3.0 <code>module load gnu7/7.3.0</code> gcc 8.3.0 <code>module load gnu8/8.3.0 # Loaded by default</code>"},{"location":"software/modules/#installing-additional-software","title":"Installing additional software","text":"<p>To submit a request to have software installed on the UArizona HPC systems, use our HPC Software Install Request form. There is no expected time frame for how long it takes to install software, there are many variables that determine this. If you haven't heard back in a week, it is reasonable for you to follow up with a support ticket</p> <p>You may also install software packages into the space that is allocated to you with your HPC account.  However, you cannot install software that requires root permission or use a method like \"yum install\" that accesses system paths. For information on installing software locally, see our online guide for an example.</p>"},{"location":"software/overview/","title":"Software Overview","text":""},{"location":"software/overview/#overview","title":"Overview","text":"Module availability <p>Software modules are not available on the login nodes. To access them, you will need to connect to a compute either via an interactive session or batch job.</p> <p>Software packages are available as modules and are accessible from the compute nodes of any of our three clusters. </p>"},{"location":"software/overview/#policies","title":"Policies","text":""},{"location":"software/overview/#academicfree-software","title":"Academic/Free Software","text":"<p>There is a plethora of software generally available for scientific and research usage.  We will install that software if it meets the following requirements:</p> Requirements Compatible with our module environment Some software is not written with clusters in mind and tries to install into system directories, or needs a custom environment on every compute node. Generally useful Some software has to be configured to the specific compute environment of the user. You are encouraged to use our \"contrib\" environment to install your own. Public license We do not install software if that would be a violation of its licensing. Reasonably well written Some software takes days of effort and still does not work right.  We have limited resources and reserve the right to \"give up\". Sometimes software is written for workstations and does not function correctly in a shared environment. Downloadable Some software requires additional steps to download installation files, such as registering on a website or accepting a license agreement. In these cases we ask researchers to download files and put them in a directory on the HPC storage. When you submit a software installation request let us know that you have already downloaded the files and provide path to the directory where they are located."},{"location":"software/overview/#commercialfee-based-software","title":"Commercial/Fee-based Software","text":"<p>The University of Arizona Research Computing facility has many commercial and freeware packages installed on our supercomputers. Our approach to acquisition of additional software depends upon its cost, licensing restrictions, and user interest.   </p> Audience Single user interest The license for the software is purchased by the user and his/her department or sponsor.  This software is best installed by the user.  There are two main options; the first and easiest, is to install the software locally in the relevant user's account using the example procedure. The second is to use the \"contrib\" environment.  The advantage is that you can share the software built here with other users. This is created through a support ticket where a consultant will create a \"contrib\" group in which you can build software and add users. Group interest If a package is of interest to a group of several users, the best approach at first is for one user to act as a primary sponsor and arrange to split the procurement/licensing costs among the group. We can install the software and manage the user access according to requests from the group. Broad interest The High Performance Computing team will consider acquiring and supporting software packages that have broad interest among our users. Full facility support will depend on the cost of the package and our ability to comply with any restrictive licensing conditions."},{"location":"software/overview/#federal-regulations","title":"Federal Regulations","text":"<p>By policy, it is prohibited to use any of the facility's resources in any manner that violates the US Export Administration Regulations (EAR) or the International Trafficking in Arms Regulations (ITAR). It is relevant in this regard to be aware that the facility employs analysts who are foreign, nonresident, nationals and who have root-access privileges to all files and data. Specifically, you must agree not to use any software or data on facility systems that are restricted under EAR and/or ITAR.</p>"},{"location":"software/popular_software/R/","title":"R","text":"<p>R is a popular language for data analysis and visualization. Different versions are available as software modules and we provide the graphical interface RStudio for R through our Open OnDemand web interface.</p> <p>Similar to other languages that use package managers to install libraries contributed by the user community, we recommend you create and manage your own local libraries in your account. This ensures a stable global environment for all users and that you have the most control over your packages' versions and dependencies.</p> <p>We provide instructions below for how to create, use, and switch between libraries as well as some debugging techniques for when package installations fail. We also provide some script examples (click the button in the banner at the top of this page) for submitting R scripts as batch jobs.</p> <p>RStudio is a popular method for running analyses (and for good reason!), but for longer-running jobs (say, many hours or days) or workflows that need more flexibility in their environment (e.g., need access to software installed as system modules such as gdal), we recommend batch submissions.</p>"},{"location":"software/popular_software/R/#creating-a-custom-library","title":"Creating a Custom Library","text":"<p>R packages can be finicky. See Switching Between Custom Libraries and Common Problems below to help with frequent user issues.</p> <p>Creating your first library</p> <ol> <li> <p>Make a local directory to store your packages    <pre><code>mkdir -p ~/R/library_4.2\n</code></pre></p> </li> <li> <p>Tell R where the directory is by creating an environment file<sup>1</sup> <pre><code>echo 'R_LIBS=~/R/library_4.2/' &gt;&gt; ~/.Renviron\n</code></pre></p> </li> <li> <p>That's it! Now you can install packages and they will be stored in the directory you just created. For example, to install and load the package <code>ggplot2</code>:     <pre><code>module load R/4.2\nR\ninstall.packages(\"ggplot2\")\n</code></pre></p> </li> </ol>"},{"location":"software/popular_software/R/#switching-between-custom-libraries","title":"Switching Between Custom Libraries","text":"<p>If you're using different versions of R, we recommend you use different libraries. See Common Problems below for more information. When creating a library, consider including pertinent information in the name such as R version. For example, if you wanted to switch to using R 4.1, you could create a directory called <code>library_4.1</code> using: <pre><code>mkdir -p ~/R/library_4.1\n</code></pre> Then, to use your new library, edit your <code>~/.Renviron</code> file<sup>1</sup> using a text editor such as <code>nano</code>: <pre><code>nano ~/.environ\n</code></pre> Once your text editor opens, rename the <code>R_LIBS</code> you previously had in your file. In this case, this value would look like: <pre><code>R_LIBS=~/R/library_4.1\n</code></pre> To exit <code>nano</code>, use Ctrl+X and save at the prompt. Once your file is saved, you're ready to start installing files into your new library. </p>"},{"location":"software/popular_software/R/#common-problems-and-how-to-debug-them","title":"Common Problems and How to Debug Them","text":"<p>Working on a cluster without root privileges can lead to complications. For general information on package installations, see the r-bloggers documentation. For information on common installation problems on our clusters, see the section below with with suggested solutions:</p> AnacondaA Corrupted EnvironmentLibrary IssuesMixing R VersionsOpen OnDemand RStudio IssuesLinking Third Party Software <p>One common reason R packages won't install is an altered environment. This can frequently be caused by the presence Anaconda (or Miniconda) installed locally or initialized in your account from our system module.</p> <p>When Anaconda is initialized, your <code>.bashrc</code> file is edited so that it becomes the first thing in your <code>PATH</code> variable. This can cause all sorts of mayhem. To get around this, you can either remove anaconda from your <code>PATH</code> and deactivate your environment, or comment out/delete the initialization in your <code>~/.bashrc</code> if you want the change to be permanent.</p> <p>If Anaconda is not initialized in your account, there might be other culprits that are corrupting your environment.</p> <p>Look for any of the file types listed below on your account. If you find them, try removing them (make a backup if you need them) and try the installation again.</p> <ul> <li>Saved R sessions. If this is the case, after starting a session, you will get the message \"[Previously saved workspace restored]\". Old sessions are saved as a hidden file <code>.RData</code> in your home directory. </li> <li>Gnu compilers</li> <li>Windows files</li> </ul> <p>Have you set up a custom library? Are you switching between custom libraries? You may want to check that everything is being loaded from the correct location and that there are not multiple or unwanted libraries being used.</p> <p>Double-check that you have an <code>.Renviron</code> file. This is a hidden file located in your home directory and should set the path to your custom R library. If you do not have a custom library name set up, R will create one for you saved as something like: <pre><code>~/R/x86_64-pc-linux-gnu-library\n</code></pre> This directory can lead to unwanted behavior. For example, if you're trying to use a new custom library (such as when switching R version), R will still search x86_64-pc-linux-gnu-library for package dependencies and may cause installs to fail. To fix this, rename these types of folders something unique and descriptive.</p> <p>To set up/switch custom libraries, follow the instructions in the Creating a Custom R Library section above.</p> <p>Because HPC is a cluster where multiple versions of R are available, users should take care to avoid mixing and matching. Because packages often depend on one another, libraries using different versions of R can turn into a tangled mess.  Common errors that can crop up include: \"Error: package or namespace load failed.\"</p> <p>If you're switching R versions, we recommend creating a new library.</p>"},{"location":"software/popular_software/R/#using-rstudio","title":"Using RStudio","text":""},{"location":"software/popular_software/R/#example-r-scripts","title":"Example R Scripts","text":""},{"location":"software/popular_software/R/#popular-packages","title":"Popular Packages","text":"<ol> <li> <p>The file <code>~/.Renviron</code> is a \"dot\" file which means it does not show up when you run a standard <code>ls</code>. Files that start with a <code>.</code> are hidden and are typically used for important configuration information. This particular file can be used to control your R environment for each subsequent time you start a session. All the echo command does is append the line <code>R_LIBS=~/R/library</code> to this file.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"software/popular_software/matlab/","title":"Matlab","text":""},{"location":"software/popular_software/matlab/#overview","title":"Overview","text":"<p>There are four ways to run Matlab:</p> <ol> <li>Using the Matlab graphical application through Open OnDemand.</li> <li>Graphical mode using the Open OnDemand Desktops.</li> <li>The command line version using modules.  This is the most common as you will typically submit a job using SLURM.</li> <li>Python </li> </ol> <p>Like any other application, Matlab has to be loaded as a module before you can use it. To see all the installed versions of the Matlab, use the command module avail matlab.</p>"},{"location":"software/popular_software/matlab/#running-matlab-analyses-in-batch","title":"Running Matlab Analyses in Batch","text":"<p>The typical procedure for performing calculations on UArizona HPC systems is to run your program non-interactively on compute nodes. The easiest way to run Matlab non-interactively is to use input/output redirection. This method uses Linux operators <code>&lt;</code> and <code>&gt;</code> to point Matlab to the input file and tell where to write the output (see the example script). The other method is to invoke Matlab from the Slurm script and execute specified statement using <code>-r</code> option. For details, refer to the manual page for the matlab command.</p> <p>An example batch script might look like the following:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=matlab\n#SBATCH --account=group_name\n#SBATCH --partition=standard\n#SBATCH --ntasks=20\n#SBATCH --nodes\n#SBATCH --mem-per-cpu=5gb\n#SBATCH --time=01:00:00\n\nmodule load matlab/&lt;version&gt;\n\nmatlab -nodisplay -nosplash &lt; script_name.m &gt; output.txt\n</code></pre> <p>The options <code>-nodisplay</code> and <code>-nosplash</code> in the example prevent Matlab from opening graphical elements. To view the full list of options for the <code>matlab</code> command, load the Matlab module and type <code>matlab -h</code> at the prompt. Alternatively, use the link above to see the manual page on the MathWorks website.</p>"},{"location":"software/popular_software/matlab/#parallel-computing-toolbox","title":"Parallel Computing Toolbox","text":""},{"location":"software/popular_software/matlab/#temporary-files","title":"Temporary Files","text":"<p>By default, Matlab PCT will dump files to <code>~/.matlab/MATLAB_VERSION</code>. This causes problems when multiple Matlab PCT jobs are running simultaneously. Users should always define the environment variable <code>MATLAB_PREFDIR</code> so each job uses a unique temporary folder. Files there will be cleaned after the job finishes. For example: <pre><code>export MATLAB_PREFDIR=$(mktemp -d $SLURM_JOBTMP/matlab-XXXX)\n</code></pre></p>"},{"location":"software/popular_software/matlab/#matlab-and-slurm-resource-requests","title":"Matlab and SLURM Resource Requests","text":"<p>If you are trying to run Matlab in parallel interactively, you may encounter the following error:</p> <pre><code>&gt;&gt; Starting parallel pool (parpool) using the 'local' profile ...\nError using parpool (line 149)\nYou requested a minimum of &lt;n&gt; workers, but the cluster \"local\" has the NumWorkers property set to allow a maximum of 1 workers. To run a communicating job on more workers than this\n(up to a maximum of 512 for the Local cluster), increase the value of the NumWorkers property for the cluster. The default value of NumWorkers for a Local cluster is the number of\nphysical cores on the local machine.\n</code></pre> <p>This is caused by an interaction between Slurm and Matlab. To resolve this issue, when requesting <code>&lt;n&gt;</code> cores for your interactive job, you will need to set Slurm's <code>--ntasks</code> directive to 1 and <code>--cpus-per-task</code> to the number of cores you need. For example:</p> <pre><code>$ salloc --nodes=1 --ntasks=1 --cpus-per-task=6 --mem-per-cpu=5GB --time=01:00:00 --job-name=interactive --account=&lt;GROUP&gt; --partition=standard\n</code></pre>"},{"location":"software/popular_software/matlab/#external-resources","title":"External Resources","text":"<p>If you are getting started with Matlab or think there might be a better way, check out the training resources.</p> Resource Link Self-Paced Online Courses https://matlabacademy.mathworks.com/ Matlab Parallel Server https://www.mathworks.com/products/matlab-parallel-server.html#resources Natural Language Processing https://www.mathworks.com/discovery/natural-language-processing.html Matlab Videos https://www.mathworks.com/videos.html"},{"location":"software/popular_software/perl/","title":"Perl","text":""},{"location":"software/popular_software/perl/#accessibility","title":"Accessibility","text":"<p>Perl is installed on the operating system of each node and is currently at 5.16.3: </p> <pre><code>[netid@compute_hostname ~]$ perl --version\n\nThis is perl 5, version 16, subversion 3 (v5.16.3) built for x86_64-linux-thread-multi\n(with 44 registered patches, see perl -V for more detail)\n...\n</code></pre>"},{"location":"software/popular_software/perl/#perl-module-policy","title":"Perl Module Policy","text":"<p>We provide a version of perl through modules or the operating. Installation of additional user libraries can be done in a perl environment using <code>perl-virtualenv</code>.</p> <p>For a helpful Perl tutorial, see: http://www.tutorialspoint.com/perl/perl_modules.htm. Additionally, O'Reilly Media is a well regarded source for Perl </p>"},{"location":"software/popular_software/perl/#installing-perl-packages-using-perl-virtualenv","title":"Installing Perl Packages Using perl-virtualenv","text":"<p>One of the best things about Perl is the number of packages provided by the user community. Installing packages generally requires root access but that is not a viable solution in the HPC environment.</p> <p>An easy solution is to use perl-virtualenv to create a consistent personal Perl environment that will persist for each time you log in. An example of usage:</p> <pre><code>[netid@i0n1 ~]$ perl-virtualenv my_project    # Create virtual environment\nperl path: /usr/bin/perl\nvenv path: /home/uxx/netid/my_project\n[netid@i0n1 ~]$ source my_project/bin/activate # Activate virtual environment\n(my_project)[netid@i0n1 ~]$ cpanm -i Config::Trivial\n--&gt; Working on Config::Trivial\nFetching http://www.cpan.org/authors/id/A/AT/ATRICKETT/Config-Trivial-0.81.tar.gz ... OK\nConfiguring Config-Trivial-0.81 ... OK\n...\n4 distributions installed\n(my_project)[netid@i0n1 ~]$\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/","title":"Anaconda","text":""},{"location":"software/popular_software/python_and_anaconda/anaconda/#overview","title":"Overview","text":"<p>We have three versions of Anaconda installed as system modules for use. You can initialize these in your home directory for access and package management. </p> Version Accessibility 2020.02 <code>module load anaconda/2020.02</code> 2020.11 <code>module load anaconda/2020.11</code> 2022.05 <code>module load anaconda/2022.05</code>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#initializing-anaconda","title":"Initializing Anaconda","text":"<p>Initializing Anaconda in your account only needs to be performed once and is what makes Anaconda available and ready for customization (e.g., installing custom packages) in your account. </p> <p>Tip</p> <ul> <li>Conda will direct you to close and reopen your shell to complete the initialization process. You can skip this by running the command <code>source ~/.bashrc</code> listed in the instructions below. </li> <li>Running <code>conda config --set auto_activate_base false</code> is highly recommended. Conda contains a lot of accompanying software which may interfere with other software. </li> </ul> <p>In an interactive session:</p> <pre><code>module load anaconda/&lt;version&gt;\nconda init bash                  \nsource ~/.bashrc  \nconda config --set auto_activate_base false\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#creating-a-conda-environment","title":"Creating a Conda Environment","text":"<p>Once conda has been configured following the steps above, you can create a local environment. This allows you to control the version of Python you want to use, install your own software, and even create custom Juypyter kernels (making your environment accessible in an OnDemand notebook). To do this, you can use the command <code>conda create</code>. For example, in an interactive session:</p> <pre><code>conda activate\nconda create --name py37 python=3.7 # Build a local environment with a specific version of python\nconda activate py37\n</code></pre> <p>To view the environments available to you, use the command</p> <pre><code>conda env list\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#installing-conda-packages-and-other-software","title":"Installing Conda Packages and Other Software","text":"<p>Once you have created a conda environment, you can install the packages you need. To do this, follow the software-specific installation instructions. This may be as simple as running \"conda install \", or it may involve installing a handful of dependencies. If the installation instructions ask you to create a new environment, you do not have to repeat this step.  <p>Once you have performed the install, you should then be able to access your software within this environment. If you are unable to load your software, check your active environment with</p> <pre><code>conda info\n</code></pre> <p>and the installed packages with </p> <pre><code>conda list\n</code></pre>"},{"location":"software/popular_software/python_and_anaconda/anaconda/#custom-jupyter-kernel","title":"Custom Jupyter Kernel","text":"<p>If you want to make one of your conda environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment:</p> <pre><code>conda activate\nconda activate &lt;your_environment&gt;\n</code></pre> <p>Next, pip-install Jupyter and use it to create a custom kernel using the command <code>ipython</code> and replacing <code>&lt;your_environment&gt;</code> with your own environment's name:</p> <pre><code>pip install jupyter\nipython kernel install --name &lt;your_environment&gt; --user\n</code></pre> <p>Once you've configured your kernel, go to Open OnDemand and start a Jupyter notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom name. For example, if you created a conda environment with the kernel name py38, you should see the following:</p> <p></p> <p>Once you've selected your environment, try checking the Python version in your notebook using the <code>sys</code> module. Additionally, for demonstration purposes, we'll check that a custom package installed in py38 (emoji) can be imported and is working. </p> <p></p>"},{"location":"software/popular_software/python_and_anaconda/python/","title":"Python Modules","text":""},{"location":"software/popular_software/python_and_anaconda/python/#overview","title":"Overview","text":"<p>Different versions of Python are available on HPC both as system modules as well as system software on each compute node. Python 2 is available but is no longer supported by the Python Foundation, so we recommend you use Python 3. Python version 3 requires the <code>python3</code> command or <code>pip3</code> list to differentiate.  It is very different from Python version 2, so do not assume that Python 3 will work for you or that all older modules will work with version 3.</p>"},{"location":"software/popular_software/python_and_anaconda/python/#installation-and-package-policy","title":"Installation and Package Policy","text":"<p>We maintain a two tiered approach to Python packages</p> <ul> <li> <p>Tier 1: We install the basic Python packages that are required by most users (these are mostly libraries rather than packages, such as numpy and scipy). This is done for the versions of Python that we install as modules. Adding some packages might force an upgrade of numpy for example, which might break a user's environment that was dependent on the prior version.</p> </li> <li> <p>Tier 2: For packages that we do not provide we STRONGLY recommend the use of virtualenv, which is detailed below and provides a custom and easy to use person Python environment.</p> </li> </ul>"},{"location":"software/popular_software/python_and_anaconda/python/#available-python-versions","title":"Available Python Versions","text":"<p>Python 2 is no longer officially supported by the Python Software Foundation.</p> <p>Six versions of Python are available on HPC. They are only available on compute nodes and are accessible either using a batch submission or interactive session. </p> Version Accessibility<sup>1</sup> Python 2.7.5 system version (no module) Python 3.6.8 system version (no module) Python 3.6.5 module load python/3.6/3.6.5 Python 3.8.2 module load python/3.8/3.8.2 Python 3.9.10 module load python/3.9/3.9.10 Python 3.11.4 module load python/3.11/3.11.4"},{"location":"software/popular_software/python_and_anaconda/python/#installing-python-packages-using-a-virtual-environment","title":"Installing Python Packages Using a Virtual Environment","text":"Virtual environment tips <ul> <li>Useful overview of virtualenv and venv: InfoWorld Article: Python virtualenv and venv do's and don'ts</li> <li>In the following instructions any module commands have to be run from an interactive session on a compute node</li> </ul> <p>One of the best things about Python is the number of packages provided by the user community. On a personal machine, the most popular method today for managing these packages is the use of a package manager, like <code>pip</code>. Unfortunately, these may require root access preventing you from being able to successfully install the libraries you need.</p> <p>There is an easy solution, however. You can use a virtual environment to create a personal python environment that will persist each time you log in. There is no risk of packages being updated under you for another user and allows greater control over your environment.</p> <p>Virtual Environment Instructions</p> <ol> <li> <p>Set up your virtual environment in your account. This step is done one time only and will be good for all future uses of your Python environment. You will need to be in an interactive session to follow along. </p> <p>Note: In the commands below, <code>/path/to/virtual/env</code> is the path to the directory where all of your environment's executables and packages will be saved. For example, if you use the path <code>~/mypyenv</code>, this will create a directory in your home called <code>mypyenv</code>. Inside will be directories <code>bin</code>, <code>lib</code>, <code>lib64</code>, and <code>include</code>. </p> Python Version \\(\\geq\\) 3.8Python Version \\(&lt;\\) 3.8 <pre><code>module load python/&lt;version&gt;\nvirtualenv --system-site-packages /path/to/virtual/env\n</code></pre> <pre><code>module load python/&lt;version&gt;\npython3 -m venv --system-site-packages /path/to/virtual/env\n</code></pre> </li> <li> <p>To use your new environment, you'll need to activate it. Inside your virtual environment, there's a directory called bin that has a file called activate. Sourcing this will add all of the paths needed to your working environment. To activate, run the following, replacing /path/to/virtual/env with the path specific to your account:</p> <p><pre><code>source /path/to/virtual/env/bin/activate\n</code></pre> 3. Once your environment is active, you can use pip to install your python packages.  You should first upgrade to the latest version of pip. For example, to add the pycurl package to the virtual environment:</p> <p><pre><code>pip install --upgrade pip\npip install pycurl\n</code></pre> 4. That's it! As long as your virtual environment is active, you will have access to the packages installed there. Virtual environments deactivate when you log out, so for each subsequent session or in batch jobs, you will just need to reactivate the environment to get access to your packages: <pre><code>module load python/&lt;version&gt;\nsource /path/to/virtual/env/bin/activate\n</code></pre></p> </li> </ol>"},{"location":"software/popular_software/python_and_anaconda/python/#custom-jupyter-kernel","title":"Custom Jupyter Kernel","text":"<p>Warning</p> <p>The default version of Python available in an OnDemand Jupyter Notebook is 3.8.2. If you would like to create a virtual environment using a standard Python module, you will need to use Python version 3.8.2. If you want to use a different version of python, you can use an Anaconda environment.</p> <p>If you want to make one of your virtual environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment (if you do not have an environment, refer to the sections above on how to do so):</p> <pre><code>module load python/3.8/3.8.2                     \nsource /path/to/your/virtual/environment/bin/activate\n</code></pre> <p>Once your environment is ready to go, pip-install <code>jupyter</code> and create your own custom kernel. The <code>--force-reinstall</code> flag will allow you to install the <code>jupyter</code> package in your local environment and will not affect the system version. This will create a directory in <code>~/.local/share/jupyter/kernels/</code> in your account. In the following commands, replace <code>&lt;your_environment&gt;</code> with the name of your own environment: </p> <pre><code>pip install jupyter --force-reinstall\nipython kernel install --name &lt;your_environment&gt; --user \n</code></pre> <p>Once you've successfully created your kernel, go to Open OnDemand and start a Jupyter Notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom kernel's name. For example, if the custom kernel's name was <code>py38-env</code>:</p> <p></p> <p>Once you've selected your environment, try loading a custom package you've installed to check that everything is working as expected. In this example, we'll check with the non-standard package <code>emoji</code> which has been installed in this environment:</p> <p></p> <ol> <li> <p>Note: The command <code>python</code> defaults to the system 2.7.5 version. To use Python 3, use the command <code>python3</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"software/popular_software/vscode_remote_connection/","title":"VSCode Remote Connection","text":""},{"location":"software/popular_software/vscode_remote_connection/#overview","title":"Overview","text":"<p>Visual Studio Code can be used to edit source code and other files on the HPC systems.  VSCode is available to run directly on HPC through the Open on Demand system. </p> <p>VSCode can also be run locally on laptop or desktop computers and used to make a remote connection to the HPC systems.  This documentation is intended to detail the steps that must be taken to allow such a connection.  The details of how to make such a connection within the VSCode software itself is beyond the scope of this documentation. Refer to the VSCode documentation here: https://code.visualstudio.com/docs/remote/ssh-tutorial.  </p>"},{"location":"software/popular_software/vscode_remote_connection/#creating-and-using-a-remote-connection","title":"Creating and Using a Remote Connection","text":"<p>Remote VSCode sessions must connect to a compute node.  Briefly, the procedure is as follows:</p> <pre><code>graph LR\nA[Request resources&lt;br&gt;on a compute node] --&gt; B[Connect to the HPC VPN]\nB --&gt; C[Connect VSCode to the&lt;br&gt;allocated compute node]</code></pre> <p>The specific steps are these:</p> <ol> <li> <p>Set up ssh key authentication on the file transfer node (<code>filexfer.hpc.arizona.edu</code>), which will allow VSCode to directly connect to the HPC systems without using passwords or Duo authentication.  Our documentation for setting up ssh keys on the bastion host appears here: SSH Keys.  Follow the procedure documented on that page, but replace <code>hpc.arizona.edu</code> in any commands with <code>filexfer.hpc.arizona.edu</code>.</p> </li> <li> <p>Use the Cisco AnyConnect VPN software to connect to <code>vpn.hpc.arizona.edu</code>.  Cisco AnyConnect is the software that you would also use to connect to the general UArizona VPN.  Information on downloading and connecting Cisco AnyConnect appears here: https://it.arizona.edu/service/ua-virtual-private-network-vpn</p> </li> <li> <p>From the login node, start an interactive session for the length of time that you\u2019d like to connect VSCode. Note that executing <code>elgato</code> or <code>ocelote</code> first will request your session on those less busy clusters and it will likely start faster:</p> <pre><code>elgato\ninteractive -t 4:00:00 -a PI_name\n</code></pre> <p>After the interactive session starts, type <code>hostname</code>, which will give something like <code>cpu25.elgato.hpc.arizona.edu</code>. This is the name that you will enter in your local VSCode as the remote computer to connect to. Note that each time you start an interactive session you will likely get a different node, and will therefore need to tell VSCode the specific host to connect to each time.</p> </li> </ol>"},{"location":"software/user_installations/","title":"User Installation Help","text":"<p>While users cannot add or update system software or libraries using tools that require root privileges such as <code>yum</code>, many software packages can be installed locally without needing to be a superuser. Frequently linux packages make use of the \"<code>configure</code>, <code>make</code>, <code>make install</code>\" method and an example of how to do this is shown under Example Installation below. </p> <p>Tip</p> <ul> <li>Software is not available on the login nodes. To install custom software, log into an interactive session.</li> <li>For a typical Linux installation, the default settings may attempt to install files in system directories outside your directories. This is not permitted, so the installation process (specifically, the <code>./configure</code> step) needs to be changed so that files are installed in a specified location.  There is frequently a <code>--prefix=/path/to/software option</code>.</li> </ul> configure/make/make install example <p>Here is a typical example of installing software on a Linux cluster: Installing GROMACS (molecular simulation software):</p> <ol> <li> <p>Get the software</p> <p>I usually download a software package to my laptop, then transfer the downloaded package to my /home directory  for installation. Alternatively, if I have the http or ftp address for the package I need, I can transfer that package directly to my /home directory while logged in using the wget utility:</p> <pre><code>wget ftp://ftp.gromacs.org/pub/gromacs/gromacs-4.5.5.tar.gz\n</code></pre> </li> <li> <p>Unpack the \"tarball\"     <pre><code>tar -zxf gromacs-4.5.5.tar.gz\ncd gromacs-4.5.5\n</code></pre></p> </li> <li>Set up your environment for compiling the software.      Set up your environment for compiling the software.</li> </ol> <p>Here, to compile a parallel build of GROMACS, I need the MPICH2 libraries. Compiling GROMACS also requires some version of the GCC compilers and the FFTW libraries:         <pre><code>module load gnu8/8.3.0 mpich/3.3.1 aocl-fftw/2.2\n</code></pre>     4. Run the configure script. </p>"},{"location":"storage_and_transfers/storage/hpc_storage/checking_your_usage/","title":"Checking Your Storage Usage","text":"Command LineUser Portal <p>To check your storage usage, on a compute node, file transfer node, or login node, use the command <code>uquota</code>. This will show you all the spaces you have access to, their quotas, and current usage. <pre><code>(puma) [netid@junonia ~]$ uquota\n                                        used  soft limit  hard limit\n/groups/pi_netid                            6.6G      500.0G      500.0G\n/home                                      37.1G       50.0G       50.0G\n/xdisk/pi_netid                            12.9G        9.8T        9.8T\n</code></pre></p> <p>You can check your storage allocation through our online user portal by navigating to the Storage tab and clicking Check Disk Quotas:</p> <p></p>"},{"location":"storage_and_transfers/storage/hpc_storage/storage_summary/","title":"HPC Storage Summary Overview","text":"<p>The University\u2019s Research Data Center provides data storage for active analysis on the high-performance computers (HPCs). Using central computing storage services and resources, University researchers, faculty researchers, and post-doctoral researchers are able to:</p> <ul> <li>Share research data in a collaborative environment with other UArizona affiliates on the HPC system</li> <li>Store large-scale computational research data</li> <li>Request additional storage for further data analysis</li> </ul> <p>The storage is mounted as a filesystem and all the clusters have access to the same filesystems.</p> <p>Every user has access to individual and shared storage on the system where they can host data for active analyses. A summary of these locations is shown below:</p> Path Description Quota Duration <code>/home/uxx/netid</code> An individual storage allocation provided for every HPC user 50gb Accessible for the duration of user's account <code>/groups/pi_netid</code> A communal storage allocation provided for every research group 500gb Accessible for the duration of a PI's account <code>/xdisk/pi_netid</code> Temporary communal storage available for every group on request. See xdisk section below for details. 200gb-20tb Up to 300 days <code>/tmp</code> Local storage available on individual compute nodes. \\(&lt;\\) 800GB to 1.4TB Only accessible for the duration of a job's run."},{"location":"storage_and_transfers/storage/hpc_storage/xdisk/","title":"xdisk","text":""},{"location":"storage_and_transfers/storage/hpc_storage/xdisk/#what-is-xdisk","title":"What is xdisk?","text":"<p>xdisk is a temporary storage allocation available to all PIs and offers up to 20 TB of usable space for their group for up to 300 days. PIs may only have one active xdisk at a time.</p> <p>A PI can request an allocation either via the command line or through our web portal (no paperwork necessary!). Only faculty members (PIs) may request, alter, or delete an allocation from the command line. Members of their research group may be delegated management rights allowing them to manage a group's xdisk on their PI's behalf through our web portal.</p> <p>Once an xdisk allocation is created, it is immediately available for use. Groups can find their allocations under <code>/xdisk/pi_netid</code>. By default, a subdirectory is created for each group member under <code>/xdisk/pi_netid/netid</code>. If a group member is added after the allocation is created, a directory is not automatically created for them. To add one, reach out to our consultants.</p> <p>Because xdisk allocations are temporary, they will expire as soon as their time limit is reached. Warnings will be sent to every group member at their netid@arizona.edu addresses beginning one week before the expiration. It is the group's responsibility to renew xdisk allocations or copy files to an alternate storage location prior to the expiration date. Once an xdisk allocation expires, everything in it is permanently deleted.</p> <p>PIs may request a new xdisk allocation immediately after their previous one has expired. This ensures groups will always have access to increased storage on HPC on a rolling basis with the requirement that housekeeping be done once per academic year. </p>"},{"location":"storage_and_transfers/storage/hpc_storage/xdisk/#requesting-modifying-and-deleting-an-allocation","title":"Requesting, Modifying, and Deleting an Allocation","text":"Requesting an AllocationModifying an AllocationDeleting an Allocation <p>Warning</p> <p>If a group has an active xdisk allocation, a new one cannot be created until the active allocation expires or is deleted.</p> <p>PIs or delegates can request an xdisk allocation at any time through the user portal. Under the Storage tab, select Manage XDISK</p> <p></p> <p>This will open a web form where you can enter your size and duration requirements. The maximum size that can be requested is 20000GB and the maximum duration is 300 days. If a PI has created multiple research groups, you can specify the desired group ownership for the allocation from the Group dropdown menu. Once you click Ok, your allocation should immediately be available.</p> <p></p> <p>PIs or delegates may manage their xdisk allocation at any time through the user portal. Under the Storage tab, select Manage XDISK</p> <p></p> <p>This will open a form which will allow you to modify the size and duration of your xdisk. Xdisk allocations cannot be increased beyond 20000GB and the maximum duration of 300 days. Note: the Group field may only be modified at the time of the allocation's creation.</p> <p></p> <p>PIs or delegates may delete their xdisk allocation at any time through the user portal. Under the Storage tab, select Delete XDISK</p> <p></p> <p>Clicking this link will open a window with a prompt. Type confirm and then select Delete XDISK to complete the process.</p> <p></p> <p>If you would like to request a new xdisk, you may do so as soon as the request is processed. Note: sometimes processing the request can take a few minutes, depending on the number of files and the size of the allocation.</p>"},{"location":"storage_and_transfers/storage/hpc_storage/xdisk/#cli-commands","title":"CLI Commands","text":"<p>Warning</p> <p>The xdisk CLI commands are usable by PIs only. Group delegates can manage allocations via the user portal.</p> <p>xdisk is a locally written utility for PI's to create, delete, resize, and expire (renew) xdisk allocations. Any PIs who wish to utilize the CLI to manage their allocations can do so using the syntax shown below:</p> xdisk Function Command Examples Display xdisk help <code>xdisk -c help</code> <code>$ xdisk -c help</code> View Current Information <code>xdisk -c query</code> <code>$ xdisk -c query</code><code>XDISK on host: ericidle.hpc.arizona.edu</code><code>Current xdisk allocation for &lt;netid&gt;:</code><code>Disk location: /xdisk/&lt;netid&gt;</code><code>Allocated size: 200GB</code><code>Creation date: 3/10/2020 Expiration date: 6/8/2020</code><code>Max days: 45    Max size: 1000GB</code> Create an xdisk <code>xdisk -c create -m [size in gb] -d [days]</code> <code>$ xdisk -c create -m 300 -d 30</code><code>Your create request of 300 GB for 30 days was successful.</code><code>Your space is in /xdisk/&lt;netid&gt;</code> Extend xdisk Expiration Date <code>xdisk -c expire -d [days]</code> <code>$ xdisk -c expire -d 15</code><code>Your extension of 15 days was successfully processed</code> Resize an xdisk Allocation <code>xdisk -c size -m [size in gb]</code> <code>$ # Assuming an initial xdisk allocation size of 200 gb</code><code>$ xdisk -c size -m 200</code><code>XDISK on host: ericidle.hpc.arizona.edu</code><code>Your resize to 400GB was successful</code><code>$ xdisk -c size -m -100</code><code>XDISK on host: ericidle.hpc.arizona.edu</code><code>Your resize to 300GB was successful</code> Delete an xdisk Allocation <code>xdisk -c delete</code> <code>$ xdisk -c delete</code><code>Your delete request has been processed</code>"},{"location":"storage_and_transfers/storage/overview/","title":"Storage Overview","text":""},{"location":"storage_and_transfers/storage/overview/#where-should-i-store-my-data","title":"Where Should I Store My Data?","text":"<ol> <li>Data undergoing active analyses should be stored in HPC's local High Performance Storage.</li> <li>Large amounts of data not requiring immediate access from our HPC compute nodes can be stored at reasonable rates on our Rental Storage. </li> <li>RDAS is a research data service which supports the mounting of SMB shares. The supported operating systems are MacOS, Linux, and Windows. It provides 5TB of free storage. </li> <li>Research data not requiring immediate access should be stored in General Research Data Storage (Tier 2). For example:<ol> <li>Large datasets where only subsets are actively being analyzed</li> <li>Results no longer requiring immediate access</li> <li>Backups (highly encouraged!)</li> </ol> </li> <li>Data that require HIPAA-compliance can be stored on Soteria (currently in the pilot phase).</li> </ol>"},{"location":"storage_and_transfers/storage/overview/#storage-option-summary","title":"Storage Option Summary","text":"Purpose Capacity Cost Restricted Data Access Duration Backup Primary HPC Storage Research data. Supports compute. Directly attached to HPC. <code>/home</code>: 50GB<code>/groups</code>: 500GB<code>/xdisk</code>: 20TB Free Not for restricted data Directly mounted on HPC. Also uses Globus and DTNs. Long term. Aligns with HPC purchase cycle. No R-DAS Research Desktop Attached Storage - SMB shares 5TB Free Not for restricted data Mounted to workstations as shares Long term No Rental Storage Research data. Large datasets. Typically for staging to HPC Rented per Terabyte per year Rental rate: $47.35 per TB per year Not for restricted data Uses Globus and DTNs. Copy data to Primary Long term. Aligns with HPC purchase cycle No Tier 2 Typically research data. Unused data is archived 15GB to TBs Tier-based system. First 1TB of active data and archival data are free. Active data &gt; 1TB is paid. Not for restricted data Uses Globus and AWS command line interface Typically long term since use of Glacier is free and slow Archival ReData Research data. Managed by UA Libraries Quota system Free Not for restricted data Log in and fill out fields, then upload Longer than 10 years No Soteria HIPAA Secure data enclave Individual requests Free upon qualification Restricted data; HIPAA, ePHI HIPAA training required, followed by request process Long term No Box General Data 50gb Free Not for restricted data Browser Long term No Google Drive General data 15gb Free. Google rates for amounts &gt; 15gb Not for restricted data Browser Unlimited usage expires March 1, 2023 No"},{"location":"storage_and_transfers/storage/overview/#nih-data-management-and-sharing-policy","title":"NIH Data Management and Sharing Policy","text":"<p>The NIH has issued a new data management and sharing policy, effective January 25, 2023. The university libraries now offers a comprehensive guide for how to navigate these policies and what they mean for you.</p> <p>What's new about the 2023 NIH Data Management and Sharing Policy?   Previously, the NIH only required grants with $500,000 per year or more in direct costs to provide a brief explanation of how and when data resulting from the grant would be shared.   The 2023 policy is entirely new. Beginning in 2023, ALL grant applications or renewals that generate Scientific Data must now include a robust and detailed plan for how you will manage and share data during the entire funded period. This includes information on data storage, access policies/procedures, preservation, metadata standards, distribution approaches, and more. You must provide this information in a data management and sharing plan (DMSP). The DMSP is similar to what other funders call a data management plan (DMP).   The DMSP will be assessed by NIH Program Staff (though peer reviewers will be able to comment on the proposed data management budget). The Institute, Center, or Office (ICO)-approved plan becomes a Term and Condition of the Notice of Award.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/","title":"Research Desktop Attached Storage (R-DAS)","text":""},{"location":"storage_and_transfers/storage/rdas_storage/#overview","title":"Overview","text":"<p>Notice</p> <p>R-DAS storage is not mounted on HPC compute or login nodes.</p> <p>Tip</p> <p>Group Sharing: Faculty members/PIs can share their allocations with group members. To do so, in step 6 in the **Accessing Your R-DAS Allocation\" section below, group members will choose the allocation with their faculty member's/PI's NetID.</p> <p>On October 16, 2023, we went live with the Research Desktop Attached Storage Array (R-DAS). R-DAS provides up to 5 TB of no-cost storage capacity for each PI group. Our requirement was to enable our users to easily share data with other research group members. You can treat the allocation as a drive mounted on your local computer. R-DAS is intended for storing open research data, but not controlled or regulated data.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#technical-requirements","title":"Technical Requirements","text":"<p>R-DAS is a storage service backed by a Qumulo branded storage array. It supports the mounting of SMB shares for SMB 3.1. The supported operating systems are MacOS (Monterey or higher), Linux (kernel 3.7 or higher), and Windows (Windows 10 or 11).</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#performance","title":"Performance","text":"<p>The storage array is located in the Research Data Center to benefit from the network infrastructure in the Computer Center. The performance you experience will depend on your network connectivity. The best case is likely wired ethernet in a newer building. Off campus usage requires connection to the VPN, and so performance can be variable. Our testing off campus regularly reached 3 MB/s.</p>"},{"location":"storage_and_transfers/storage/rdas_storage/#requesting-an-allocation","title":"Requesting an Allocation","text":"<p>PIs can request an allocation on R-DAS from https://portal.hpc.arizona.edu/portal</p> <ol> <li>Go to the Storage tab</li> <li> <p>Select Create Shared Desktop Storage under Research Desktop Storage</p> <p></p> </li> <li> <p>Select Create from the window that opens. </p> <p></p> </li> <li> <p>A window will open with the MOU agreement. Review it and, if it is acceptable to you, select Agree.</p> <p></p> </li> <li> <p>You can now select the View Shared Desktop Storage option from the main Storage page in the user portal</p> <p></p> </li> </ol>"},{"location":"storage_and_transfers/storage/rdas_storage/#accessing-your-r-das-allocation","title":"Accessing Your R-DAS Allocation","text":"<p>Tip</p> <p>UA IP Address Required: To access your R-DAS allocation you need to be connected to either the UA campus network, or the UA SSL VPN. For information about connecting to a VPN, see VPN - Virtual Private Network. If you are accessing your R-DAS allocation from an HPC cluster, then you are already on the UA campus network and do not need to connect to the UA SSL VPN. </p> <p>R-DAS can be accessed from Linux, MacOS, or Windows. The screenshots are intended to be visual aids, but they include information from the consulting team. When you proceed, please enter your own information.</p> <p>Choose your operating system</p> Linux/HPCMac OSWindows <p>Tip</p> <p>To connect to R-DAS from HPC, do not attempt to run <code>sudo</code> commands, these are only meant for your personal Linux machines. All required packages are already installed on the HPC clusters.</p> <p>First, install the necessary software packages to access your allocation</p> <p>Choose your distribution</p> Debian/UbuntuFedora/CentOSOther Linux Distributions <pre><code>sudo apt install samba gvfs-backends smbclient\n</code></pre> <pre><code>sudo yum install samba gvfs-samba samba-client \n</code></pre> <p>Please check the documentation of your distribution.</p> <p>Next, access your allocation</p> <p>Choose your connection method</p> GUICLI <p>If you are on a Mac, then you can mount your R-DAS allocation as a local drive with the following steps:</p> <ol> <li>Go to Finder</li> <li>Select Go from the top menu bar.</li> <li>From the drop-down menu, select Connect to Server.</li> <li> <p>In the window that opens, enter <code>smb://rdas.hpc.arizona.edu</code> in the address bar, and select Connect.</p> <p></p> </li> <li> <p>After a few moments a window opens prompting for your Name (UA NetID) and Password (UA NetID password). After entering the details, select Connect.</p> <p></p> </li> <li> <p>A window will open with the list of allocations on the array. Select the allocation named after your group, and then select OK.</p> <p></p> </li> </ol>"},{"location":"storage_and_transfers/storage/rdas_storage/#faqs","title":"FAQs","text":"<p>For any questions about R-DAS, see our Storage FAQs page for some common issues. </p>"},{"location":"storage_and_transfers/storage/rental_storage/","title":"Rental Storage","text":""},{"location":"storage_and_transfers/storage/rental_storage/#overview","title":"Overview","text":"<p>Danger</p> <p>Your <code>/rental</code> allocation is only mounted on our Data Transfer Nodes and is not directly accessible from the HPC login or compute nodes. </p> <p>The storage we offer is configured with consideration given to the direct relationship between capacity, performance and cost. We offer a rental storage solution that has less performance and so is affordable for researchers to rent. This storage array is located in the Research Data Center and is mounted on our data transfer nodes which makes it more accessible than most other options. Data in your rental space will be accessible via the command line and the graphical transfer application Globus. </p>"},{"location":"storage_and_transfers/storage/rental_storage/#pricing","title":"Pricing","text":""},{"location":"storage_and_transfers/storage/rental_storage/#cost-per-year","title":"Cost per Year","text":"<p>The first-year rate is $94.50 per TB, and RII will provide matching funds for first-year allocations to make the actual first-year cost to researchers $47.35. These matching funds will be applied automatically, so in practice you will only see the $47.35 rate. The ongoing rate after year one is $47.35 per TB per year.</p>"},{"location":"storage_and_transfers/storage/rental_storage/#billing","title":"Billing","text":"<p>Researchers must provide a KFS account for this service. Charges will be applied at the end of the academic year (June).</p>"},{"location":"storage_and_transfers/storage/rental_storage/#size-modifications","title":"Size Modifications","text":"<p>If the size of your allocation is modified, you will be billed for the maximum amount of space reserved during that fiscal year. </p>"},{"location":"storage_and_transfers/storage/rental_storage/#data-location","title":"Data Location","text":"<p>Danger</p> <p>Your <code>/rental</code> allocation is only mounted on our Data Transfer Nodes and is not directly accessible from the HPC login or compute nodes. </p> <p>Your rental space will be on a storage array in our Research Data Center and mounted on our data transfer nodes (hostname: <code>filexfer.hpc.arizona.edu</code>). Your space will be findable under </p> <pre><code>/rental/&lt;pi_netid&gt;\n</code></pre> <p>Where <code>&lt;pi_netid&gt;</code> is the NetID of the faculty member who requested the allocation.</p>"},{"location":"storage_and_transfers/storage/rental_storage/#data-transfers","title":"Data Transfers","text":"<p>A few data transfer options are Globus, <code>sftp</code>, and <code>scp</code> which will allow you to move data external to the data center to your allocation.</p> <p>For data transfers between HPC storage (<code>/home</code>, <code>/groups</code>, or <code>/xdisk</code>) and your rental allocation, you may also <code>ssh</code> into <code>filexfer.hpc.arizona.edu</code> and use <code>mv</code> or <code>cp</code>. For large copies done using this method, we recommend using a <code>screen</code> session to prevent timeouts. For example: <pre><code>[netid@home ~]$ ssh netid@filexfer.hpc.arizona.edu\nAuthorized uses only. All activity may be monitored and reported.\nLast login: Fri Sep 15 10:53:27 2023\n[netid@sdmz-dtn-3 ~]$ cd /rental/pi/netid/example\n[netid@sdmz-dtn-3 example]$ screen\n[netid@sdmz-dtn-3 example]$ cp -r /xdisk/pi/CONTAINERS/ $PWD/CONTAINERS\n[netid@sdmz-dtn-3 example]$ ls\nCONTAINERS\n[netid@sdmz-dtn-3 example]$ exit # exits screen session\n[netid@sdmz-dtn-3 example]$ exit # exits filexfer node\nlogout\nCome again soon!\nConnection to filexfer.hpc.arizona.edu closed.\n[netid@home ~]$\n</code></pre></p>"},{"location":"storage_and_transfers/storage/rental_storage/#how-to-request-rental-storage","title":"How to Request Rental Storage","text":"<p>Warning</p> <p>Allocations up to 20TB in size can be requested through the user portal. For allocations larger than 20TB, contact our consulting team for help.</p> <p>Tip</p> <p>It can take a few days to process the request is it has to route through the Financial Services Office (FSO). You will receive an email confirmation once it is complete.</p> <ol> <li> <p>PIs or Group Delegates can request rental storage on behalf of their group. To do so, navigate to the User Portal in your browser, then choose the Storage tab</p> <p></p> </li> <li> <p>Select Submit Rental Storage Request under the Rental Storage heading and fill out the form. </p> <p></p> </li> <li> <p>Once your space has been created, you will receive an email notification that it is ready for use.</p> </li> </ol>"},{"location":"storage_and_transfers/storage/rental_storage/#resizing-your-allocation","title":"Resizing Your Allocation","text":""},{"location":"storage_and_transfers/storage/tier2_storage/","title":"Tier 2 AWS Storage","text":"<p>Research Technologies in partnership with UITS has implemented an AWS rental storage solution. This AWS option is called Tier 2 which differs from Tier 1, the primary storage that is directly connected to the HPC clusters. Tier 1 is very fast, very expensive, and immediately available for active analyses. Tier 2 is intended for data not immediately undergoing active analyses and for backups (highly encouraged!). Researchers can use the software Globus to move data to Tier 2, and can also move data from other sources (called endpoints) like Google Drive. The data in Tier 2 will not be mounted on HPC, and so Globus will be used to move it back to Tier 1 if needed.</p> <p>AWS storage is organized in buckets. One S3 intelligent tiering bucket is supported per KFS account. A PI could sponsor multiple buckets by submitting separate requests each with a unique KFS number, and then provide permissions as they see fit.  Note this is different from Google Drive where anyone could create one.</p>"},{"location":"storage_and_transfers/storage/tier2_storage/#data-lifecycle","title":"Data Lifecycle","text":"<p>Danger</p> <p>Because AWS is set up for automatic archiving, files are moved into tiers where restore requests need to be submitted for each individual file that needs to be downloaded after a period of time. We strongly recommend archiving directories (.zip, .tar.gz files, etc) prior to moving them to AWS. This will significantly speed up your data transfers as well as reduce the complexity of file restorations. If you transfer hundreds or thousands of files to AWS, restore requests may take days or weeks to submit. </p> <p>Small files</p> <p>Warning: Very small files (less than 128KB ) are not subject to intelligent tiering and are not migrated to Glacier/Deep Glacier. This means they are permanently stored in the paid storage class. If you have many small files, we recommend making archives of your directories (.tar.gz, .zip, etc) prior to uploading them to AWS. This will also reduce transfer times significantly. </p> <p>Tier 2 AWS buckets use intelligent tiering to determine the archival status of files. When data are first uploaded to a group's bucket, they are in the standard access class. This essentially means they are stored on higher performant storage and are available for immediate download. After three months of inactivity(1), data are automatically migrated to Glacier storage. This is less performant and data are no longer instantly downloadable. Users will need to request a restore before downloading their files. Restore requests can be submitted either in the user portal or using a command line tool available on our compute nodes (more details below).</p> <ol> <li>Activity in this context means the user has interacted with the file in some way, e.g. by downloading. </li> </ol> <p>After three months of inactivity in the Glacier access tier, data are automatically migrated to Deep Glacier. Deep Glacier is tape storage and requires a restore request to be submitted to download files, similar to Glacier. Deep Glacier restore requests typically take more time than Glacier files. </p> <pre><code>graph LR\n  A[Data uploaded&lt;br&gt;to AWS bucket] --&gt; B[Data stored in&lt;br&gt;standard access tier];\n  B --&gt;|Data downloaded| C[Counter resets];\n  C --&gt; B;\n  B --&gt;|Three months inactivity| D[Glacier access&lt;br&gt;storage tier];\n  D --&gt; |Restore request&lt;br&gt;submitted| C; \n  D --&gt; |Three months inactivity| E[Deep Glacier access&lt;br&gt;storage tier];\n  E --&gt; |Restore request&lt;br&gt;submitted| C;\n</code></pre>"},{"location":"storage_and_transfers/storage/tier2_storage/#pricing","title":"Pricing","text":""},{"location":"storage_and_transfers/storage/tier2_storage/#storage-costs","title":"Storage Costs","text":"<p>Part of this service is paid for by researchers and the rest is either subsidized or covered by UITS. The data stored in S3 will be billed monthly by AWS to the KFS account used when this is set up. Data in archival storage will be stored at no cost to the researcher. You will receive an email with detailed billing information when charges are made to your account.</p> Tier Cost to Researchers Duration Data Retrieval Standard $0 (First TB)$23/TB/Month<sup>1</sup> (data &gt; 1TB) Three months (if data not downloaded). After three months, untouched data automatically migrate to Glacier. Data Retrieval Glacier $0 Three months (if data not downloaded*). After three months, untouched data automatically migrated to Deep Glacier. A restore request must be submitted. Restores may take a few minutes to hours. Data may be transferred once restored. Deep Glacier $0 Unlimited (if data not downloaded) A restore request must be submitted. Restores may take a few hours to days. Data may be transferred once restored."},{"location":"storage_and_transfers/storage/tier2_storage/#data-transfer-costs","title":"Data Transfer Costs","text":"<p>Data movement costs are subsidized by UITS so researchers are not charged any AWS transfer fees.</p>"},{"location":"storage_and_transfers/storage/tier2_storage/#request-a-bucket","title":"Request a Bucket","text":"<p>Who can submit a request?</p> <p>A group's PI is responsible for submitting a storage request unless they have a group delegate. Delegates may perform Tier 2 storage operations on behalf of their PI by clicking Switch Users and entering their PI's NetID in the user portal. PIs may add delegates by entering their group member's NetID in the user portal under Add Delegate.</p> <p>First, log into the User Portal and navigate to the Storage tab at the top of the page. Select Submit Tier 2 Storage Request.</p> <p></p> <p>This will open a web form. Add your KFS number under KFS Number(1) and the email address for the Department's financial contact under Business contact email. There will also be two optional fields: Subaccount and Project. These are used for tagging/reporting purposes in KFS billing. You can safely leave these entries blank if you're not sure what they are. Once you have completed the form, click Send request. The KFS number can be obtained from the same financial contact.</p> <ol> <li>A KFS number is used for accounting purposes and used by your Department's finance specialist. If you do not know your KFS number, contact your department's financial office. </li> </ol> <p></p> <p>Submitting this form will open a ServiceNow ticket. Processing time may take up to a few days. Once your request has been completed, you will receive a confirmation email with a link to subscribe for account alerts (e.g., notifications for a sudden spike in usage). </p> <p></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#checking-your-usage","title":"Checking Your Usage","text":"<p>Tip</p> <p>AWS runs a batch update every night with the results being reported the following day. This means that if you have made any modifications to your allocation, your usage information will not be accurately reflected until the next batch update. </p> User PortalCLI <p>You may check your storage usage at any time in the User Portal. Navigate to the Storage tab, select View Tier 2 Storage, and click Query Usage.</p> <p></p> <p>To view the size and storage classes of individuall objects, you will need to use the CLI interface.</p> <p>A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool (see the next section). This can be accessed using: <pre><code>(elgato) [netid@junonia ~]$ interactive\n[netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer\n</code></pre> For information on usage: <pre><code>tier2-viewer --help\n</code></pre> To play a tutorial in your terminal, use: <pre><code>tier2-viewer --example\n</code></pre></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#generate-access-keys","title":"Generate Access Keys","text":"<p>Access keys will allow you to connect your AWS bucket using tools such as Globus which will enable you to make transfers directly between HPC and your Tier 2 storage allocation. Access keys should be treated as passwords and should only be shared with trusted group members and collaborators. </p> <p>To generate an access key, log into the User Portal, navigate to the Storage tab, and select Regenerate IAM Access Key.</p> <p></p> <p>This will generate a KeyID and Secret Access Key used to establish the connection. Save these keys somewhere safe since once the window is closed, they cannot be retrieved. If you forget your keys, you can regenerate a new pair.</p> <p></p>"},{"location":"storage_and_transfers/storage/tier2_storage/#transferring-files","title":"Transferring Files","text":"<p>The easiest way to transfer files from AWS to HPC is using Globus. We have instructions in our Transferring Files page on how to set up an endpoint to access your AWS bucket as well as how to initiate file transfers.</p> <p>Some other file transfer programs include rclone and Cyberduck.</p>"},{"location":"storage_and_transfers/storage/tier2_storage/#restoring-archived-data","title":"Restoring Archived Data","text":"<p>Data that are not touched for at least 90 and 180 days are automatically retiered to archival storage (Glacier and Deep Glacier, respectively). Files stored in an archival state cannot be transferred out of AWS until they are restored. Restore requests can be submitted either via the User Portal or using a command line utility available on our compute nodes. </p> <p>The time it takes for an object to be retrieved is dependent on its storage class. Objects in Glacier may take a few hours while objects in Deep Glacier may take up to a day or two. Once an object has been restored, it will move back up to the frequent access tier and can be downloaded using any transfer method you prefer.</p> User PortalCLI <p>File count</p> <p>Warning: If you are restoring a directory, the portal will only support restore requests for directories containing up to 50 files. If you need to restore a large directory, use the CLI.</p> <p>In the User Portal, navigate to the Storage tab by clicking Restore Archived Tier 2 Storage Object:</p> <p></p> <p>This will open a box where you can enter the path to a file or directory in your bucket. Enter the path to the object you would like to restore:</p> <p></p> <p>Once you select an object, click Send Request to initiate the retrieval</p> <p></p> <p>A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool (see the next section). This can be accessed using: <pre><code>(elgato) [netid@junonia ~]$ interactive\n[netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer\n</code></pre> For information on usage: <pre><code>tier2-viewer --help\n</code></pre> To play a tutorial in your terminal, use: <pre><code>tier2-viewer --example\n</code></pre> The <code>--restore</code> flag can be used to either restore a file or a full directory. </p> <ol> <li> <p>More up-to-date pricing information can be found on AWS's website.\u00a0\u21a9</p> </li> </ol>"},{"location":"storage_and_transfers/transfers/cyberduck/","title":"Cyberduck","text":"<p>Cyberduck is a graphical file transfer application that can be used to connect to and transfer files between your local computer and various remote servers and cloud storage services. To get started, you can download the application onto your local workstation from their website here: https://cyberduck.io/</p>"},{"location":"storage_and_transfers/transfers/cyberduck/#initiating-transfers","title":"Initiating Transfers","text":"<p>Once you have Cyberduck installed, open the software and select New Browser from the toolbar</p> <p></p> <p>In the window that opens, select Open Connection</p> <p></p> <p>This will give you a number of options to choose from.</p>"},{"location":"storage_and_transfers/transfers/cyberduck/#connection-options","title":"Connection Options","text":"HPCGoogle Drive <p>To connect to HPC, select SFTP (SSH File Transfer Protocol) from the top dropdown, enter <code>filexfer.hpc.arizona.edu</code> under Server, and your university credentials under Username and Password.</p> <p></p> <p>Once you click Connect, you will be prompted to duo-authenticate</p> <p></p> <p>If your connection is successful, you will see a window open with the contents of your home directory.</p> <p></p> <p>To connect to Google Drive, select the Google Drive option from the dropdown tab and select Connect</p> <p></p> <p>This will open a browser where you will be prompted to log into your Google Drive account.</p> <p></p> <p>Once you have successfully logged in, grant access to Cyberduck where prompted. If this process is successful, you should see a connection window where you can navigate through the contents of your Google Drive.</p> <p></p> <p>To initiate transfers, simply drag and drop your files between the Cyberduck window and your local computer. If you have multiple connections open, you can also initiate transfers between two remotes by dragging and dropping files between two connection windows.</p>"},{"location":"storage_and_transfers/transfers/globus/","title":"Globus","text":""},{"location":"storage_and_transfers/transfers/globus/#overview","title":"Overview","text":"<p>Globus</p> <p>GridFTP is an extension of the standard File transfer Protocol (FTP) for high-speed, reliable, and secure data transfer. Because GridFTP provides a more reliable and high performance file transfer (compared to protocols such as SCP or rsync), it enables the transmission of very large files. GridFTP also addresses the problem of incompatibility between storage and access systems. (You can read more about the advantages of GridFTP here).</p> <p>To use GridFTP, we recommend you use Globus. Globus uses endpoints to make transfers. </p>"},{"location":"storage_and_transfers/transfers/globus/#accessing-globus","title":"Accessing Globus","text":"<p>Globus can be used as a web application. To access it, go to https://www.globus.org/. Next, click Log In in the upper right-hand corner</p> <p></p> <p>On the next page, enter The University of Arizona in the search field and click the result.</p> <p></p> <p>This will take you through the standard university WebAuth login process. Once you successfully log in, you will be placed in a File Manager window. The various steps for setting up endpoints, initiating transfers, and viewing a transfer's progress can be found in the sections below.</p> <p></p>"},{"location":"storage_and_transfers/transfers/irods/","title":"iRods","text":""},{"location":"storage_and_transfers/transfers/rsync/","title":"rsync","text":""},{"location":"storage_and_transfers/transfers/rsync/#overview","title":"Overview","text":"<p>rsync is a fast and extraordinarily versatile file copying tool. It synchronizes files and directories between two different locations (or servers). Rsync copies only the differences of files that have actually changed. An important feature of rsync not found in most similar programs/protocols is that the mirroring takes place with only one transmission in each direction. Rsync can copy or display directory contents and copy files, optionally using compression and recursion. You use rsync in the same way you use scp. You must specify a source and a destination, one of which may be remote. </p>"},{"location":"storage_and_transfers/transfers/rsync/#example-1","title":"Example 1","text":"<p>Recursively transfers all files from the directory <code>src/directory-name</code> on the machine <code>computer-name</code> into the <code>/data/tmp/directory-name</code> directory on the local machine. The files are transferred in archive mode, which ensures that symbolic  links, devices, attributes, permissions, ownerships, etc. are preserved in the transfer. Additionally, compression will be used to reduce the size of data portions of the transfer. </p> <pre><code>rsync -avz  computer-name:src/directory-name  user@remote.host:/data/tmp --log-file=hpc-user-rsync.log  \n</code></pre>"},{"location":"storage_and_transfers/transfers/rsync/#example-2","title":"Example 2","text":"<p>A trailing slash on the source changes the behavior slightly. This inclusion avoids creating an additional directory level at the destination. You can think of a trailing / on a source as meaning \u201ccopy the contents of this directory\u201d as opposed to \u201ccopy the directory by name\u201d, but in both cases the attributes of the containing directory are transferred to the containing directory on the destination. </p> <pre><code>rsync -avz  computer-name:src/directory-name/  user@remote.host:/data/tmp --log-file=hpc-user-rsync.log \n</code></pre>"},{"location":"storage_and_transfers/transfers/rsync/#additional-options","title":"Additional Options","text":"Flag Meaning <code>-a</code> Archive mode; will preserve time stamps <code>-v</code> Increase verbosity <code>-z</code> Compress file data during the transfer <code>--log-file=FILE</code> Log everything done in specified <code>FILE</code>"},{"location":"storage_and_transfers/transfers/scp/","title":"SCP","text":"<p>SCP uses Secure Shell (SSH) for data transfer and utilizes the same mechanisms for authentication, thereby ensuring the authenticity and confidentiality of the data in transit.</p>"},{"location":"storage_and_transfers/transfers/scp/#maclinux","title":"Mac/Linux","text":"<p>You will need to use an SSH v2 compliant terminal to move files to/from HPC. For more information on using SCP, use man scp.</p>"},{"location":"storage_and_transfers/transfers/scp/#moving-a-file-or-directory-to-hpc","title":"Moving a File or Directory to HPC","text":"<p>In your terminal, navigate to the desired working directory on your local machine (laptop or desktop usually). To move a file or directory to a designated subdirectory in your account on HPC:</p> <pre><code>scp -rp filenameordirectory NetId@filexfer.hpc.arizona.edu:subdirectory\n</code></pre>"},{"location":"storage_and_transfers/transfers/scp/#downloading-a-file-or-directory-from-hpc","title":"Downloading a File or Directory from HPC","text":"<p>Tip</p> <p>The space folllowed by a period at the end of the command below means the destination is the current directory</p> <p>In your terminal, navigate to the desired working directory on your local machine. The copy a remote file from HPC to your current directory: <pre><code>scp -rp NetId@filexfer.hpc.arizona.edu:filenameordirectory .\n</code></pre></p>"},{"location":"storage_and_transfers/transfers/scp/#wildcards","title":"Wildcards","text":"<p>Wildcards can be used for multiple file transfers (e.g. all files with .dat extension). Note the backslash <code>\\</code> preceding <code>*</code></p> <pre><code>scp NetId@filexfer.hpc.arizona.edu: subdirectory /\\*. dat\n</code></pre>"},{"location":"storage_and_transfers/transfers/scp/#windows","title":"Windows","text":"<p>Windows users can use software like WinSCP to make SCP transfers. To use WinSCP, first download/install the software from: https://winscp.net/eng/download.php</p> <p>To connect, enter <code>filexfer.hpc.arizona.edu</code> in the Host Name field, enter your NetID under User name, and enter your password. Accept by clicking Login. You'll be prompted to Duo Authenticate:</p> <p></p>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/","title":"SFTP/FTP/LFTP","text":""},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/#sftp","title":"SFTP","text":"<p>The intent is that <code>filexfer.hpc.arizona.edu</code> is to be used for most file transfers. SFTP encrypts data before it is sent across the network. Additional capabilities include resuming interrupted transfers, directory listings, and remote file removal. To transfer files with SFTP, you will need to open an SSH v2 compliant terminal and navigate to a desired working directory on your local machine. To access HPC:</p> <pre><code>sftp your_netid@filexfer.hpc.arizona.edu\n</code></pre> <p>You will then be able to move files between your machine and HPC using <code>get</code> and <code>put</code> commands. For example:</p> <pre><code>sftp&gt; get /path/to/remote/file /path/to/local/directory ### Retrieves file from HPC. Omitting paths will default to working directories.\nsftp&gt; put /path/to/local/file /path/to/remote/directory ### Uploads a file from your local computer to HPC. Omitting paths will default to working directories.\nsftp&gt; help ### prints detailed sftp usage\n</code></pre>"},{"location":"storage_and_transfers/transfers/sftp_ftp_lftp/#ftplftp","title":"FTP/LFTP","text":"<p>Warning</p> <p>Due to security risks, it is not possible to FTP to the file transfer node from a remote machine, however, you may FTP from the file transfer node to a remote machine.</p> <p>Tip</p> <p>For more information on LFTP, see their official documentation.</p> <p>HPC uses the FTP client LFTP to transfer files between the file transfer node and remote machines. This can be done using get and put commands. To use lftp, you must first connect to our file transfer node using an SSH v2 compliant terminal:</p> <pre><code>ssh your_netid@filexfer.hpc.arizona.edu\n</code></pre> <p>Once connected, you may connect to the external host using the command <code>lftp</code>. For example:</p> <pre><code>lftp ftp.hostname.gov\n</code></pre> <p>You will then be able to move files between HPC and the remote host using <code>get</code> and <code>put</code> commands. For example:</p> <pre><code>&gt; get /path/to/remote/file /path/to/local/directory ### retrieves file from remote host\n&gt; put /path/to/local/file /path/to/remote/directory ### Uploads file from HPC to remote host\n</code></pre>"},{"location":"support_and_training/","title":"Index","text":"<p>foo</p>"},{"location":"support_and_training/consulting_services/","title":"Consulting Services","text":""},{"location":"support_and_training/consulting_services/#hpc-consulting-services","title":"HPC Consulting Services","text":"<p> Consulting is available free of cost to everyone and we welcome you to reach out! Our services include, but are not limited to:</p> <ul> <li>Help for new users getting started with our resources. We know using HPC systems for the first time can be intimidating so scheduling an in-person meeting where you can ask loads of questions can help a lot.</li> <li>General issues that may occur, e.g., why is my job spending so long in the queue, where can I find information on topic \\&lt;A&gt;, why is this strange and unexpected thing happening, etc.</li> <li>Advice on debugging software, package installations, and job failures.</li> <li>Advice on code optimization and utilizing our resources more effectively/efficiently.</li> </ul> When can I ask for help? <p>Any time! But first, we encourage you to:</p> <ul> <li>Double-check our FAQs - We keep track of commonly asked questions and document their solutions. You might find what you're looking for there.</li> <li>Look through our online documentation - There's lots of information to help get you started that may answer your question or help give you a better idea of what to ask.</li> </ul> How can I effectively write a support request? <p>Glad you asked! Helping us help you goes a long way and can give you better answers faster. Some general rules of thumb: * Detail detail detailA full error log may seem like a lot to send, but the more information we have, the more likely we are to be able to diagnose and/or replicate your issue. * Use reply-all to email chains We will cc hpc-consult in our responses so that our (small) consulting team is able to view the issue and contribute. * Provide contextThere's a common support issue called The XY Problem. Say you have a problem and try to solve it yourself but the attempted solution produces an additional problem. Submitting a ticket requesting help with the attempted solution without information about the original issue can lead to more confusion.  * Submit a ticket for your questionsIf you reply to general system announcements or send emails to an HPC staff member's private inbox without cc'ing hpc-consult, your ticket may get lost and go unanswered. Submitting a ticket will ensure we have a record of your question and will get to it as promptly as we are able.</p> What are our support policies? <p>Mostly it is common sense rather than strict rules. The primary consideration is that our consultants work regular hours with some flexibility built-in. So don't expect detailed responses at night or on the weekends. You might get a quick response but don't count on it.</p> <p>Our consultants typically don't know how to run your applications unless they have broad usage like Python or R. So once we determine it is likely an issue with the code we will refer you to the provider. We don't troubleshoot bugs or run profilers, but we support tools like Valgrind for you to use.</p> <p>We don't mind you asking lots of questions. We encourage you to ask for a consulting session via zoom or Office Hours (see below). We are not in the office so you can't drop by (although we kind of miss that personal engagement).</p> <p>The bottom line is that the supercomputers are only really productive tools when you have the support to gain the most out of them to improve both your results and the time to get results.</p> How can I reach HPC consulting? <p>We use ServiceNow and can be reached with a support ticket. Many in our research community are accustomed to using the list service for hpc-consult. That continues to work but is not as efficient. And we really want to discourage sending emails directly to your favorite consultant.  </p>"},{"location":"support_and_training/consulting_services/#office-hours","title":"Office Hours","text":"<p>We host virtual, drop-in office hours every Wednesday from 2pm to 4pm. Drop by to visit with our consultants to ask any questions you have about using HPC resources. It might be a bunch of getting started questions, or you might want to share your screen to walk us through a particular problem you're hung up on. We have private spaces for one-on-one consults. You can join us in Gather Town at this link. </p> <p>If you have never used Gather Town before and would like additional information, check out this page.</p>"},{"location":"support_and_training/external_resources/","title":"External Resources","text":"<p>Explore a number of resources beyond our center's offerings, from local community events to comprehensive software support and opportunities for accessing national supercomputing clusters. We've curated a collection of external organizations aimed at enhancing your HPC journey, providing diverse avenues for support and collaboration.</p>"},{"location":"support_and_training/external_resources/#access","title":"ACCESS","text":"<p>XSEDE supports 13 supercomputers and high-end visualization and data analysis resources across the country.  These are made available for use by researchers including at the University of Arizona. This program ends at the end of August 2022 to be replaced with a new program called ACCESS. This is also funded by the National Science Foundation.</p> <p>The new program is going through many changes so we will just include their website while has the most current information: https://access-ci.org/.</p> <p>Researchers who have current project allocations that were awarded via XSEDE \u2013including projects awarded at the August 22 XRAC meeting \u2013 should notice no changes to their resource access after August 31. For projects that expire on December 31, the ACCESS team recommends planning to submit a proposal during the usual September 15 to October 15 submission window under the existing XRAC guidelines.</p> <p>Researchers whose needs are at the smaller end of the scale should review the new ACCESS Allocations Marketplace information that is previewed on the XSEDE site pending the launch of the ACCESS site.</p>"},{"location":"support_and_training/external_resources/#ansys","title":"Ansys","text":"<p>For help with local installations, contact the College of Engineering IT services: support@engr.arizona.edu</p> <p>Ansys-specific support (debugging, questions about usage, etc) is available through PADT: support@padtinc.com</p> <p>To report license connection issues, contact: HPC consulting</p>"},{"location":"support_and_training/external_resources/#code-commons","title":"Code Commons","text":"<p>Code Commons provides a physical space for community and collaboration. Join to share experience, learn, mentor, discover opportunities, and work on your programming projects in the presence of others doing the same. Held every Wednesday from 2:00-6:00pm at the UArizona Library in the CATalyst Data Studios. For more information, see: https://codecommons.net/</p>"},{"location":"support_and_training/external_resources/#cyverse","title":"CyVerse","text":"<p>CyVerse provides life scientists with powerful computational infrastructure to handle huge datasets and complex analyses, thus enabling data-driven discovery. Their extensible platforms provide data storage, bioinformatics tools, image analyses, cloud services, APIs, and more: http://www.cyverse.org/about</p> <p>CyVerse is funded by the National Science Foundation\u2019s Directorate for Biological Sciences. They are a dynamic virtual organization led by the University of Arizona to fulfill a broad mission that spans our partner institutions: Texas Advanced Computing Center, Cold Spring Harbor Laboratory, and the University of North Carolina at Wilmington.</p> <p>Check out the comprehensive suite of tools supporting research: http://www.cyverse.org/products</p>"},{"location":"support_and_training/external_resources/#open-science-framework","title":"Open Science Framework","text":"<p>The OSF is a free, open source service maintained by the Center for Open Science. Here are a few things you can do with the OSF:</p> <ul> <li> <p>Store your filesArchive your materials, data, manuscripts, or anything else associated with your research during the research process or after it is complete.</p> </li> <li> <p>Affiliate your projects with your institutionAssociate your projects with the University of Arizona which is a member. They will be added to UA's central commons, improving discoverability of your work and fostering collaboration.</p> </li> <li> <p>Share your workKeep your research materials and data private, make it accessible to specific others with view-only links, or make it publicly accessible. You have full control of what parts of your research are public and what remains private.</p> </li> <li> <p>Register your researchCreate a permanent, time-stamped version of your projects and files. Do this to preregister your design and analysis plan to conduct a confirmatory study, or archive your materials, data, and analysis scripts when publishing a report.</p> </li> <li> <p>Make your work citableEvery project and file on the OSF has a permanent unique identifier, and every registration can be assigned a DOI. Citations for public projects are generated automatically so that visitors can give you credit for your research.</p> </li> <li> <p>Measure your impactYou can monitor traffic to your public projects and downloads of your public files.</p> </li> <li> <p>Connect services that you useGitHub, Dropbox, Google Drive, Box, Dataverse, figshare, Amazon S3, ownCloud, Bitbucket, GitLab, OneDrive, Mendeley, Zotero. Do you use any of these? Link the services that you use to your OSF projects so that all parts of your project are in one place.</p> </li> <li> <p>CollaborateAdd your collaborators to have a shared environment for maintaining your research materials and data and never lose files again.</p> </li> </ul> <p>Learn more about the OSF on their Guides page, or email contact@osf.io with questions for support.</p>"},{"location":"support_and_training/external_resources/#research-bazaar","title":"Research Bazaar","text":"<p>Want to get involved with the Tucson coding community? ResBaz AZ offers weekly events that brings together scientists, software engineers, and enthusiasts of all skill levels. Additionally, an annual Research Bazaar is held each spring hosting research computing workshops and career panels: https://researchbazaar.arizona.edu/</p>"},{"location":"support_and_training/external_resources/#uarizona-data-science","title":"UArizona Data Science","text":"<p>Have some code-specific, data science, or related questions? Consider joining the UArizona Data Science Slack channel: https://jcoliver.github.io/uadatascience-slack/user-guide.html</p>"},{"location":"support_and_training/faqs/account_access/","title":"Account Access","text":"How do I create an account? <p>A step by step guide is available in our Account Creation page. </p> Why can't I log in? <p>There are many reasons you may be having issues logging in. A possible list of reasons may include:     <ul> <li>You haven't created an account yet or you have not yet been sponsored.</li> <li>You aren't using two-factor authentication (NetID+).</li> <li>You need to wait 15 minutes. If you just created your account, it takes time before you can log in.</li> <li>You're trying to connect using ssh <code>netid@login.hpc.arizona.edu</code>. This will not work. Instead, use: <code>ssh netid@hpc.arizona.edu</code>.</li> <li>You're using <code>netid@hpc.arizona.edu</code> or <code>netid@email.arizona.edu</code> as your username in PuTTY. Instead, use only your NetID.</li> <li>You've entered your password incorrectly too many times. After multiple failed password entries, the system will place a 60 minute ban on your account for security reasons. Your account will automatically unlock after 60 minutes. Attempting to log in before your account unlocks will reset the timer. </li> </ul></p> Why can't I enter my password in my terminal? <p>Linux systems do not display character strokes while entering your password which can make it look like the ssh client is frozen. Even though it doesn't appear that anything is happening, the system is still logging your input. To proceed, type your password at the prompt and press enter.</p> Why am I getting \"You do not appear to have registered for an HPC account\"? <p>If you have just registered for an HPC account, you need to wait a little while for the request to propagate through the University systems (this can take up to an hour).  Patience \ud83d\ude42 </p> Why am I getting \"permission denied\" when I try to log in? <p>You need an HPC account - see our Account Creation page for details.  Once you've done that, you'll need to wait a little while to log in. If your PI hasn't already added you to their group, you'll need to wait for that as well.</p> Why am I unable to log in with the error \"Your account is disabled and cannot access this application. Please contact your administrator.\"? <p>This error shows up when your NetID has been locked, usually due to multiple failed login attempts when trying to access university services. Contact 24/7 to unlock your account: https://it.arizona.edu/get-support</p> Why am I getting the message \"incorrect password\" when I try to log in? <p> <ul> <li>Ensure you are using the correct password. Sometimes typing your password into a plain text file and copying/pasting it into the terminal can help.</li> <li>You need to wait about 15 minutes after your account is approved for the account to be available</li> <li>You must enroll in NetID. Depending on the application you use to log in, you may not get the typical NetID+/DUO menu of options, or an error message indicating this is your problem</li> </ul> </p> I've forgotten my password, how can I reset it? <p>HPC uses the same NetID login credentials as all UA services. If you need to reset your NetID password you can do so using the NetID portal.</p> How do I add members to my HPC research group? <p>Faculty members who manage their own HPC groups can follow the instructions in our Research and Class Groups page.</p> I'm leaving the university/not affiliated with the university, can I maintain/receive access to HPC? <p>Yes, if you are a former university affiliate or campus collaborator participating in research, you may register as a [Designated Campus Colleague (DCC)](https://it.arizona.edu/service/designated-campus-colleague-accounts). Once your DCC status has been approved, you will receive a NetID+ which you may use to create an HPC Account. If you already have an HPC Account, no further action is required.</p>"},{"location":"support_and_training/faqs/general_computing/","title":"General Computing","text":"Why aren't common commands working? <p>There may be a few reasons for this. First, make sure your shell is set to Bash. If your shell is not set to Bash, contact our consultants so that they can reset it for you.      If your shell is set to bash, double-check that you haven't changed, overwritten, or aliased anything important either your <code>~/.bashrc</code> or <code>~/.bash_profile</code>. E.g., unsetting your <code>PATH</code>, aliasing <code>.</code>, and so on will corrupt your environment and prevent you from interacting with HPC normally.    </p> Why is my terminal glitching (e.g. Ctrl+a puts me in the middle of my command prompt)? <p>When you log into HPC, the variable <code>$COMMAND_PROMPT</code> is set to display your current cluster (e.g. <code>(puma)</code>). Sometimes this can cause formatting problems. If you'd prefer to modify your <code>$PS1</code> (command prompt variable) instead, you can add the following to your <code>~/.bashrc</code>:    <pre><code>if [ -n \"${PROMPT_COMMAND}\" -a -r /usr/local/bin/slurm-selector.sh ]; then\n  SavePS1=${PS1}\n  Cur_Cluster=$(eval ${PROMPT_COMMAND} 2&gt;/dev/null)\n  PS1=\"${Cur_Cluster}${SavePS1}\"\n  unset PROMPT_COMMAND\n  for c in puma ocelote elgato; do\n    alias ${c}=\"PS1=\\\"(${c}) ${SavePS1}\\\"; . /usr/local/bin/slurm-selector.sh ${c}; unset PROMPT_COMMAND\"\n  done\n  unset Cur_Cluster SavePS1\nfi\n  </code></pre> </p> What are dot files and what is a <code>~/.bashrc</code>? <p>Files that start with a dot, i.e. a <code>.</code>, are hidden when executing <code>ls</code> without additional options. If you use <code>ls -la</code>, that will show you all the files that exist, both standard and hidden. Dot files are generally important configuration files so it's important to be careful with their contents and deleting them.       Your bashrc is a specific dot file that lives in your home directory (<code>~</code> is just shorthand for your home) and defines your environment every time you log in. Any commands you add to that file will be run whenever you access the system. This means, if you have common aliases you like to use, paths you like exported in your environment, etc., these can be added to your bashrc to avoid the headache of having to define them in every session.    A word of caution. Be careful with the contents of this file. Some things to avoid:        \u20021. Don't alias important Linux commands. For example, the character <code>.</code> is a shortcut for source. If you do something like add alias <code>.=\"echo foo\"</code> to your bashrc, you will lose basic functionality in the terminal, e.g., access to modules, virtual environments, etc.           \u20022. Do not recursively source your configuration files. For example, if you add <code>source ~/.bashrc</code> or <code>source ~/.bash_profile</code> to your bashrc, then you will enter an infinite sourcing loop. This means when you try to log in, your terminal will freeze for a moment before your access is denied.           \u20023. Be careful with echoes. If you use CLI tools for data transfer, e.g. <code>scp</code> or <code>sftp</code>, they may require a \"silent\" terminal. If you're trying to initiate transfers and are getting the error \"Received message too long\", check your bashrc to make sure you aren't printing anything to the terminal.    </p>"},{"location":"support_and_training/faqs/jobs_and_scheduling/","title":"Jobs and Scheduling","text":"Why isn't my job running? <p> <ul> <li>There are a few reasons your job may not be running, check below for some ideas on diagnosing the issue:       Run <code>squeue --job  and see if there is anything listed under <code>(REASON)</code>. This may give an idea why your job is stuck in queue. We have a table in our Slurm documentation that describes what each Reason code means.  <li>Due to the number of HPC users, it may not always be possible to run a submitted job immediately. If there are insufficient resources available, your job will be queued and it may take up to a few hours for it to begin executing.</li> <li>Your group may have run out of standard hours. You can check your allocation using the command <code>va</code>.</li> <li>Your group/job has reached a resource usage limit (e.g., number of GPUs that may be used concurrently by a group, or a job has requested more than the 10 day max walltime). Try running <code>job-limits  to see what limits you're subject to and if there are any problem jobs listed. <li>You may be requesting a rare resource (e.g., 4 GPUs on a single node on Puma or a high memory node).       <ul> <li>If you are requesting a single GPU on Puma and are frustrated with the wait times, you might consider checking if Ocelote will work for your analyses. There are more GPU nodes available on that cluster with shorter wait times.</li> <li>If you are trying to run a job on a standard node and have been waiting for a very long time, try checking its status using <code>job-history . If you see <code>Allocated RAM/CPU</code> above 5gb on Puma or above 6gb on Ocelote, then you are queued for the high memory node which can have very long wait times. To queue for a standard node, cancel your job and check that your script has the correct ratios. My job has a Reason code when I check it with squeue. What does this mean? <p>If your job is in queue, sometimes Slurm will give you information on why it's not running. This may be for a number of reasons, for example there may be an upcoming maintenance cycle, your group's allocation may be exhausted, you may have requested resources that surpass system limits, or the node type you've requested may be very busy running jobs. We have a list of reason codes in our Running Jobs With Slurm page that will give more comprehensive information on what these messages mean. If you don't see the reason code listed, contact our consultants.   </p> Why do my jobs keep getting interrupted? <p>If your jobs keep stopping and restarting, it's likely because you are using Windfall. Windfall is considered lower priority and is subject to preemption by higher priority jobs. Before submitting a job to Windfall, consider using your group's allotted monthly hours first. Jobs using Standard hours will queue for a shorter period of time and will not be interrupted. You can check your group's remaining hours using the command <code>va</code>. To see more information on your allotted hours and the different job queues, see our page on compute allocations.    </p> Can I run programs on the login nodes? <p>No. Software to run applications is not available on the login nodes. To run/test your code interactively, start an interactive session on one of the system's compute nodes. Processes running on the head node are subject to being terminated if we think they are affecting other users. Think of these as 'submit' nodes where you prepare and submit job scripts.    </p> Can I get root access to my compute nodes? <p> Unfortunately, that is not possible. The compute nodes get their image from the head node and have to remain the same. If you need to install software that needs root access, for example, you can install the software locally in your account. See this example.    </p> Can I ssh to compute nodes? <p>Slurm will let you ssh to nodes that are assigned to your job, but not to others.    </p> Why am I getting out of memory errors? <p>       There are a few reasons you might get out of memory errors:       <ul> <li>You're using <code>-c  to request CPUs. Based on the way our scheduler is set up, this will reduce the memory allocation for your job to ~4MB. To solve this, change your CPU request by either setting <code>--ntasks= or <code>--ntasks=1 --cpus-per-task=. <li>You may not have specified the number of nodes required for your job. For non-MPI workflows, if SLURM scatters your CPUs across multiple nodes, you will only have access to the resources on the executing node. Explicitly setting --nodes in your script should help, e.g.:            <pre><code>\n#SBATCH --nodes=1\n          </code></pre> </li> <li>You may not have allocated enough memory to your job. Try running <code>seff  to see your memory usage. You may consider using memory profiling techniques, allocating more CPUs, or using the high memory node. Why shouldn't I use Windfall with OnDemand? <p>       Windfall jobs can be preempted by a higher priority queue. Each session creates an interactive job on a node and it is unsatisfactory to be dumped in the middle of that session. A desktop session would have the same unpleasant result.  Windfall can be used if you do not have enough standard time left. Consider though that a one hour session using one compute core only takes up 1 cpu hour out of your group's 100,000 hours.    </p> My interactive terminal session has been disconnected, can I return to it? <p>       No, unfortunately when an interactive job ends it is no longer accessible. This applies to both OOD sessions and those accessed via the command line. We recommend using the standard partition rather than windfall when running interactive jobs to prevent preemption.    </p> Are any modules loaded by default? <p>       Yes, when you start an interactive session via the terminal or submit a batch job, the modules gnu8, openmpi3, and cmake are loaded by default. If you need to use intel, you'll want to unload openmpi3 and gnu8 first.       However, if you start a terminal in an interactive desktop session through Open OnDemand, no modules are loaded by default in this environment. To start, at the minimum you'll want to run the command:       <pre><code>\nmodule load ohpc\n      </code></pre> </p>"},{"location":"support_and_training/faqs/open_on_demand_issues/","title":"Open OnDemand Issues","text":"Why am I getting a message saying I'm not sponsored when trying to log in? <p>       If you are trying to log in to Open Ondemand and are seeing the following:        <ul> <li>You have not yet been sponsored by a faculty member. See our Account Creation page for instructions on getting registered for HPC.</li> <li>If you are already registered for HPC, this may be a browser issue. Try logging in again in an incognito session or different browser to test. If this succeeds, clearing your browser's cookies should help.</li> </ul> </p> Why am I getting a \"Bad Request\" message when trying to connect? <p>       If you are trying to log in to Open Ondemand and are seeing the following:               this may be a browser issue. Try logging in again in an incognito session or different browser to test. If this succeeds, clearing your browser's cache should help.   </p> Why am I getting an error saying \"We're sorry, but something went wrong\" when trying to log in? <p>       If you are trying to log in to Open Ondemand and are seeing the following:               check your storage usage in your home directory. You can do this by logging into HPC in a terminal session and using the command uquota. If your storage usage is &gt;50GB, OnDemand cannot create the temporary files necessary to give access to the website. Try clearing out some space in your home and then logging back into OnDemand.   </p> Why are my Desktop sessions failing with 'Could not connect to session bus: failed to connect to socket /tmp/dbus-'? <p>       If you're seeing:               when trying to connect to an interactive desktop session, the likely culprit is Anaconda. For a permanent solution, you can run the following command from an interactive terminal session:       <pre><code>\nconda config --set auto_activate_base false\n      </code></pre>       This will prevent conda from auto-activating when you first log in and allow you to have more control over your environment. When you'd like to activate anaconda, run conda activate. See this example for information running anaconda workflows in batch with auto-activation disabled.   </p>"},{"location":"support_and_training/faqs/secure_services/","title":"Secure Services","text":"How do I get access to Soteria? <p>     You will first need to request access. Follow the instructions in our Secure HPC page to see all the steps that are required.   </p> Why am I getting \"Operation timed out\" when I try to ssh to Soteria? <p>     It's possible you're not connected to the VPN. You will need to be connected to access any Soteria resources.   </p> How do I transfer my data to Soteria? <p>     The easiest way to transfer your data to Soteria is using Globus. We have a high assurance endpoint set up accessible under the endpoint UA HPC HIPAA Filesystems   </p> Does my personal computer's Globus endpoint need to be set to high assurance? <p>   It depends.    You do not need your personal computer's Globus endpoint set to high assurance for the purposes of transferring data to Soteria using our UA HPC HIPAA Filesystems endpoint. HIPAA requires data transfer auditing. We log transfers on our DTN that use the HIPAA endpoint, regardless of the client's managed status.      If you are working with another institution/endpoint that only allows connections from high assurance endpoints, then you must enable High Assurance during your Globus Personal Connect configuration process. This requires that you are added to our Globus subscription which can be done by opening a support ticket to request help from our consultants. Your endpoint must be set to public to allow us to add you. Once you have been added, you may set your endpoint to private.    </p>"},{"location":"support_and_training/faqs/software_and_modules/","title":"Software and Modules","text":"Are any software modules loaded by default? <p>       Yes, when you start an interactive terminal session or submit a batch script, the modules ohpc, gnu8, openmpi3, and cmake are automatically loaded. If your code uses Intel compilers, you will want to manually unload gnu8 and openmpi3 to prevent conflicts.              The exception: If you are working in a terminal in an Open OnDemand interactive desktop session, nothing is loaded by default and you will need to manually load any necessary modules. To start, always use <code>module load ohpc</code> </p> What executables are available when I load a module? <p>       Load the module, find the path to the executable by checking the $PATH variable, then list the contents.  For example:       <pre><code>\nmodule load lammps\necho $PATH\nls /opt/ohpc/pub/apps/lammps/3Mar20/bin\nlmp_mpi\n      </code></pre> </p> Why am I getting \"command: module not found\"?  <p>       There are a few different possibilities:       <ul> <li>You are not in an interactive session. Modules are not available on the login nodes. You may request an interactive session by using the command <code>interactive</code>. </li> <li>Your shell is not set to bash. If this is the case, contact our consultants so that they can reset it for you.</li> <li>You have modified or deleted your <code>~/.bashrc</code>. If this is the case, open (if the file exists) or create and open (if the file is missing) the file .bashrc in your home directory and add the lines:</li> <pre><code>\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n          </code></pre> </ul> </p> How do I access the module for Gaussian or Gaussview? <p>     You need to belong to a special group called g03.  You can request to be added by submitting a help ticket. This is a constraint in Gaussian that other modules do not have.   </p> How can I maximize my software performance on Puma? <p>       If you are able to compile your software you can take advantage of most of the AMD Zen architecture.   </p> Compiler Arch-Specific Arch-Favorable GCC 9 <code>-march=znver2</code> <code>-mtune=znver2</code> LLVM 9 <code>-march=znver2</code> <code>-mtune=znver2</code> Is the Intel compiler faster than GCC on Puma? <p>     Intel compilers are optimized for Intel processors. There is some debate around the concept of unfair CPU dispatching in Intel compilers. By default, software on the HPC clusters is built with GCC (on Puma it is GCC 8.3).  This is in keeping with our preference for community software.   </p> I've been using an older version of Singularity, why isn't available anymore?  <p>     Prior versions of Singularity are routinely removed since only the latest one is considered secure.  Notify the consultants if you need help with transition to the current version. Singularity is installed on the operating systems of all compute nodes so does not need to be loaded with a module.    </p> Can I use Windows applications on HPC? <p>     Unfortunately, Windows applications can't be run on HPC. However, AWS has been used successfully for Windows software with GPU needs. It\u2019s easy to set up, cost effective, and very scalable. Amazon also has a cloud credit for research program available.          You may also consider trying Jetstream2, a national resource where you can create and use Windows virtual machines. More information can be found here: https://jetstream-cloud.org/ </p> How do I install this R package/Why can't I install this R package? <p>       R installations can sometimes be frustrating. We have instructions for how to set up a usable R environment, how to diagnose and troubleshoot problems, and steps to help with known troublesome packages documented in in our Using and Customizing R Packages section.    </p> How do I install Python packages? <p>      You can install python packages locally using either a virtual environment or a local conda environment.    </p> How do I access custom Python packages from an OOD Jupyter session? <p>   Instructions on accessing custom packages are under Accessing Custom Packages from a Jupyter Session in our documentation on Using Python &amp; Python Packages.   </p> How do I take advantage of the Distributed capability of Ansys? <p>     Ansys has the Distributed capability built in to increase performance. Ansys uses the Intel compiler and so uses Intel MPI.  By default, we load OpenMPI, so you will need to do this:      <pre><code>\nmodule unload gnu8 openmpi3\nmodule load intel\nmodule load ansys\n    </code></pre> </p>"},{"location":"support_and_training/faqs/storage/aws/","title":"Tier 2 AWS Storage","text":"Who can submit a Tier 2 storage request? <p>     A PI must submit their group's storage request. The exception to this is if a group member has been designated xdisk/storage delegate. Delegates may submit a request on behalf of their PI by switching users in the user portal.   </p> How long is the turnaround time to between submitting the storage request and obtaining an S3 account? <p>      In most cases it will be in one business day.   </p> Can I move my data directly from Google Drive to the new S3 account? <p>      Yes. Using Globus you can move data from Google Drive to S3.   </p> Question goes here <p>     You should check the Amazon site. As of March 2022:     <ul> <li>Frequent Access Tier, First 50 TB / Month $0.023 per GB</li> <li>Frequent Access Tier, Next 450 TB / Month $0.022 per GB</li> <li>Frequent Access Tier, Over 500 TB / Month $0.021 per GB</li> </ul> </p> Can I move my data directly to Glacier if I know it is archival. That way I can skip the S3 expenses? <p>     Not in the first release, but potentially as a future offering.   </p> Is the monthly billing based on average usage? <p>      Yes. The capacity used is recorded daily and the billing is a monthly average.   </p> What is a KFS number? <p>      It is used for accounting purposes and used by your Department's finance specialist.   </p> Can I view my storage in each of S3, Glacier and Deep Glacier? <p>      Yes, you can use the CLI (command line interface) for information about your usage   </p> Can I limit the amount of data that goes into S3 so I can control the expense? <p>      No, but you can track the usage and remove any data that should not be there.   </p> What is the difference between Glacier and Deep Glacier? <p>      Glacier is effectively large, slow disks and Deep Glacier is tape storage.   </p> I use workflows to move data using google drive, and to share with others. Will AWS support something similar? <p>      Amazon S3 will likely support what you do. Perhaps our consultants can help to rework your workflows.   </p> Are there charges for data movement? <p>      You will not be charged for data ingress, egress or other operations.   </p> Can I have multiple S3 buckets associated with my account? <p>     Yes   </p> Are there any limits on uploads, e.g. may transfer size/day? <p>     The max file size is 5TB based on Amazon's official documentation.   </p> What why am I receiving an email: \"ALARM: \"ua-rt-t2-netid capacity growth alarm\" in US West (Oregon)\"? <p>     This alert is sent to notify you whenever your storage usage grows by 10% relative to the previous week. This ensures that you're aware of any unintended spikes and the potential resulting costs.    </p>"},{"location":"support_and_training/faqs/storage/hpc/","title":"HPC Storage","text":"General Storage Do you allow users to NFS mount their own storage onto the compute nodes? <p>        No. We NFS mount storage across all compute nodes so that data is available independent of which compute nodes are used. See our page on transferring data for more information.   </p> I can't transfer my data to HPC with an active account. What's wrong? <p>       After creating your HPC Account, your home directory will not be created until you log in for the first time. Without your home directory, you will not be able to transfer your data to HPC. If you are struggling and receiving errors, sign into your account either using the CLI through the bastion or logging into Open OnDemand and then try again.   </p> I accidentally deleted files, can I get them back? <p>       Unfortunately, no. Backups are not made and anything deleted is permanently erased. It is impossible for us to recover it. To ensure your data are safe, we recommend:       <ul> <li>Make frequent backups, ideally in three places and two formats. Helpful information on making backups can be found on our page Transferring Data.</li> <li>Use <code>rm</code> and <code>rm -r</code> with caution as these commands cannot be undone! Consider using <code>rm -i</code> when removing files/directories. The <code>-i</code> flag will prompt you to manually confirm file removals to make really sure they can be deleted.</li> <li>You can open a support ticket to request assistance.  Files that are deleted may not have been removed from the storage array immediately (though this is not guaranteed), don't wait more than a few days.</li> </ul> </p> My home directory is full, what's using all the space? <p>       If your home directory is full and you can't find what is taking up all the space, it's possible the culprit is a hidden file or directory. Hidden objects are used for storing libraries, cached singularity/apptainer objects, saved R session, anaconda environments, configuration files, and more. It's important to be careful with hidden, or \"dot\", files since they often control your environment and modifying them can lead to unintended consequences.       To view the sizes of all the objects (including hidden) in your home, one quick command is <code>du -hs $(ls -A ~)</code>, for example:       <pre><code>\n[netid@junonia ~]$ du -hs $(ls -A ~)\n32K     Archives\n192M    bin\n4.7G    Software\n46M     .anaconda\n1.9M    .ansys\n4.0K    .apptainer\n16K     .bash_history\n4.0K    .bash_logout\n4.0K    .bash_profile\n12K     .bashrc\n20M     ondemand\n      </code></pre> </p> I'd like to share data I have stored on HPC with an external collaborator, is this possible? <p>       Unfortunately, without active university credentials it is not possible to access HPC compute or storage resources. External collaborates who need ongoing access may apply for Designated Campus Colleague, or DCC, status. This is a process done through HR and will give the applicant active university credentials allowing them to receive HPC sponsorship.              Otherwise, data will need to be moved off HPC and made available on a mutually-accessible platform. This may include (but is not limited to): Google Drive, AWS S3, Box, and CyVerse's Data Store.   </p> xdisk Can someone other than a PI manage a group's xdisk? <p>     Yes, a PI can add a trusted group member as a delegate by following the instructions in our Research and Class Groups page. Once a group member is added as a delegate, they can manage the group's xdisk allocation on behalf of their PI in the user portal.   </p> Who owns our group's /xdisk? <p>       A group's PI owns the xdisk allocation. By default, your PI has exclusive read/write/execute privileges for the root folder <code>/xdisk/pi_netid</code>.   </p> Can a PI make their /xdisk writeable for their whole group?  <p>       By default, members of a research group only have write access to their subdirectories under <code>/xdisk/pi_netid</code>. If they so choose, a PI may allow their group members to write directly to that location by running the following command:       <pre><code>\nchmod g+w /xdisk/pi_netid\n      </code></pre> </p> Where can group members store their files? <p>       When an xdisk allocation is created, a subdirectory is automatically generated for and owned by each individual group member. If the directory <code>/xdisk/pi_netid</code> does not have group privileges, group members may not access the root directory, but may access their individual spaces by:       <pre><code>\ncd /xdisk/pi_netid/netid\n      </code></pre>       If a user joins the group after the xdisk was created and <code>/xdisk/pi_netid</code> is not writeable for group members, contact our consultants and they can create one.   </p> A group member's directory isn't in our /xdisk, how can we add it? <p>       Typically when an xdisk allocation is created, it will automatically generate a directory for each group member. In the unlikely event that it doesn't or, more commonly, a group member is added after the allocation has been created, contact our consultants and they can create one.    </p> Do we need to request an individual allocation within the /xdisk for each user in our group? <p>       No, the full xdisk allocation is available for every member of the group. It's up to group members to communicate with one another on how they want to utilize the space.   </p> Why am I getting xdisk emails?  <p>        xdisk is a temporary storage space available to your research group. When it's close to its expiration date, notifications will be sent to all members of your group..   </p> Why am I getting \"/xdisk allocations can only be authorized by principal investigators\"?  <p>       xdisks are managed by your group's PI by default. This means if you want to request an xdisk or modify an existing allocation (e.g., extending the time limit or increasing the storage quota), you will need to consult your PI. Your PI may either perform these actions directly or, if they want to delegate xdisk management to a group member, they may do so by following the instructions under Delegating Group Management Rights.   </p> How can we modify our xdisk allocation? <p>       To modify your allocation's time limit or storage quota, your PI can either do so through the Web Portal under the Storage tab, or via the command line. If your PI would like to delegate management rights to a group member, they may follow the instructions under Delegating Group Management Rights. Once a group member has received management rights, they may manage the allocation through our web portal.   </p> Why am I getting \"xdisk: command not found\"? <p>       If you're getting errors using xdisk commands in a terminal session, check that you are on a login node. If you are on the bastion host (hostname: gatekeeper), are in an interactive session, or are on the filexfer node, you won't be able to check or modify your xdisk. When you are on a login node, your terminal prompt should show the hostname junonia or wentletrap. You can also check your hostname using the command <code>hostname</code> </p> Why am I getting errors when trying to extend my allocation? <p>       If you're trying to extend your group's allocation but are seeing something like:       <pre><code>\n(puma) [netid@junonia ~]$ xdisk -c expire -d 1\ninvalid request_days: 1\n      </code></pre>       for every value you enter, your xdisk has likely reached its maximum time limit. To check, have a delegate or PI go to portal.hpc.arizona.edu, click Manage XDISK, and look at the box next to Duration. If you see 300, your allocation cannot be extended further.      If your allocation is at its limit, you will need to back up your data to external storage (e.g., a local machine, lab server, or cloud service). Once your xdisk has been removed (either by expiring or through manual deletion), you can immediately create a new allocation and restore your data. Detailed xdisk information can be found on our HPC High Performance Storage page. You may also want to look at our page on Transferring Data.   </p> Can we keep our xdisk allocation for more than 300 days? <p>       No, once an xdisk has reached its time limit it will expire. It's a good idea to start preparing for this early by making frequent backups and paying attention to xdisk expiration emails.    </p> What happens when our xdisk allocation expires? <p>       Once an xdisk expires, all the associated data are deleted. Deleted data are non-retrievable since HPC is not backed up. It's advised to keep frequent backups of your data on different platforms, for example a local hard drive or a cloud-based service, or (even better) both! Check our Storage documentation for more information on alternative storage offerings.   </p> What's the best way to backup/transfer our data before our xdisk expires? <p>       Before your group's xdisk expires, you'll want to make an external backup of anything you need to keep. External storage options include personal computers, lab servers, external hard drives, or cloud services such as AWS.       If you're moving large quantities of data, Globus is a great option. We have instructions in our Globus documentation for setting up and using this software.      We strongly recommend making archives (.tar, .zip, files etc.) of large directories prior to transferring them off the system. In general, transfer software struggles with moving many small files and performs much more efficiently moving fewer large files. You will get the better transfer speeds (sometimes by orders of magnitude) if you compress your files prior to transferring them. This can be done on our filexfer node which is designed for large file management operations (hostname: filexfer.hpc.arizona.edu).    </p> Once our xdisk expires, can we request a new one? <p>       Yes, a new xdisk may be requested immediately after the old partition expires. Data, however, may not be transferred directly from the old partition to the new one.    </p> Can a PI have more than one xdisk active at a time? <p>       No, only one xdisk may be active per PI at a given time.    </p>"},{"location":"support_and_training/faqs/storage/rdas/","title":"R-DAS Storage","text":"If my VPN connection is dropped, will my connection survive?  <p>       Yes, if you reconnect to the VPN, your connection will still be available.    </p> Why am I getting \"There was a problem connecting to the server\"? <p>     This error is seen if you are not connected to the University VPN.   </p> Can I access R-DAS for HPC usage? <p>       R-DAS is not mounted on the HPC compute nodes or login nodes, and is not meant for running computations. But you can follow the steps in our R-DAS documentation to share data between your R-DAS allocation and your HPC storage (<code>/home</code>, <code>/groups</code>, <code>/xdisk</code>):    </p>"},{"location":"support_and_training/faqs/storage/rental/","title":"Rental Storage","text":"How do I request rental storage? <p>        Rental storage can be requested by PIs through the user portal. A guide with screenshots can found in our rental storage documentation.    </p> Why can't I see my rental allocation when I log into HPC? <p>       Rental allocations are not mounted on the login or compute nodes. You can access your allocation by logging in to our data transfer nodes: <code>filexfer.hpc.arizona.edu</code> </p> Can I analyze data stored in /rental directly in my jobs? <p>       No, rental storage is not mounted on the HPC login or compute nodes which means jobs cannot access <code>/rental</code> directly. Data stored in <code>/rental</code> need to be moved to <code>/home</code>, <code>/groups</code>, or <code>/xdisk</code> to be available.    </p>"},{"location":"support_and_training/glossary/","title":"Glossary","text":"<p>Cluster</p> <p>A group of nodes connected to each other by a fast network.  The network in ElGato and Ocelote is 56Gb Infiniband.  What this gains for the user is the ability to connect nodes together to perform work beyond the capacity of a single node. Some jobs use hundreds of cores and terabytes of memory.</p> <p>CPU/processor/socket/core</p> <p>These terms are often used interchangeably, especially processor and CPU. The most straight forward way to think of the compute nodes is that they contain two physical sockets (or processor chips) which are located under their heatsinks. Each socket contains multiple cores.  Each core functions like a separate processor. Ocelote has 2 sockets with 14 cores in each so all you need to know is that there are 28 cores.  ElGato has 2 sockets with 6 cores in each, for a total of 12 cores. If your laptop is quad core, it has one socket with four cores, as a comparison.</p> <p>Data Mover Node</p> <p>A node connected to the public internet and dedicated to moving data to/from external computers. We have two DTN nodes known collectively as <code>filexfer.hpc.arizona.edu</code>.</p> <p>Distributed memory computing</p> <p>In software, a program or group of programs that run on multiple nodes or shared-memory instances and use programs such as MPI to communicate between the nodes. In hardware, a cluster that runs distributed-memory programs. Distributed-memory programs are limited in memory size only by job limits to support many users. </p> <p>Embarrassingly parallel</p> <p>A program where little effort is involved in separating the code into parallel tasks and so parallel scaling is very efficient.  Some astronomy codes fit this model.</p> <p>Encumbered hours</p> <p>This refers to any hours in your allocation that are reserved by running jobs. When you submit a job, any hours that it requires are moved to the \"encumbered\" category. As soon as your job ends, unused hours will be refunded and used hours will permanently be deducted from your monthly allotment. </p> <p></p> <p>GPU</p> <p>A graphical processing unit, a specialized type of CPU derived from a graphics card. Effectively has hundreds of small cores. For certain tasks (those that can be effectively parallelized), a GPU is much faster than a general-purpose CPU.</p> <p> </p> <p>Head node</p> <p>The head node is for managing the cluster and is not available to users.</p> <p></p> <p>HPC</p> <p>High performance computing. Implies a program too large for, or that takes too long on, a laptop or workstation. Also HTC (high throughput computing) similar but oriented to processing many small compute jobs.</p> <p></p> <p>Hyperthreading</p> <p>Intel processors (in this case \"cores\") have hyper-threading which can make one core look like two; but it does not add compute capacity in most HPC cases, so we turn it off.</p> <p></p> <p>Login node</p> <p>A cluster node accessible to users and dedicated to logins, editing, moving data, submitting jobs.  </p> <p></p> <p>MPI computing</p> <p>Message passing interface, software standard used for most programs that use distributed memory. MPI calls lower-level functions, either networking or shared memory. On a cluster that means it can run transparently either on one node or multiple nodes. MPI has multiple implementations (OpenMPI, MVAPICH, OpenMPI or Intel MPI) that must be used consistently to both compile and run an MPI program.</p> <p></p> <p>Network bandwidth</p> <p>The amount of data that can be moved over a network per second. For FDR Infiniband on Ocelote that is 56Gbps (Giga bits per second)</p> <p></p> <p>Network latency</p> <p>In HPC terms, it is usually the delay in the network for messages being passed from one node to another.  This is optimized by a hardware technology called RDMA (Remote Direct Memory Access)</p> <p></p> <p>Node (aka compute node)</p> <p>A single computer in a box, functionally similar to a desktop computer but typically more powerful and packaged for rackmount in a datacenter. Usually two CPU sockets or four sockets with very large memory vs. one socket for a desktop. Ocelote standard nodes have 28 cores and 192GB memory.</p> <p></p> <p>Parallel Programming</p> <p>A program that is either multi-tasking (like MPI) or multi-threaded (like OpenMP) or both, in order to effectively use more cores and more nodes and get more computing done. May be either shared-memory or distributed-memory. Unlike a serial program.</p> <p></p> <p>Parallel Scaling</p> <p>The efficiency of a parallel program, usually defined as the parallel speedup of the program divided by the number of cores occupied. Speedup is defined as the serial run time divided by the parallel run time. Usually parallel computing introduces overhead, and scaling is less than 1 (or 100%).  In most cases, scaling starts at 1 on 1 core (by definition) and decreases as more cores are added, until some point is reached at which adding more cores adds overhead and makes the program slower.</p> <p></p> <p>Scheduler/HPC scheduler</p> <p>A program that maintains a list of batch jobs to be executed on a cluster, ranks them in some priority order, and executes batch jobs on compute nodes as they become available. It tries to keep the cluster from being overloaded or idle. Puma, Ocelote, and ElGato use SLURM.</p> <p></p> <p>Scratch storage</p> <p>A temporary file system, designed for speed rather than reliability, and the first tier in the storage hierarchy. On Ocelote and ElGato these are internal SATA disks and referenced as <code>/tmp</code>.</p> <p></p> <p>Shared memory computing</p> <p>A program that runs multiple tasks or software threads, each of which sees the same available memory available from the operating system, and shares that memory using one of the multiple shared memory/multi-threading communication methods (OpenMP, pthreads, POSIX shm, MPI over shared memory, etc.). Shared memory programs cannot run across multiple nodes. Implies a limit (a little less than the amount of memory in the node) to the memory size of the running program.</p> <p></p> <p>Single-threaded computing</p> <p>A software program that cannot take advantage of multi-threading because it was written without multi-threading support. Essentially can use only one core on one node regardless of the number of cores available. Multiple single-threaded programs can be run on a single node on multiple cores.</p> <p></p> <p>SSD</p> <p>Solid state disk, memory chips packaged with an interface that appears to the computer to be a disk drive. Faster than rotating disk drives and still more expensive, though decreasing in price over time.</p> <p></p> <p>Storage hierarchy</p> <p>Each tier of storage is larger and slower than the preceding tier. The first is data in the processor including the processor cache.  The next tier is memory.  Page or swap is an extension of memory but is very inefficient since it actually writes to disk. You should next consider <code>/tmp</code> which is the local disk on each node.  You have no access to <code>/tmp</code> once the job ends.  Shared storage is all of <code>/home</code>, <code>/groups/PI</code>, and <code>/xdisk</code>, and is the slowest.</p> <p></p> <p>Supercomputer</p> <p>A large and powerful cluster. We currently have three: Puma, Ocelote, and ElGato.</p> <p></p> <p>VM or virtual machine</p> <p>This compute model is not usually found in the HPC environment.  It is a method of running several or many virtual machines on one physical machine.  Since HPC nodes are busy most of the time the cost of the VM overhead and management is not worthwhile. </p>"},{"location":"support_and_training/grants/","title":"Grant Resources","text":""},{"location":"support_and_training/grants/#overview","title":"Overview","text":"<p>University of Arizona (UArizona) researchers have access to a variety of high performance computing and storage resources through both UArizona HPC and the NSF-funded CyVerse project. A moderate amount of compute time and storage is free to any research faculty, also known as principal investigators (PIs).</p> <p>You might consider these scenarios:</p> <ol> <li> <p>Run proof-of-concept on the HPC clusters</p> <p>Compute time and storage are free on a moderate scale so no funding is needed to test and develop code, workflows, and methodologies for grants. 2. Run the funded compute on the HPC clusters </p> <p>Using our buy-in model, the compute part of the funding can complement the existing compute resources and reduce personnel costs.</p> </li> <li> <p>Co-locate your cluster in the Computer Center</p> <p>Take advantage of the enterprise class facilities and support to save space and reduce administration and setup costs. More information here.</p> </li> <li> <p>CyVerse data center</p> <p>These reside at UArizona, are funded by the National Science Foundation\u2019s Directorate for Biological Sciences, and provide life scientists with powerful computational infrastructure.</p> </li> <li> <p>ACCESS (formerly XSEDE) national cyberinfrastructure HPC and Cloud providers.</p> <p>Available for free to any US-based researcher. Get started here. UArizona researchers can request startup allocations to test out their system(s) of choice. After the startup allocation, full allocations can be requested for millions of CPU-hours.</p> </li> </ol>"},{"location":"support_and_training/grants/#the-university-of-arizona-research-computing-and-cyberinfrastructure-plan-2022","title":"The University of Arizona Research Computing and Cyberinfrastructure Plan 2022","text":"<p> Click here to download .docx version <p></p>"},{"location":"support_and_training/grants/#data-management-plans","title":"Data Management Plans","text":"<p>A data management plan documents the lifecycle of your data. The plan provides details on data collection for storage, access, sharing, and reproducibility of your results.  A good data management plan will ensure the availability and accessibility of your research results after your project is complete and you have published the results, increasing the value of your research and possible reuse by other researchers. </p> <p>The UArizona Libraries host automated tools to design and execute data management plans for grants. More information here: https://data.library.arizona.edu/</p>"},{"location":"support_and_training/grants/#funding-agency-requirements","title":"Funding Agency Requirements","text":"<p>In 1999, the U.S. Office of Management and Budget amended OMB Circular A-110 to require research data produced with funding from Federal agencies be made publicly available through procedures established through the Freedom of Information Act (FOIA).</p> <p>Federally funded research - access to publications and data</p> <p>The White House Office of Science and Technology Policy (OSTP) released a policy memorandum, \u201cIncreasing Access to the Results of Federally Funded Scientific Research,\u201d on February 22, 2013. See the Federal Agency Policies for Public Access website for brief descriptions of the policies for top UA federal funding agencies and where to get further help.</p> <p>The Library has created a summary table of public access plans for all of the 19 federal funding agencies that fall under the OSTP memo.</p> <p>The University offers other resources for grant applications here: https://rgw.arizona.edu/development/proposal-development/external-grant-writing-resources</p>"},{"location":"support_and_training/workshops/","title":"Index","text":"<p>foo</p>"},{"location":"support_and_training/workshops/intro_to_hpc/","title":"Intro to HPC","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#about","title":"About","text":"<p>We encourage you to take training course to help you get started on using HPC resources. If you have recently started, you might learn there is a better way, or there are capabilities you are not taking advantage of. Either attend the workshops which are held once each semester or view the video attached below. We can also bring these to department meetings.</p>"},{"location":"support_and_training/workshops/intro_to_hpc/#pdf","title":"PDF","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#video-recording","title":"Video Recording","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#interactive-guide","title":"Interactive Guide","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#system-basics","title":"System Basics","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#logging-in","title":"Logging In","text":"<p>=== Local Terminal     === Mac/Linux</p> <pre><code>=== Windows\n</code></pre> <p>=== Open OnDemand</p>"},{"location":"support_and_training/workshops/intro_to_hpc/#working-on-the-command-line","title":"Working on the Command Line","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#transferring-files","title":"Transferring Files","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#submitting-a-batch-job","title":"Submitting a Batch Job","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#accessing-workshop-files","title":"Accessing Workshop Files","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#submissions-script-details","title":"Submissions Script Details","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#submitting-your-job","title":"Submitting Your Job","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#job-output","title":"Job Output","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#interactive-vs-batch","title":"Interactive Vs. Batch","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#accessing-software","title":"Accessing Software","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#command-line","title":"Command Line","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#open-ondemand","title":"Open OnDemand","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#testing-your-knowledge","title":"Testing Your Knowledge","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#getting-help","title":"Getting Help","text":""},{"location":"support_and_training/workshops/intro_to_hpc/#community-events","title":"Community Events","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/python/","title":"Intro to Machine Learning in Python","text":"<p>This short training class provides a brief introduction to key concepts of machine learning.  The short lecture will be followed by two hands-on examples that emphasize running a Jupyter notebook on the HPC supercomputers. For the in-person workshop you can stick around and use this as a consulting session.</p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#pdf","title":"PDF","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#video-presentation","title":"Video Presentation","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#hands-on-content","title":"Hands On Content","text":""},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#setting-up","title":"Setting up","text":"<p>A component of these workshops is interactive where users will learn to:</p> <ol> <li>Train and visualize a linear regression model using a training set.</li> <li>Build and visualize a clustering model using the elbow method.</li> </ol> <p>Both of these exercises will make use of Python in a Jupyter Notebook through Open OnDemand.</p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#accessing-the-data","title":"Accessing the Data","text":"<p>To begin, start a terminal to log into the system and copy the necessary files into your account. If you're unsure of how to use or access a terminal, see our online documentation for information (or, if you're in a live workshop, flag one of us down and we can help). To get the files you need, use the following commands:  <pre><code>ssh your_netid@hpc.arizona.edu\nshell\nelgato\n</code></pre> Now, to download the example, use: <pre><code>wget https://ua-researchcomputing-hpc.github.io/Intro-to-Machine-Learning/intro-to-ML.tar.gz\ntar xzvf intro-to-ML.tar.gz\nrm intro-to-ML.tar.gz\n</code></pre></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#starting-a-jupyter-session","title":"Starting a Jupyter Session","text":"<p>For this tutorial, we'll use a Jupyter Notebook which is available as an interactive application and can be accessed through Open OnDemand.</p> <p> </p> <p>Once you log in using your university credentials, click the Interactive Apps dropdown menu and select Jupyter Notebook. This will bring you to a web form that's used to request compute resources on one of our clusters. Use the following options in your request:</p> Option Value Cluster ElGato Cluter Run Time 2 Core count on a single node 1 Memory per core 4 GPUs required 0 PI Group your group** Queue standard <p>** If you don't know your group's name, go to a terminal session (see section above) and use the command <code>va</code>.</p> <p>Once you complete the form, click Launch. This will bring you to a page with a tile that shows your pending job. When it's first submitted, its status will show as Queued. Once it starts, it's status will change to Running and you'll be given a link you can use to connect. </p> <p></p> <p></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#opening-a-notebook","title":"Opening a Notebook","text":"<p>Once you've clicked Launch, you'll see a file navigator. This is your home directory on HPC. To access the files for the examples, click the intro-to-hpc directory you created earlier. To open a notebook, click the New dropdown menu in the upper right and select python3.</p> <p></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#running-code","title":"Running Code","text":"<p>To run Python code in a notebook, enter your commands into a cell and click <code>Run</code>. To add a new cell, click the <code>+</code> in the upper left. </p> <p></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#linear-regression-example","title":"Linear Regression Example","text":"<p>Import Libraries <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n</code></pre> Use Pandas to load the data and view the first five rows <pre><code>data = pd.read_excel(\"king_county_house_data.xls\")\ndata.head(5)\n</code></pre> Choose the columns from the data and split into train and test sets <pre><code>space = data['sqft_living']\nprice = data['price']\n# Change X into 2D array\nX = np.array(space).reshape(-1, 1)\nY = np.array(price)\n# Split data into train sets and test sets\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=1/3,random_state=0)\n</code></pre> Visualize the train set <pre><code># Visualize training set\nplt.scatter(X_train,Y_train,color=\"red\",label=\"Living Area\")\nplt.title(\"Housing Prices\")\nplt.xlabel(\"Area\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.show()\n</code></pre> Train the model with the training set and predict with the test set <pre><code># Train\nregressor = LinearRegression()\nregressor.fit(X_train, Y_train)\n# Prediction\ny_pred = regressor.predict(X_test)\n</code></pre></p> <p>Visualize the train data and the best fit line <pre><code># Visualize the data and the best fit line\nplt.scatter(X_train,Y_train,color=\"red\",label=\"Living Area\")\nplt.title(\"Housing Prices in King County\")\nplt.plot(X_train,regressor.predict(X_train),color=\"blue\",label=\"Price\")\nplt.xlabel(\"Area\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.show()\n</code></pre> Predict the price of a house with a certain area <pre><code># Make a prediction\narea = 5000\nprice = regressor.predict([[area]])\nprint('House of %d sq-ft costs about $%d' % (area, price))\n</code></pre> Visualize the test data <pre><code># Visualize test set\nplt.scatter(X_test,Y_test,color='red',label=\"Living Area\")\nplt.plot(X_test,regressor.predict(X_test),color=\"blue\",label=\"Price\")\nplt.xlabel=\"Area (sq-ft)\")\nplt.ylabel(\"Price (USD)\")\nplt.legend()\nplt.show()\n</code></pre></p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#clustering-model-example","title":"Clustering Model Example","text":"<p>Import libraries  <pre><code># import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\n</code></pre> Load the data <pre><code># load the data\niris=load_iris()\niris\n</code></pre> Convert to a dataframe <pre><code>df=pd.DataFrame(data=iris.data, columns=['sepal length','sepal width','petal length','petal width'])\ndf['target']=pd.Series(iris.target)\ndf\n</code></pre> Visualize the data <pre><code># visualize the data\nplt.scatter(x=df['sepal length'], y=df['sepal width'] ,c=iris.target, cmap='gist_rainbow')\nplt.xlabel('Sepal Width', fontsize=18)\nplt.ylabel('Sepal Length', fontsize=18)\n</code></pre> Estimate k with elbow method. First try k=5. <pre><code># Estimate k with elbow at k=5\nx = iris.data\nkmeans5 = KMeans(n_clusters=5,init = 'k-means++', random_state = 0)\ny = kmeans5.fit_predict(x)\nprint(y)\n</code></pre> Visualize centers. <pre><code>kmeans5.cluster_centers_\nplt.scatter(x[y == 0,0], x[y==0,1], s = 15, c= 'red', label = 'Cluster_1')\nplt.scatter(x[y == 1,0], x[y==1,1], s = 15, c= 'blue', label = 'Cluster_2')\nplt.scatter(x[y == 2,0], x[y==2,1], s = 15, c= 'green', label = 'Cluster_3')\nplt.scatter(x[y == 3,0], x[y==3,1], s = 15, c= 'cyan', label = 'Cluster_4')\nplt.scatter(x[y == 4,0], x[y==4,1], s = 15, c= 'magenta', label = 'Cluster_5')\nplt.scatter(kmeans5.cluster_centers_[:,0], kmeans5.cluster_centers_[:,1], s = 25, c = 'yellow', label = 'Centroids')\nplt.legend()\nplt.show()\n</code></pre> Estimate k with elbow method. <pre><code># estimate k with elbow method\nError =[]\nfor i in range(1, 11):\n    kmeans11 = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0).fit(x)\n    kmeans11.fit(x)\n    Error.append(kmeans11.inertia_)\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 11), Error)\nplt.title('Elbow Method with k=1-11') #within cluster sum of squares\nplt.xlabel('Number of clusters')\nplt.ylabel('Error')\nplt.show()\n</code></pre> Get the optimal k=3 from the elbow method. Cluster centers and visualize.</p> <pre><code>kmeans3 = KMeans(n_clusters=3, random_state=21)\ny = kmeans3.fit_predict(x)\nkmeans3.cluster_centers_\nplt.scatter(x[y == 0,0], x[y==0,1], s = 15, c= 'red', label = 'Cluster_1')\nplt.scatter(x[y == 1,0], x[y==1,1], s = 15, c= 'blue', label = 'Cluster_2')\nplt.scatter(x[y == 2,0], x[y==2,1], s = 15, c= 'green', label = 'Cluster_3')\nplt.scatter(kmeans3.cluster_centers_[:,0], kmeans3.cluster_centers_[:,1], s = 25, c = 'black', label = 'Centroids')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#neural-network-classification-example","title":"Neural Network Classification Example","text":"<p>In this example we will use the same Iris dataset that we used in the unsupervised clustering example to train a neural network model to classify the plants using labeled input.</p> <p>We will use the Keras interface to the popular Tensorflow neural network package</p> <p>Import Sscikit learn and Tensorflow/Keras packages:</p> <pre><code>from numpy import argmax\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.datasets import load_iris\nimport tensorflow\n</code></pre> <p>Prepare the built in input data * change measurements to floating point numbers * Change names to integers</p> <pre><code>iris = load_iris()\nX, y = iris.data, iris.target\nX = X.astype('float32')\ny = LabelEncoder().fit_transform(y)\n</code></pre> <p>split into train and test datasets</p> <p>Try this</p> <p>use different proportions for test_size, to change the amount of training data and make the classification problem more or less difficult</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nn_features = X_train.shape[1]\nprint(\"%d measurements per sample\" %  n_features)\n</code></pre> <p>Now set up the details of the neural network model.</p> <ul> <li>Add \"Dense\" (fully connected) neural network layers, specifying the number of nodes per layer</li> <li>activation functions define node output values as a function of node input values</li> <li>kernel_initializer specifies initial values for node connection weights</li> <li>input_shape ensures that the inputs to the first layer are equal to the number of measurements per sample</li> <li>the 3 nodes of the final  layer correspond to the three species designations</li> <li>softmax activation function ensures that the layer outputs all sum to 1.0 (i.e., are probabilities)</li> </ul> <p>Try this</p> <p>change the number of nodes in the first and second layers</p> <pre><code>model = Sequential()\n\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n\nmodel.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n</code></pre> <p>Now train the model using the training data subset * epochs and batch_size control the efficiency of the fitting</p> <pre><code>model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n</code></pre> <p>Evaluate and use the model:</p> <pre><code>loss, acc = model.evaluate(X_test, y_test)\nprint('Test Accuracy: %.3f' % acc)\nrow = [5.1,3.5,1.4,0.2]\nyhat = model.predict([row])\nprint('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\n</code></pre> <p>Try making changes and see how it affects the classification accuracy.</p>"},{"location":"support_and_training/workshops/intro_to_machine_learning/python/#image-classification-with-pytorch","title":"Image Classification with PyTorch","text":"<p>For a more in-depth Neural Network example that is run as a batch submission instead of through Jupyter, see this page:</p> <p>Image Classification</p>"}]}